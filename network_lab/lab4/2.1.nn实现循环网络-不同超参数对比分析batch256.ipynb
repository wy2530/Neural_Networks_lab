{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import pandas\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13580, 13) (3386, 13)\n",
      "[[0.09904153 0.09744409 0.11341853 ... 0.2284345  0.22683706 0.23162939]\n",
      " [0.09744409 0.11341853 0.13738019 ... 0.22683706 0.23162939 0.25878594]\n",
      " [0.11341853 0.13738019 0.16453674 ... 0.23162939 0.25878594 0.20127796]\n",
      " ...\n",
      " [0.0798722  0.09105431 0.06389776 ... 0.08626198 0.05591054 0.07667732]\n",
      " [0.09105431 0.06389776 0.07188498 ... 0.05591054 0.07667732 0.07827476]\n",
      " [0.06389776 0.07188498 0.07507987 ... 0.07667732 0.07827476 0.1086262 ]] [[0.11182109 0.13897764 0.08785942 ... 0.15335463 0.15974441 0.17891374]\n",
      " [0.13897764 0.08785942 0.07827476 ... 0.15974441 0.17891374 0.16453674]\n",
      " [0.08785942 0.07827476 0.07188498 ... 0.17891374 0.16453674 0.15974441]\n",
      " ...\n",
      " [0.24920128 0.21246006 0.17891374 ... 0.11182109 0.1086262  0.11980831]\n",
      " [0.21246006 0.17891374 0.19648562 ... 0.1086262  0.11980831 0.10543131]\n",
      " [0.17891374 0.19648562 0.1884984  ... 0.11980831 0.10543131 0.12300319]]\n"
     ]
    }
   ],
   "source": [
    "## 划分数据集\n",
    "def sliding_window(seq,window_size):\n",
    "    result=[]\n",
    "    for i in range(len(seq)-window_size):\n",
    "        result.append(seq[i:i+window_size])\n",
    "    return result\n",
    "\n",
    "data=np.load(\"./pems-traffic-flow/PEMS04.npz\")  # npz是二进制格式\n",
    "\n",
    "data=data[\"data\"][:,0:1,0:1]\n",
    "# data\n",
    "## 归一化\n",
    "dmin,dmax=data.min(),data.max()\n",
    "data=(data-dmin)/(dmax-dmin)\n",
    "sensordata_num,sensor_num,_ = data.shape  \n",
    "# print(data.shape)  #(16992,1,1)\n",
    "\n",
    "train_set,test_set = [],[]  \n",
    "for  i in range(sensor_num) :  \n",
    "    train_seq = data[:int(sensordata_num*0.8),i,:]  \n",
    "    test_seq = data[int(sensordata_num*0.8):,i,:]  \n",
    "    train_set += sliding_window(train_seq,window_size=13)  \n",
    "    test_set += sliding_window(test_seq,window_size=13)  \n",
    "train_set,test_set= np.array(train_set).squeeze(), np.array(test_set).squeeze()  \n",
    "print(train_set.shape,test_set.shape)    #（13580,13)\n",
    "print(train_set,test_set)                #(3386,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型+优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  \n",
    "model = nn.RNN(input_size=1, hidden_size=32, num_layers=1, batch_first=True).to(device)\n",
    "out_linear = nn.Sequential(nn.Linear(32, 1), nn.LeakyReLU()).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(out_linear.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mape函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):  \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)  \n",
    "    non_zero_index = (y_true > 0)  \n",
    "    y_true = y_true[non_zero_index]  \n",
    "    y_pred = y_pred[non_zero_index]  \n",
    "  \n",
    "    mape = np.abs((y_true - y_pred) / y_true)  \n",
    "    mape[np.isinf(mape)] = 0  \n",
    "    return np.mean(mape) * 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next_batch函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(data, batch_size):  \n",
    "    data_length = len(data)  \n",
    "    num_batches = math.ceil(data_length / batch_size)  \n",
    "    for batch_index in range(num_batches):  \n",
    "        start_index = batch_index * batch_size  \n",
    "        end_index = min((batch_index + 1) * batch_size, data_length)  \n",
    "        yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "import math \n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image \n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, train_loss 0.145161,Time used 0.711760s\n",
      "batch 2, train_loss 0.159379,Time used 0.004960s\n",
      "batch 3, train_loss 0.157479,Time used 0.002976s\n",
      "batch 4, train_loss 0.161151,Time used 0.002975s\n",
      "batch 5, train_loss 0.155898,Time used 0.002976s\n",
      "batch 6, train_loss 0.160776,Time used 0.002972s\n",
      "batch 7, train_loss 0.147301,Time used 0.003472s\n",
      "batch 8, train_loss 0.136342,Time used 0.002977s\n",
      "batch 9, train_loss 0.150729,Time used 0.002976s\n",
      "batch 10, train_loss 0.153755,Time used 0.002976s\n",
      "batch 11, train_loss 0.153493,Time used 0.002976s\n",
      "batch 12, train_loss 0.153272,Time used 0.002975s\n",
      "batch 13, train_loss 0.142501,Time used 0.002976s\n",
      "batch 14, train_loss 0.151281,Time used 0.002976s\n",
      "batch 15, train_loss 0.125219,Time used 0.003968s\n",
      "batch 16, train_loss 0.145576,Time used 0.004960s\n",
      "batch 17, train_loss 0.130294,Time used 0.003472s\n",
      "batch 18, train_loss 0.137121,Time used 0.013887s\n",
      "batch 19, train_loss 0.120249,Time used 0.003472s\n",
      "batch 20, train_loss 0.137565,Time used 0.002983s\n",
      "batch 21, train_loss 0.118316,Time used 0.003484s\n",
      "batch 22, train_loss 0.129027,Time used 0.002976s\n",
      "batch 23, train_loss 0.129716,Time used 0.003472s\n",
      "batch 24, train_loss 0.130046,Time used 0.003472s\n",
      "batch 25, train_loss 0.113786,Time used 0.002976s\n",
      "batch 26, train_loss 0.122932,Time used 0.003472s\n",
      "batch 27, train_loss 0.116251,Time used 0.014880s\n",
      "batch 28, train_loss 0.131025,Time used 0.003472s\n",
      "batch 29, train_loss 0.101615,Time used 0.002976s\n",
      "batch 30, train_loss 0.116594,Time used 0.003472s\n",
      "batch 31, train_loss 0.108272,Time used 0.002973s\n",
      "batch 32, train_loss 0.129776,Time used 0.002976s\n",
      "batch 33, train_loss 0.123678,Time used 0.002977s\n",
      "batch 34, train_loss 0.123988,Time used 0.002480s\n",
      "batch 35, train_loss 0.112019,Time used 0.002976s\n",
      "batch 36, train_loss 0.113591,Time used 0.002976s\n",
      "batch 37, train_loss 0.107235,Time used 0.002976s\n",
      "batch 38, train_loss 0.112742,Time used 0.002480s\n",
      "batch 39, train_loss 0.093055,Time used 0.002476s\n",
      "batch 40, train_loss 0.096135,Time used 0.003472s\n",
      "batch 41, train_loss 0.098277,Time used 0.002973s\n",
      "batch 42, train_loss 0.102419,Time used 0.002976s\n",
      "batch 43, train_loss 0.108858,Time used 0.002973s\n",
      "batch 44, train_loss 0.087309,Time used 0.002976s\n",
      "batch 45, train_loss 0.103793,Time used 0.003472s\n",
      "batch 46, train_loss 0.098178,Time used 0.003472s\n",
      "batch 47, train_loss 0.090479,Time used 0.002976s\n",
      "batch 48, train_loss 0.088456,Time used 0.002986s\n",
      "batch 49, train_loss 0.085999,Time used 0.002976s\n",
      "batch 50, train_loss 0.089496,Time used 0.002976s\n",
      "batch 51, train_loss 0.085192,Time used 0.002976s\n",
      "batch 52, train_loss 0.090175,Time used 0.002981s\n",
      "batch 53, train_loss 0.086416,Time used 0.002971s\n",
      "batch 54, train_loss 0.104811,Time used 0.004464s\n",
      "batch 55, train_loss 0.088719,Time used 0.002480s\n",
      "batch 56, train_loss 0.074445,Time used 0.002976s\n",
      "batch 57, train_loss 0.082949,Time used 0.002978s\n",
      "batch 58, train_loss 0.072312,Time used 0.002976s\n",
      "batch 59, train_loss 0.077811,Time used 0.003968s\n",
      "batch 60, train_loss 0.071420,Time used 0.002977s\n",
      "batch 61, train_loss 0.079006,Time used 0.002976s\n",
      "batch 62, train_loss 0.082036,Time used 0.003473s\n",
      "batch 63, train_loss 0.076625,Time used 0.003473s\n",
      "batch 64, train_loss 0.076706,Time used 0.002976s\n",
      "batch 65, train_loss 0.080300,Time used 0.003467s\n",
      "batch 66, train_loss 0.073474,Time used 0.003472s\n",
      "batch 67, train_loss 0.067731,Time used 0.002975s\n",
      "batch 68, train_loss 0.076301,Time used 0.003968s\n",
      "batch 69, train_loss 0.072143,Time used 0.002975s\n",
      "batch 70, train_loss 0.073663,Time used 0.002971s\n",
      "batch 71, train_loss 0.072243,Time used 0.003473s\n",
      "batch 72, train_loss 0.078282,Time used 0.002976s\n",
      "batch 73, train_loss 0.071351,Time used 0.003968s\n",
      "batch 74, train_loss 0.072596,Time used 0.002976s\n",
      "batch 75, train_loss 0.070530,Time used 0.002975s\n",
      "batch 76, train_loss 0.066497,Time used 0.003968s\n",
      "batch 77, train_loss 0.067108,Time used 0.002975s\n",
      "batch 78, train_loss 0.069288,Time used 0.002977s\n",
      "batch 79, train_loss 0.065886,Time used 0.002976s\n",
      "batch 80, train_loss 0.062108,Time used 0.002985s\n",
      "batch 81, train_loss 0.066364,Time used 0.003473s\n",
      "batch 82, train_loss 0.070941,Time used 0.002977s\n",
      "batch 83, train_loss 0.065072,Time used 0.002480s\n",
      "batch 84, train_loss 0.067855,Time used 0.003472s\n",
      "batch 85, train_loss 0.060891,Time used 0.003472s\n",
      "batch 86, train_loss 0.066825,Time used 0.003468s\n",
      "batch 87, train_loss 0.067282,Time used 0.002976s\n",
      "batch 88, train_loss 0.068391,Time used 0.002975s\n",
      "batch 89, train_loss 0.063992,Time used 0.002975s\n",
      "batch 90, train_loss 0.070196,Time used 0.002976s\n",
      "batch 91, train_loss 0.064827,Time used 0.002975s\n",
      "batch 92, train_loss 0.059306,Time used 0.002976s\n",
      "batch 93, train_loss 0.065389,Time used 0.003967s\n",
      "batch 94, train_loss 0.056915,Time used 0.002976s\n",
      "batch 95, train_loss 0.058865,Time used 0.002976s\n",
      "batch 96, train_loss 0.061820,Time used 0.003968s\n",
      "batch 97, train_loss 0.065901,Time used 0.003476s\n",
      "batch 98, train_loss 0.057734,Time used 0.002480s\n",
      "batch 99, train_loss 0.061406,Time used 0.002975s\n",
      "batch 100, train_loss 0.063959,Time used 0.003472s\n",
      "***************************test_batch 100, test_rmse_loss 0.249772,test_mae_loss 0.217683,test_mape_loss 132.679234,Time used 0.011408s\n",
      "batch 101, train_loss 0.054868,Time used 0.002976s\n",
      "batch 102, train_loss 0.056772,Time used 0.003472s\n",
      "batch 103, train_loss 0.063665,Time used 0.002976s\n",
      "batch 104, train_loss 0.057313,Time used 0.003472s\n",
      "batch 105, train_loss 0.066249,Time used 0.002976s\n",
      "batch 106, train_loss 0.064230,Time used 0.002976s\n",
      "batch 107, train_loss 0.060469,Time used 0.003472s\n",
      "batch 108, train_loss 0.058895,Time used 0.003472s\n",
      "batch 109, train_loss 0.061962,Time used 0.002975s\n",
      "batch 110, train_loss 0.063264,Time used 0.002976s\n",
      "batch 111, train_loss 0.061567,Time used 0.002977s\n",
      "batch 112, train_loss 0.065769,Time used 0.011904s\n",
      "batch 113, train_loss 0.065396,Time used 0.003472s\n",
      "batch 114, train_loss 0.062479,Time used 0.002977s\n",
      "batch 115, train_loss 0.060444,Time used 0.002972s\n",
      "batch 116, train_loss 0.069311,Time used 0.002976s\n",
      "batch 117, train_loss 0.058930,Time used 0.002976s\n",
      "batch 118, train_loss 0.060522,Time used 0.002976s\n",
      "batch 119, train_loss 0.057019,Time used 0.002970s\n",
      "batch 120, train_loss 0.064667,Time used 0.003472s\n",
      "batch 121, train_loss 0.069611,Time used 0.002976s\n",
      "batch 122, train_loss 0.061939,Time used 0.003472s\n",
      "batch 123, train_loss 0.062347,Time used 0.003471s\n",
      "batch 124, train_loss 0.059763,Time used 0.002976s\n",
      "batch 125, train_loss 0.052916,Time used 0.003472s\n",
      "batch 126, train_loss 0.060209,Time used 0.003968s\n",
      "batch 127, train_loss 0.063791,Time used 0.003472s\n",
      "batch 128, train_loss 0.056045,Time used 0.002976s\n",
      "batch 129, train_loss 0.062391,Time used 0.002976s\n",
      "batch 130, train_loss 0.057812,Time used 0.003469s\n",
      "batch 131, train_loss 0.058283,Time used 0.002977s\n",
      "batch 132, train_loss 0.055139,Time used 0.002976s\n",
      "batch 133, train_loss 0.063247,Time used 0.002976s\n",
      "batch 134, train_loss 0.056336,Time used 0.002975s\n",
      "batch 135, train_loss 0.061036,Time used 0.003968s\n",
      "batch 136, train_loss 0.056234,Time used 0.003968s\n",
      "batch 137, train_loss 0.060843,Time used 0.003472s\n",
      "batch 138, train_loss 0.062396,Time used 0.002479s\n",
      "batch 139, train_loss 0.056201,Time used 0.002977s\n",
      "batch 140, train_loss 0.057278,Time used 0.002976s\n",
      "batch 141, train_loss 0.060220,Time used 0.002480s\n",
      "batch 142, train_loss 0.057108,Time used 0.003473s\n",
      "batch 143, train_loss 0.056448,Time used 0.002977s\n",
      "batch 144, train_loss 0.056811,Time used 0.002977s\n",
      "batch 145, train_loss 0.055236,Time used 0.002976s\n",
      "batch 146, train_loss 0.056342,Time used 0.002976s\n",
      "batch 147, train_loss 0.057837,Time used 0.002976s\n",
      "batch 148, train_loss 0.054620,Time used 0.003472s\n",
      "batch 149, train_loss 0.055168,Time used 0.002975s\n",
      "batch 150, train_loss 0.063389,Time used 0.002976s\n",
      "batch 151, train_loss 0.058236,Time used 0.003472s\n",
      "batch 152, train_loss 0.057053,Time used 0.002976s\n",
      "batch 153, train_loss 0.058597,Time used 0.002976s\n",
      "batch 154, train_loss 0.055960,Time used 0.003472s\n",
      "batch 155, train_loss 0.056412,Time used 0.002977s\n",
      "batch 156, train_loss 0.055824,Time used 0.003468s\n",
      "batch 157, train_loss 0.057857,Time used 0.002976s\n",
      "batch 158, train_loss 0.053782,Time used 0.002976s\n",
      "batch 159, train_loss 0.063714,Time used 0.004465s\n",
      "batch 160, train_loss 0.060323,Time used 0.002977s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 161, train_loss 0.057107,Time used 0.002976s\n",
      "batch 162, train_loss 0.080219,Time used 0.003962s\n",
      "batch 163, train_loss 0.059348,Time used 0.002976s\n",
      "batch 164, train_loss 0.057792,Time used 0.002976s\n",
      "batch 165, train_loss 0.050140,Time used 0.002977s\n",
      "batch 166, train_loss 0.053722,Time used 0.002976s\n",
      "batch 167, train_loss 0.053536,Time used 0.003472s\n",
      "batch 168, train_loss 0.059112,Time used 0.003472s\n",
      "batch 169, train_loss 0.056443,Time used 0.003472s\n",
      "batch 170, train_loss 0.055509,Time used 0.002977s\n",
      "batch 171, train_loss 0.059968,Time used 0.003464s\n",
      "batch 172, train_loss 0.058165,Time used 0.004961s\n",
      "batch 173, train_loss 0.061556,Time used 0.002976s\n",
      "batch 174, train_loss 0.053379,Time used 0.002981s\n",
      "batch 175, train_loss 0.059490,Time used 0.002976s\n",
      "batch 176, train_loss 0.056657,Time used 0.003967s\n",
      "batch 177, train_loss 0.060017,Time used 0.002976s\n",
      "batch 178, train_loss 0.056417,Time used 0.003472s\n",
      "batch 179, train_loss 0.057230,Time used 0.002974s\n",
      "batch 180, train_loss 0.053545,Time used 0.003472s\n",
      "batch 181, train_loss 0.048962,Time used 0.003471s\n",
      "batch 182, train_loss 0.063161,Time used 0.003473s\n",
      "batch 183, train_loss 0.056444,Time used 0.003472s\n",
      "batch 184, train_loss 0.055119,Time used 0.003476s\n",
      "batch 185, train_loss 0.056794,Time used 0.002976s\n",
      "batch 186, train_loss 0.058876,Time used 0.002975s\n",
      "batch 187, train_loss 0.059191,Time used 0.002976s\n",
      "batch 188, train_loss 0.050840,Time used 0.003472s\n",
      "batch 189, train_loss 0.053760,Time used 0.003473s\n",
      "batch 190, train_loss 0.061525,Time used 0.002976s\n",
      "batch 191, train_loss 0.052391,Time used 0.002976s\n",
      "batch 192, train_loss 0.049257,Time used 0.003472s\n",
      "batch 193, train_loss 0.053541,Time used 0.002975s\n",
      "batch 194, train_loss 0.055226,Time used 0.003968s\n",
      "batch 195, train_loss 0.058447,Time used 0.004475s\n",
      "batch 196, train_loss 0.059665,Time used 0.002976s\n",
      "batch 197, train_loss 0.052931,Time used 0.012400s\n",
      "batch 198, train_loss 0.053734,Time used 0.002976s\n",
      "batch 199, train_loss 0.062337,Time used 0.002977s\n",
      "batch 200, train_loss 0.059626,Time used 0.002968s\n",
      "***************************test_batch 200, test_rmse_loss 0.234220,test_mae_loss 0.203154,test_mape_loss 127.615366,Time used 0.011904s\n",
      "batch 201, train_loss 0.054586,Time used 0.002976s\n",
      "batch 202, train_loss 0.057328,Time used 0.003472s\n",
      "batch 203, train_loss 0.054437,Time used 0.003471s\n",
      "batch 204, train_loss 0.052484,Time used 0.003472s\n",
      "batch 205, train_loss 0.052962,Time used 0.002976s\n",
      "batch 206, train_loss 0.049782,Time used 0.002977s\n",
      "batch 207, train_loss 0.057542,Time used 0.002975s\n",
      "batch 208, train_loss 0.062366,Time used 0.002976s\n",
      "batch 209, train_loss 0.053060,Time used 0.003472s\n",
      "batch 210, train_loss 0.056428,Time used 0.002977s\n",
      "batch 211, train_loss 0.055551,Time used 0.002978s\n",
      "batch 212, train_loss 0.054782,Time used 0.002976s\n",
      "batch 213, train_loss 0.051572,Time used 0.002976s\n",
      "batch 214, train_loss 0.051768,Time used 0.003968s\n",
      "batch 215, train_loss 0.050098,Time used 0.013888s\n",
      "batch 216, train_loss 0.037015,Time used 0.002480s\n",
      "batch 217, train_loss 0.050068,Time used 0.002976s\n",
      "batch 218, train_loss 0.055492,Time used 0.002975s\n",
      "batch 219, train_loss 0.056414,Time used 0.003980s\n",
      "batch 220, train_loss 0.055375,Time used 0.004464s\n",
      "batch 221, train_loss 0.052073,Time used 0.003472s\n",
      "batch 222, train_loss 0.054247,Time used 0.002976s\n",
      "batch 223, train_loss 0.049714,Time used 0.002975s\n",
      "batch 224, train_loss 0.056409,Time used 0.002975s\n",
      "batch 225, train_loss 0.052402,Time used 0.002975s\n",
      "batch 226, train_loss 0.059147,Time used 0.002976s\n",
      "batch 227, train_loss 0.050871,Time used 0.003472s\n",
      "batch 228, train_loss 0.052354,Time used 0.002976s\n",
      "batch 229, train_loss 0.053056,Time used 0.003471s\n",
      "batch 230, train_loss 0.051182,Time used 0.002976s\n",
      "batch 231, train_loss 0.056976,Time used 0.002976s\n",
      "batch 232, train_loss 0.052794,Time used 0.002976s\n",
      "batch 233, train_loss 0.049676,Time used 0.002480s\n",
      "batch 234, train_loss 0.051321,Time used 0.002479s\n",
      "batch 235, train_loss 0.054598,Time used 0.003471s\n",
      "batch 236, train_loss 0.053662,Time used 0.002975s\n",
      "batch 237, train_loss 0.054917,Time used 0.002480s\n",
      "batch 238, train_loss 0.054935,Time used 0.003472s\n",
      "batch 239, train_loss 0.049330,Time used 0.002976s\n",
      "batch 240, train_loss 0.057034,Time used 0.002976s\n",
      "batch 241, train_loss 0.057504,Time used 0.003968s\n",
      "batch 242, train_loss 0.049980,Time used 0.002976s\n",
      "batch 243, train_loss 0.049724,Time used 0.003472s\n",
      "batch 244, train_loss 0.054780,Time used 0.002480s\n",
      "batch 245, train_loss 0.057018,Time used 0.002975s\n",
      "batch 246, train_loss 0.051658,Time used 0.002976s\n",
      "batch 247, train_loss 0.047430,Time used 0.002976s\n",
      "batch 248, train_loss 0.050549,Time used 0.002976s\n",
      "batch 249, train_loss 0.053033,Time used 0.002976s\n",
      "batch 250, train_loss 0.051119,Time used 0.002976s\n",
      "batch 251, train_loss 0.050067,Time used 0.002976s\n",
      "batch 252, train_loss 0.051215,Time used 0.002976s\n",
      "batch 253, train_loss 0.053849,Time used 0.002976s\n",
      "batch 254, train_loss 0.051413,Time used 0.002976s\n",
      "batch 255, train_loss 0.055387,Time used 0.002976s\n",
      "batch 256, train_loss 0.048467,Time used 0.002971s\n",
      "batch 257, train_loss 0.049797,Time used 0.004957s\n",
      "batch 258, train_loss 0.054091,Time used 0.003968s\n",
      "batch 259, train_loss 0.053853,Time used 0.002976s\n",
      "batch 260, train_loss 0.049500,Time used 0.003471s\n",
      "batch 261, train_loss 0.055245,Time used 0.003472s\n",
      "batch 262, train_loss 0.046093,Time used 0.003463s\n",
      "batch 263, train_loss 0.059055,Time used 0.002976s\n",
      "batch 264, train_loss 0.046625,Time used 0.002976s\n",
      "batch 265, train_loss 0.045930,Time used 0.002481s\n",
      "batch 266, train_loss 0.047430,Time used 0.002473s\n",
      "batch 267, train_loss 0.054042,Time used 0.002976s\n",
      "batch 268, train_loss 0.054916,Time used 0.002976s\n",
      "batch 269, train_loss 0.049355,Time used 0.002481s\n",
      "batch 270, train_loss 0.057531,Time used 0.003472s\n",
      "batch 271, train_loss 0.051983,Time used 0.002481s\n",
      "batch 272, train_loss 0.050951,Time used 0.002976s\n",
      "batch 273, train_loss 0.052313,Time used 0.003472s\n",
      "batch 274, train_loss 0.050311,Time used 0.003472s\n",
      "batch 275, train_loss 0.053813,Time used 0.002975s\n",
      "batch 276, train_loss 0.049371,Time used 0.003475s\n",
      "batch 277, train_loss 0.056131,Time used 0.012892s\n",
      "batch 278, train_loss 0.048488,Time used 0.002976s\n",
      "batch 279, train_loss 0.051798,Time used 0.002976s\n",
      "batch 280, train_loss 0.048587,Time used 0.002976s\n",
      "batch 281, train_loss 0.045460,Time used 0.003472s\n",
      "batch 282, train_loss 0.047292,Time used 0.002976s\n",
      "batch 283, train_loss 0.049234,Time used 0.002976s\n",
      "batch 284, train_loss 0.050553,Time used 0.002976s\n",
      "batch 285, train_loss 0.052958,Time used 0.003472s\n",
      "batch 286, train_loss 0.050078,Time used 0.002975s\n",
      "batch 287, train_loss 0.047299,Time used 0.002472s\n",
      "batch 288, train_loss 0.046154,Time used 0.002976s\n",
      "batch 289, train_loss 0.049807,Time used 0.003472s\n",
      "batch 290, train_loss 0.046270,Time used 0.002976s\n",
      "batch 291, train_loss 0.049966,Time used 0.002976s\n",
      "batch 292, train_loss 0.049675,Time used 0.002976s\n",
      "batch 293, train_loss 0.054052,Time used 0.002976s\n",
      "batch 294, train_loss 0.049671,Time used 0.002972s\n",
      "batch 295, train_loss 0.052438,Time used 0.002976s\n",
      "batch 296, train_loss 0.050187,Time used 0.002976s\n",
      "batch 297, train_loss 0.053031,Time used 0.002480s\n",
      "batch 298, train_loss 0.048587,Time used 0.002976s\n",
      "batch 299, train_loss 0.049858,Time used 0.003472s\n",
      "batch 300, train_loss 0.054545,Time used 0.003468s\n",
      "***************************test_batch 300, test_rmse_loss 0.221243,test_mae_loss 0.191541,test_mape_loss 120.452980,Time used 0.011409s\n",
      "batch 301, train_loss 0.046534,Time used 0.002976s\n",
      "batch 302, train_loss 0.048380,Time used 0.002976s\n",
      "batch 303, train_loss 0.050957,Time used 0.002976s\n",
      "batch 304, train_loss 0.045782,Time used 0.003472s\n",
      "batch 305, train_loss 0.045640,Time used 0.003472s\n",
      "batch 306, train_loss 0.050171,Time used 0.002976s\n",
      "batch 307, train_loss 0.042933,Time used 0.003966s\n",
      "batch 308, train_loss 0.049282,Time used 0.002976s\n",
      "batch 309, train_loss 0.050246,Time used 0.002976s\n",
      "batch 310, train_loss 0.053127,Time used 0.003968s\n",
      "batch 311, train_loss 0.048320,Time used 0.002976s\n",
      "batch 312, train_loss 0.052035,Time used 0.003473s\n",
      "batch 313, train_loss 0.050451,Time used 0.002976s\n",
      "batch 314, train_loss 0.048202,Time used 0.003476s\n",
      "batch 315, train_loss 0.047848,Time used 0.002975s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 316, train_loss 0.046483,Time used 0.002976s\n",
      "batch 317, train_loss 0.044602,Time used 0.002975s\n",
      "batch 318, train_loss 0.045278,Time used 0.002971s\n",
      "batch 319, train_loss 0.048038,Time used 0.003472s\n",
      "batch 320, train_loss 0.046197,Time used 0.003471s\n",
      "batch 321, train_loss 0.050188,Time used 0.003477s\n",
      "batch 322, train_loss 0.048153,Time used 0.002976s\n",
      "batch 323, train_loss 0.046340,Time used 0.003472s\n",
      "batch 324, train_loss 0.047764,Time used 0.002976s\n",
      "batch 325, train_loss 0.049249,Time used 0.003968s\n",
      "batch 326, train_loss 0.045566,Time used 0.002976s\n",
      "batch 327, train_loss 0.050923,Time used 0.002977s\n",
      "batch 328, train_loss 0.047010,Time used 0.002976s\n",
      "batch 329, train_loss 0.048821,Time used 0.002977s\n",
      "batch 330, train_loss 0.045653,Time used 0.002976s\n",
      "batch 331, train_loss 0.046087,Time used 0.003472s\n",
      "batch 332, train_loss 0.044204,Time used 0.002976s\n",
      "batch 333, train_loss 0.045753,Time used 0.002976s\n",
      "batch 334, train_loss 0.046542,Time used 0.002976s\n",
      "batch 335, train_loss 0.043566,Time used 0.002977s\n",
      "batch 336, train_loss 0.047761,Time used 0.003472s\n",
      "batch 337, train_loss 0.052946,Time used 0.003968s\n",
      "batch 338, train_loss 0.048199,Time used 0.002975s\n",
      "batch 339, train_loss 0.041394,Time used 0.002980s\n",
      "batch 340, train_loss 0.048216,Time used 0.003472s\n",
      "batch 341, train_loss 0.047607,Time used 0.003472s\n",
      "batch 342, train_loss 0.044881,Time used 0.003472s\n",
      "batch 343, train_loss 0.048762,Time used 0.003473s\n",
      "batch 344, train_loss 0.051225,Time used 0.002976s\n",
      "batch 345, train_loss 0.049436,Time used 0.003472s\n",
      "batch 346, train_loss 0.054981,Time used 0.003469s\n",
      "batch 347, train_loss 0.046001,Time used 0.002971s\n",
      "batch 348, train_loss 0.042429,Time used 0.002972s\n",
      "batch 349, train_loss 0.041300,Time used 0.003472s\n",
      "batch 350, train_loss 0.043972,Time used 0.002976s\n",
      "batch 351, train_loss 0.038622,Time used 0.003468s\n",
      "batch 352, train_loss 0.046261,Time used 0.003468s\n",
      "batch 353, train_loss 0.051852,Time used 0.003472s\n",
      "batch 354, train_loss 0.043566,Time used 0.002975s\n",
      "batch 355, train_loss 0.041451,Time used 0.002975s\n",
      "batch 356, train_loss 0.049940,Time used 0.002481s\n",
      "batch 357, train_loss 0.050663,Time used 0.002981s\n",
      "batch 358, train_loss 0.046499,Time used 0.002976s\n",
      "batch 359, train_loss 0.046419,Time used 0.002976s\n",
      "batch 360, train_loss 0.043691,Time used 0.003472s\n",
      "batch 361, train_loss 0.044162,Time used 0.002480s\n",
      "batch 362, train_loss 0.046538,Time used 0.012900s\n",
      "batch 363, train_loss 0.043547,Time used 0.002976s\n",
      "batch 364, train_loss 0.048704,Time used 0.002975s\n",
      "batch 365, train_loss 0.039914,Time used 0.002976s\n",
      "batch 366, train_loss 0.044947,Time used 0.002976s\n",
      "batch 367, train_loss 0.047941,Time used 0.002976s\n",
      "batch 368, train_loss 0.042429,Time used 0.002975s\n",
      "batch 369, train_loss 0.044360,Time used 0.002975s\n",
      "batch 370, train_loss 0.039515,Time used 0.002976s\n",
      "batch 371, train_loss 0.045618,Time used 0.002976s\n",
      "batch 372, train_loss 0.046998,Time used 0.002976s\n",
      "batch 373, train_loss 0.042859,Time used 0.003472s\n",
      "batch 374, train_loss 0.045058,Time used 0.003968s\n",
      "batch 375, train_loss 0.049874,Time used 0.002480s\n",
      "batch 376, train_loss 0.043166,Time used 0.002976s\n",
      "batch 377, train_loss 0.044838,Time used 0.002976s\n",
      "batch 378, train_loss 0.045223,Time used 0.002976s\n",
      "batch 379, train_loss 0.040689,Time used 0.002480s\n",
      "batch 380, train_loss 0.040531,Time used 0.002975s\n",
      "batch 381, train_loss 0.043798,Time used 0.003472s\n",
      "batch 382, train_loss 0.041158,Time used 0.002479s\n",
      "batch 383, train_loss 0.046487,Time used 0.002975s\n",
      "batch 384, train_loss 0.042721,Time used 0.003472s\n",
      "batch 385, train_loss 0.046794,Time used 0.002976s\n",
      "batch 386, train_loss 0.049783,Time used 0.002976s\n",
      "batch 387, train_loss 0.047517,Time used 0.002976s\n",
      "batch 388, train_loss 0.041003,Time used 0.002975s\n",
      "batch 389, train_loss 0.047716,Time used 0.002977s\n",
      "batch 390, train_loss 0.048806,Time used 0.002970s\n",
      "batch 391, train_loss 0.042589,Time used 0.003473s\n",
      "batch 392, train_loss 0.039757,Time used 0.003968s\n",
      "batch 393, train_loss 0.042111,Time used 0.002976s\n",
      "batch 394, train_loss 0.043882,Time used 0.003477s\n",
      "batch 395, train_loss 0.036972,Time used 0.003463s\n",
      "batch 396, train_loss 0.047540,Time used 0.002976s\n",
      "batch 397, train_loss 0.037479,Time used 0.003472s\n",
      "batch 398, train_loss 0.044215,Time used 0.003472s\n",
      "batch 399, train_loss 0.042252,Time used 0.002480s\n",
      "batch 400, train_loss 0.043610,Time used 0.003474s\n",
      "***************************test_batch 400, test_rmse_loss 0.206216,test_mae_loss 0.178276,test_mape_loss 110.969573,Time used 0.017351s\n",
      "batch 401, train_loss 0.044273,Time used 0.003477s\n",
      "batch 402, train_loss 0.043036,Time used 0.003974s\n",
      "batch 403, train_loss 0.040935,Time used 0.004464s\n",
      "batch 404, train_loss 0.046700,Time used 0.003968s\n",
      "batch 405, train_loss 0.048133,Time used 0.003472s\n",
      "batch 406, train_loss 0.039418,Time used 0.002984s\n",
      "batch 407, train_loss 0.039305,Time used 0.002976s\n",
      "batch 408, train_loss 0.038563,Time used 0.003464s\n",
      "batch 409, train_loss 0.040610,Time used 0.003472s\n",
      "batch 410, train_loss 0.040321,Time used 0.002976s\n",
      "batch 411, train_loss 0.040961,Time used 0.002976s\n",
      "batch 412, train_loss 0.036590,Time used 0.002481s\n",
      "batch 413, train_loss 0.038424,Time used 0.003471s\n",
      "batch 414, train_loss 0.043305,Time used 0.003472s\n",
      "batch 415, train_loss 0.042418,Time used 0.002976s\n",
      "batch 416, train_loss 0.045505,Time used 0.003472s\n",
      "batch 417, train_loss 0.041772,Time used 0.002976s\n",
      "batch 418, train_loss 0.042345,Time used 0.003472s\n",
      "batch 419, train_loss 0.043423,Time used 0.002976s\n",
      "batch 420, train_loss 0.041468,Time used 0.002976s\n",
      "batch 421, train_loss 0.038761,Time used 0.014385s\n",
      "batch 422, train_loss 0.041607,Time used 0.002976s\n",
      "batch 423, train_loss 0.036488,Time used 0.002480s\n",
      "batch 424, train_loss 0.042108,Time used 0.003472s\n",
      "batch 425, train_loss 0.038462,Time used 0.002976s\n",
      "batch 426, train_loss 0.042451,Time used 0.002977s\n",
      "batch 427, train_loss 0.040315,Time used 0.002976s\n",
      "batch 428, train_loss 0.042219,Time used 0.002480s\n",
      "batch 429, train_loss 0.039531,Time used 0.002976s\n",
      "batch 430, train_loss 0.040802,Time used 0.002480s\n",
      "batch 431, train_loss 0.040582,Time used 0.002976s\n",
      "batch 432, train_loss 0.030260,Time used 0.002480s\n",
      "batch 433, train_loss 0.039161,Time used 0.002479s\n",
      "batch 434, train_loss 0.039924,Time used 0.002976s\n",
      "batch 435, train_loss 0.044300,Time used 0.002971s\n",
      "batch 436, train_loss 0.036576,Time used 0.002976s\n",
      "batch 437, train_loss 0.032408,Time used 0.002976s\n",
      "batch 438, train_loss 0.039891,Time used 0.002975s\n",
      "batch 439, train_loss 0.043455,Time used 0.002976s\n",
      "batch 440, train_loss 0.042418,Time used 0.002975s\n",
      "batch 441, train_loss 0.037241,Time used 0.002976s\n",
      "batch 442, train_loss 0.037166,Time used 0.002976s\n",
      "batch 443, train_loss 0.042133,Time used 0.003970s\n",
      "batch 444, train_loss 0.040397,Time used 0.002976s\n",
      "batch 445, train_loss 0.042008,Time used 0.002480s\n",
      "batch 446, train_loss 0.040663,Time used 0.013888s\n",
      "batch 447, train_loss 0.037867,Time used 0.002977s\n",
      "batch 448, train_loss 0.039442,Time used 0.002972s\n",
      "batch 449, train_loss 0.038728,Time used 0.002972s\n",
      "batch 450, train_loss 0.037650,Time used 0.002975s\n",
      "batch 451, train_loss 0.035212,Time used 0.003472s\n",
      "batch 452, train_loss 0.034454,Time used 0.002976s\n",
      "batch 453, train_loss 0.041048,Time used 0.002976s\n",
      "batch 454, train_loss 0.037080,Time used 0.002976s\n",
      "batch 455, train_loss 0.038076,Time used 0.003468s\n",
      "batch 456, train_loss 0.035603,Time used 0.002976s\n",
      "batch 457, train_loss 0.040867,Time used 0.002480s\n",
      "batch 458, train_loss 0.041039,Time used 0.002480s\n",
      "batch 459, train_loss 0.032623,Time used 0.002977s\n",
      "batch 460, train_loss 0.037193,Time used 0.002976s\n",
      "batch 461, train_loss 0.038560,Time used 0.002976s\n",
      "batch 462, train_loss 0.035489,Time used 0.002976s\n",
      "batch 463, train_loss 0.037606,Time used 0.002480s\n",
      "batch 464, train_loss 0.033983,Time used 0.002976s\n",
      "batch 465, train_loss 0.038403,Time used 0.002479s\n",
      "batch 466, train_loss 0.035030,Time used 0.002480s\n",
      "batch 467, train_loss 0.037779,Time used 0.002977s\n",
      "batch 468, train_loss 0.035868,Time used 0.002480s\n",
      "batch 469, train_loss 0.038053,Time used 0.002976s\n",
      "batch 470, train_loss 0.032396,Time used 0.003472s\n",
      "batch 471, train_loss 0.035763,Time used 0.002977s\n",
      "batch 472, train_loss 0.035632,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 473, train_loss 0.036171,Time used 0.002977s\n",
      "batch 474, train_loss 0.036570,Time used 0.002976s\n",
      "batch 475, train_loss 0.034739,Time used 0.002480s\n",
      "batch 476, train_loss 0.034863,Time used 0.002480s\n",
      "batch 477, train_loss 0.036794,Time used 0.002480s\n",
      "batch 478, train_loss 0.036849,Time used 0.003968s\n",
      "batch 479, train_loss 0.040156,Time used 0.002972s\n",
      "batch 480, train_loss 0.030997,Time used 0.002480s\n",
      "batch 481, train_loss 0.031372,Time used 0.002976s\n",
      "batch 482, train_loss 0.033396,Time used 0.002480s\n",
      "batch 483, train_loss 0.033054,Time used 0.003471s\n",
      "batch 484, train_loss 0.032972,Time used 0.002977s\n",
      "batch 485, train_loss 0.033962,Time used 0.003471s\n",
      "batch 486, train_loss 0.046084,Time used 0.002480s\n",
      "batch 487, train_loss 0.035847,Time used 0.002976s\n",
      "batch 488, train_loss 0.033686,Time used 0.003467s\n",
      "batch 489, train_loss 0.033450,Time used 0.003472s\n",
      "batch 490, train_loss 0.031653,Time used 0.003479s\n",
      "batch 491, train_loss 0.029675,Time used 0.002976s\n",
      "batch 492, train_loss 0.033505,Time used 0.002974s\n",
      "batch 493, train_loss 0.030036,Time used 0.002975s\n",
      "batch 494, train_loss 0.032926,Time used 0.002978s\n",
      "batch 495, train_loss 0.031634,Time used 0.003472s\n",
      "batch 496, train_loss 0.032613,Time used 0.003472s\n",
      "batch 497, train_loss 0.032596,Time used 0.002976s\n",
      "batch 498, train_loss 0.034752,Time used 0.002976s\n",
      "batch 499, train_loss 0.032720,Time used 0.003474s\n",
      "batch 500, train_loss 0.031245,Time used 0.002976s\n",
      "***************************test_batch 500, test_rmse_loss 0.177002,test_mae_loss 0.151818,test_mape_loss 95.437757,Time used 0.011904s\n",
      "batch 501, train_loss 0.032479,Time used 0.002975s\n",
      "batch 502, train_loss 0.030555,Time used 0.002976s\n",
      "batch 503, train_loss 0.028964,Time used 0.002976s\n",
      "batch 504, train_loss 0.030407,Time used 0.003472s\n",
      "batch 505, train_loss 0.030226,Time used 0.002975s\n",
      "batch 506, train_loss 0.029836,Time used 0.002976s\n",
      "batch 507, train_loss 0.028600,Time used 0.003472s\n",
      "batch 508, train_loss 0.025583,Time used 0.002976s\n",
      "batch 509, train_loss 0.027611,Time used 0.002976s\n",
      "batch 510, train_loss 0.028909,Time used 0.002976s\n",
      "batch 511, train_loss 0.029522,Time used 0.002975s\n",
      "batch 512, train_loss 0.028261,Time used 0.002976s\n",
      "batch 513, train_loss 0.031507,Time used 0.002975s\n",
      "batch 514, train_loss 0.028877,Time used 0.003472s\n",
      "batch 515, train_loss 0.026046,Time used 0.002976s\n",
      "batch 516, train_loss 0.030128,Time used 0.002481s\n",
      "batch 517, train_loss 0.030427,Time used 0.003472s\n",
      "batch 518, train_loss 0.027161,Time used 0.002986s\n",
      "batch 519, train_loss 0.029873,Time used 0.002976s\n",
      "batch 520, train_loss 0.026996,Time used 0.002976s\n",
      "batch 521, train_loss 0.028510,Time used 0.002976s\n",
      "batch 522, train_loss 0.028506,Time used 0.002985s\n",
      "batch 523, train_loss 0.026852,Time used 0.002976s\n",
      "batch 524, train_loss 0.025544,Time used 0.002976s\n",
      "batch 525, train_loss 0.028792,Time used 0.002976s\n",
      "batch 526, train_loss 0.029445,Time used 0.002973s\n",
      "batch 527, train_loss 0.024160,Time used 0.002977s\n",
      "batch 528, train_loss 0.026492,Time used 0.002976s\n",
      "batch 529, train_loss 0.028968,Time used 0.002976s\n",
      "batch 530, train_loss 0.025377,Time used 0.002975s\n",
      "batch 531, train_loss 0.028110,Time used 0.003472s\n",
      "batch 532, train_loss 0.025402,Time used 0.002976s\n",
      "batch 533, train_loss 0.022831,Time used 0.013887s\n",
      "batch 534, train_loss 0.026515,Time used 0.002976s\n",
      "batch 535, train_loss 0.023462,Time used 0.002481s\n",
      "batch 536, train_loss 0.022878,Time used 0.002480s\n",
      "batch 537, train_loss 0.024287,Time used 0.002976s\n",
      "batch 538, train_loss 0.023791,Time used 0.003472s\n",
      "batch 539, train_loss 0.022697,Time used 0.002479s\n",
      "batch 540, train_loss 0.027166,Time used 0.002476s\n",
      "batch 541, train_loss 0.020175,Time used 0.002481s\n",
      "batch 542, train_loss 0.022723,Time used 0.002480s\n",
      "batch 543, train_loss 0.020346,Time used 0.003472s\n",
      "batch 544, train_loss 0.019761,Time used 0.002975s\n",
      "batch 545, train_loss 0.019088,Time used 0.002470s\n",
      "batch 546, train_loss 0.021868,Time used 0.002976s\n",
      "batch 547, train_loss 0.019774,Time used 0.002976s\n",
      "batch 548, train_loss 0.021500,Time used 0.002480s\n",
      "batch 549, train_loss 0.023119,Time used 0.002976s\n",
      "batch 550, train_loss 0.019710,Time used 0.002480s\n",
      "batch 551, train_loss 0.020439,Time used 0.002975s\n",
      "batch 552, train_loss 0.018805,Time used 0.002976s\n",
      "batch 553, train_loss 0.018838,Time used 0.002480s\n",
      "batch 554, train_loss 0.018851,Time used 0.003472s\n",
      "batch 555, train_loss 0.014691,Time used 0.002976s\n",
      "batch 556, train_loss 0.018027,Time used 0.002976s\n",
      "batch 557, train_loss 0.017251,Time used 0.002976s\n",
      "batch 558, train_loss 0.015702,Time used 0.002480s\n",
      "batch 559, train_loss 0.016272,Time used 0.002976s\n",
      "batch 560, train_loss 0.015225,Time used 0.002976s\n",
      "batch 561, train_loss 0.014359,Time used 0.002976s\n",
      "batch 562, train_loss 0.016933,Time used 0.002976s\n",
      "batch 563, train_loss 0.015231,Time used 0.002972s\n",
      "batch 564, train_loss 0.012947,Time used 0.003471s\n",
      "batch 565, train_loss 0.014687,Time used 0.002977s\n",
      "batch 566, train_loss 0.013356,Time used 0.003472s\n",
      "batch 567, train_loss 0.012866,Time used 0.002968s\n",
      "batch 568, train_loss 0.012174,Time used 0.003473s\n",
      "batch 569, train_loss 0.012520,Time used 0.002976s\n",
      "batch 570, train_loss 0.011598,Time used 0.002976s\n",
      "batch 571, train_loss 0.011351,Time used 0.002976s\n",
      "batch 572, train_loss 0.012649,Time used 0.002976s\n",
      "batch 573, train_loss 0.010313,Time used 0.003472s\n",
      "batch 574, train_loss 0.010753,Time used 0.002976s\n",
      "batch 575, train_loss 0.010739,Time used 0.002481s\n",
      "batch 576, train_loss 0.010126,Time used 0.002976s\n",
      "batch 577, train_loss 0.009337,Time used 0.002976s\n",
      "batch 578, train_loss 0.010922,Time used 0.003472s\n",
      "batch 579, train_loss 0.009376,Time used 0.002976s\n",
      "batch 580, train_loss 0.008157,Time used 0.002976s\n",
      "batch 581, train_loss 0.008124,Time used 0.002976s\n",
      "batch 582, train_loss 0.009007,Time used 0.003473s\n",
      "batch 583, train_loss 0.006963,Time used 0.002976s\n",
      "batch 584, train_loss 0.007330,Time used 0.002481s\n",
      "batch 585, train_loss 0.006334,Time used 0.002976s\n",
      "batch 586, train_loss 0.005902,Time used 0.002986s\n",
      "batch 587, train_loss 0.007739,Time used 0.002480s\n",
      "batch 588, train_loss 0.006932,Time used 0.002479s\n",
      "batch 589, train_loss 0.006309,Time used 0.003468s\n",
      "batch 590, train_loss 0.006872,Time used 0.002975s\n",
      "batch 591, train_loss 0.005103,Time used 0.002478s\n",
      "batch 592, train_loss 0.006474,Time used 0.002976s\n",
      "batch 593, train_loss 0.005651,Time used 0.002976s\n",
      "batch 594, train_loss 0.002633,Time used 0.002977s\n",
      "batch 595, train_loss 0.004081,Time used 0.002976s\n",
      "batch 596, train_loss 0.005554,Time used 0.002976s\n",
      "batch 597, train_loss 0.005456,Time used 0.002977s\n",
      "batch 598, train_loss 0.005549,Time used 0.002476s\n",
      "batch 599, train_loss 0.004573,Time used 0.002976s\n",
      "batch 600, train_loss 0.004700,Time used 0.002977s\n",
      "***************************test_batch 600, test_rmse_loss 0.074670,test_mae_loss 0.055760,test_mape_loss 18.724737,Time used 0.010416s\n",
      "batch 601, train_loss 0.005648,Time used 0.002976s\n",
      "batch 602, train_loss 0.005916,Time used 0.002976s\n",
      "batch 603, train_loss 0.005603,Time used 0.002480s\n",
      "batch 604, train_loss 0.006529,Time used 0.002976s\n",
      "batch 605, train_loss 0.005653,Time used 0.002975s\n",
      "batch 606, train_loss 0.006248,Time used 0.002480s\n",
      "batch 607, train_loss 0.006345,Time used 0.002977s\n",
      "batch 608, train_loss 0.004775,Time used 0.002983s\n",
      "batch 609, train_loss 0.006415,Time used 0.002976s\n",
      "batch 610, train_loss 0.005648,Time used 0.002481s\n",
      "batch 611, train_loss 0.005229,Time used 0.002480s\n",
      "batch 612, train_loss 0.005619,Time used 0.002976s\n",
      "batch 613, train_loss 0.004496,Time used 0.002977s\n",
      "batch 614, train_loss 0.006391,Time used 0.003472s\n",
      "batch 615, train_loss 0.005924,Time used 0.002966s\n",
      "batch 616, train_loss 0.005271,Time used 0.002976s\n",
      "batch 617, train_loss 0.004673,Time used 0.002977s\n",
      "batch 618, train_loss 0.006009,Time used 0.003473s\n",
      "batch 619, train_loss 0.005410,Time used 0.002481s\n",
      "batch 620, train_loss 0.006329,Time used 0.002972s\n",
      "batch 621, train_loss 0.005742,Time used 0.002481s\n",
      "batch 622, train_loss 0.006810,Time used 0.003968s\n",
      "batch 623, train_loss 0.006986,Time used 0.003471s\n",
      "batch 624, train_loss 0.005255,Time used 0.002976s\n",
      "batch 625, train_loss 0.006262,Time used 0.002976s\n",
      "batch 626, train_loss 0.005599,Time used 0.002480s\n",
      "batch 627, train_loss 0.006078,Time used 0.012400s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 628, train_loss 0.005155,Time used 0.002976s\n",
      "batch 629, train_loss 0.005794,Time used 0.002976s\n",
      "batch 630, train_loss 0.006065,Time used 0.002976s\n",
      "batch 631, train_loss 0.005911,Time used 0.002976s\n",
      "batch 632, train_loss 0.004491,Time used 0.003472s\n",
      "batch 633, train_loss 0.004333,Time used 0.003471s\n",
      "batch 634, train_loss 0.005609,Time used 0.004958s\n",
      "batch 635, train_loss 0.005627,Time used 0.003472s\n",
      "batch 636, train_loss 0.005245,Time used 0.003477s\n",
      "batch 637, train_loss 0.006403,Time used 0.003461s\n",
      "batch 638, train_loss 0.005098,Time used 0.003472s\n",
      "batch 639, train_loss 0.005813,Time used 0.012401s\n",
      "batch 640, train_loss 0.004605,Time used 0.002976s\n",
      "batch 641, train_loss 0.005061,Time used 0.003472s\n",
      "batch 642, train_loss 0.004721,Time used 0.003472s\n",
      "batch 643, train_loss 0.005237,Time used 0.002976s\n",
      "batch 644, train_loss 0.006723,Time used 0.003473s\n",
      "batch 645, train_loss 0.006294,Time used 0.002977s\n",
      "batch 646, train_loss 0.005313,Time used 0.003473s\n",
      "batch 647, train_loss 0.004970,Time used 0.002977s\n",
      "batch 648, train_loss 0.003081,Time used 0.002480s\n",
      "batch 649, train_loss 0.006329,Time used 0.002480s\n",
      "batch 650, train_loss 0.005576,Time used 0.003473s\n",
      "batch 651, train_loss 0.006036,Time used 0.003472s\n",
      "batch 652, train_loss 0.006218,Time used 0.002975s\n",
      "batch 653, train_loss 0.005945,Time used 0.003477s\n",
      "batch 654, train_loss 0.005353,Time used 0.002481s\n",
      "batch 655, train_loss 0.005434,Time used 0.002977s\n",
      "batch 656, train_loss 0.006164,Time used 0.002976s\n",
      "batch 657, train_loss 0.004774,Time used 0.002976s\n",
      "batch 658, train_loss 0.005324,Time used 0.002976s\n",
      "batch 659, train_loss 0.005738,Time used 0.002480s\n",
      "batch 660, train_loss 0.006951,Time used 0.002982s\n",
      "batch 661, train_loss 0.005241,Time used 0.002976s\n",
      "batch 662, train_loss 0.005545,Time used 0.002976s\n",
      "batch 663, train_loss 0.007146,Time used 0.002975s\n",
      "batch 664, train_loss 0.006044,Time used 0.002976s\n",
      "batch 665, train_loss 0.005246,Time used 0.002976s\n",
      "batch 666, train_loss 0.004718,Time used 0.002976s\n",
      "batch 667, train_loss 0.005561,Time used 0.002976s\n",
      "batch 668, train_loss 0.004402,Time used 0.003472s\n",
      "batch 669, train_loss 0.006598,Time used 0.002976s\n",
      "batch 670, train_loss 0.006362,Time used 0.002976s\n",
      "batch 671, train_loss 0.004569,Time used 0.002472s\n",
      "batch 672, train_loss 0.005163,Time used 0.003472s\n",
      "batch 673, train_loss 0.005959,Time used 0.002968s\n",
      "batch 674, train_loss 0.005425,Time used 0.002976s\n",
      "batch 675, train_loss 0.005337,Time used 0.003968s\n",
      "batch 676, train_loss 0.005055,Time used 0.002976s\n",
      "batch 677, train_loss 0.006049,Time used 0.002976s\n",
      "batch 678, train_loss 0.005395,Time used 0.003959s\n",
      "batch 679, train_loss 0.005149,Time used 0.002974s\n",
      "batch 680, train_loss 0.004910,Time used 0.002480s\n",
      "batch 681, train_loss 0.004864,Time used 0.002480s\n",
      "batch 682, train_loss 0.004677,Time used 0.003468s\n",
      "batch 683, train_loss 0.004813,Time used 0.003473s\n",
      "batch 684, train_loss 0.005356,Time used 0.002976s\n",
      "batch 685, train_loss 0.005957,Time used 0.003473s\n",
      "batch 686, train_loss 0.005188,Time used 0.003472s\n",
      "batch 687, train_loss 0.006499,Time used 0.002976s\n",
      "batch 688, train_loss 0.004852,Time used 0.002976s\n",
      "batch 689, train_loss 0.004816,Time used 0.002976s\n",
      "batch 690, train_loss 0.005385,Time used 0.002480s\n",
      "batch 691, train_loss 0.006347,Time used 0.002976s\n",
      "batch 692, train_loss 0.005724,Time used 0.003472s\n",
      "batch 693, train_loss 0.004439,Time used 0.002988s\n",
      "batch 694, train_loss 0.004948,Time used 0.002980s\n",
      "batch 695, train_loss 0.005496,Time used 0.002976s\n",
      "batch 696, train_loss 0.006560,Time used 0.002968s\n",
      "batch 697, train_loss 0.004632,Time used 0.002483s\n",
      "batch 698, train_loss 0.005261,Time used 0.002976s\n",
      "batch 699, train_loss 0.004481,Time used 0.002977s\n",
      "batch 700, train_loss 0.005923,Time used 0.002976s\n",
      "***************************test_batch 700, test_rmse_loss 0.073704,test_mae_loss 0.054583,test_mape_loss 17.492707,Time used 0.010416s\n",
      "batch 701, train_loss 0.006171,Time used 0.002976s\n",
      "batch 702, train_loss 0.002537,Time used 0.002480s\n",
      "batch 703, train_loss 0.005087,Time used 0.002982s\n",
      "batch 704, train_loss 0.006319,Time used 0.002480s\n",
      "batch 705, train_loss 0.006351,Time used 0.003468s\n",
      "batch 706, train_loss 0.006845,Time used 0.002976s\n",
      "batch 707, train_loss 0.005588,Time used 0.003472s\n",
      "batch 708, train_loss 0.005333,Time used 0.003472s\n",
      "batch 709, train_loss 0.005749,Time used 0.002976s\n",
      "batch 710, train_loss 0.004887,Time used 0.012401s\n",
      "batch 711, train_loss 0.005231,Time used 0.002976s\n",
      "batch 712, train_loss 0.006506,Time used 0.003472s\n",
      "batch 713, train_loss 0.005562,Time used 0.002977s\n",
      "batch 714, train_loss 0.005656,Time used 0.002471s\n",
      "batch 715, train_loss 0.005657,Time used 0.002976s\n",
      "batch 716, train_loss 0.006263,Time used 0.002976s\n",
      "batch 717, train_loss 0.005802,Time used 0.002976s\n",
      "batch 718, train_loss 0.006577,Time used 0.002480s\n",
      "batch 719, train_loss 0.004592,Time used 0.002480s\n",
      "batch 720, train_loss 0.005156,Time used 0.002976s\n",
      "batch 721, train_loss 0.005716,Time used 0.003472s\n",
      "batch 722, train_loss 0.004669,Time used 0.002484s\n",
      "batch 723, train_loss 0.004989,Time used 0.002976s\n",
      "batch 724, train_loss 0.005920,Time used 0.002977s\n",
      "batch 725, train_loss 0.004592,Time used 0.002480s\n",
      "batch 726, train_loss 0.006012,Time used 0.002975s\n",
      "batch 727, train_loss 0.004854,Time used 0.002976s\n",
      "batch 728, train_loss 0.005969,Time used 0.002480s\n",
      "batch 729, train_loss 0.005856,Time used 0.002976s\n",
      "batch 730, train_loss 0.006062,Time used 0.002976s\n",
      "batch 731, train_loss 0.005214,Time used 0.002976s\n",
      "batch 732, train_loss 0.005699,Time used 0.003472s\n",
      "batch 733, train_loss 0.005178,Time used 0.003472s\n",
      "batch 734, train_loss 0.005659,Time used 0.002976s\n",
      "batch 735, train_loss 0.004762,Time used 0.002976s\n",
      "batch 736, train_loss 0.004943,Time used 0.002976s\n",
      "batch 737, train_loss 0.005753,Time used 0.002984s\n",
      "batch 738, train_loss 0.005320,Time used 0.002974s\n",
      "batch 739, train_loss 0.004475,Time used 0.002977s\n",
      "batch 740, train_loss 0.005425,Time used 0.002975s\n",
      "batch 741, train_loss 0.004533,Time used 0.003968s\n",
      "batch 742, train_loss 0.004852,Time used 0.002976s\n",
      "batch 743, train_loss 0.005173,Time used 0.002975s\n",
      "batch 744, train_loss 0.006738,Time used 0.003968s\n",
      "batch 745, train_loss 0.005382,Time used 0.002975s\n",
      "batch 746, train_loss 0.004823,Time used 0.002968s\n",
      "batch 747, train_loss 0.006412,Time used 0.002976s\n",
      "batch 748, train_loss 0.005066,Time used 0.003469s\n",
      "batch 749, train_loss 0.004991,Time used 0.002976s\n",
      "batch 750, train_loss 0.004264,Time used 0.002976s\n",
      "batch 751, train_loss 0.006175,Time used 0.002976s\n",
      "batch 752, train_loss 0.005680,Time used 0.005458s\n",
      "batch 753, train_loss 0.005367,Time used 0.003967s\n",
      "batch 754, train_loss 0.004299,Time used 0.003476s\n",
      "batch 755, train_loss 0.004207,Time used 0.003472s\n",
      "batch 756, train_loss 0.008118,Time used 0.002976s\n",
      "batch 757, train_loss 0.005202,Time used 0.002975s\n",
      "batch 758, train_loss 0.004775,Time used 0.003472s\n",
      "batch 759, train_loss 0.006652,Time used 0.003472s\n",
      "batch 760, train_loss 0.005035,Time used 0.003472s\n",
      "batch 761, train_loss 0.005947,Time used 0.003968s\n",
      "batch 762, train_loss 0.004976,Time used 0.002976s\n",
      "batch 763, train_loss 0.003630,Time used 0.003471s\n",
      "batch 764, train_loss 0.004569,Time used 0.003471s\n",
      "batch 765, train_loss 0.004927,Time used 0.003468s\n",
      "batch 766, train_loss 0.004851,Time used 0.002976s\n",
      "batch 767, train_loss 0.005738,Time used 0.002976s\n",
      "batch 768, train_loss 0.005466,Time used 0.002971s\n",
      "batch 769, train_loss 0.005694,Time used 0.002976s\n",
      "batch 770, train_loss 0.006118,Time used 0.002975s\n",
      "batch 771, train_loss 0.004907,Time used 0.002976s\n",
      "batch 772, train_loss 0.005753,Time used 0.002479s\n",
      "batch 773, train_loss 0.004824,Time used 0.002976s\n",
      "batch 774, train_loss 0.005719,Time used 0.002976s\n",
      "batch 775, train_loss 0.005351,Time used 0.002481s\n",
      "batch 776, train_loss 0.005142,Time used 0.003472s\n",
      "batch 777, train_loss 0.004839,Time used 0.002976s\n",
      "batch 778, train_loss 0.004920,Time used 0.002481s\n",
      "batch 779, train_loss 0.004622,Time used 0.002480s\n",
      "batch 780, train_loss 0.004605,Time used 0.002971s\n",
      "batch 781, train_loss 0.006451,Time used 0.002976s\n",
      "batch 782, train_loss 0.004954,Time used 0.002976s\n",
      "batch 783, train_loss 0.004849,Time used 0.002976s\n",
      "batch 784, train_loss 0.007139,Time used 0.002976s\n",
      "batch 785, train_loss 0.005011,Time used 0.002480s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 786, train_loss 0.005751,Time used 0.002479s\n",
      "batch 787, train_loss 0.004019,Time used 0.002977s\n",
      "batch 788, train_loss 0.005856,Time used 0.002964s\n",
      "batch 789, train_loss 0.005294,Time used 0.002976s\n",
      "batch 790, train_loss 0.004815,Time used 0.002976s\n",
      "batch 791, train_loss 0.005804,Time used 0.003472s\n",
      "batch 792, train_loss 0.005038,Time used 0.002480s\n",
      "batch 793, train_loss 0.005011,Time used 0.002976s\n",
      "batch 794, train_loss 0.007878,Time used 0.002976s\n",
      "batch 795, train_loss 0.005695,Time used 0.002976s\n",
      "batch 796, train_loss 0.005206,Time used 0.002976s\n",
      "batch 797, train_loss 0.005893,Time used 0.002976s\n",
      "batch 798, train_loss 0.006606,Time used 0.004464s\n",
      "batch 799, train_loss 0.006114,Time used 0.002969s\n",
      "batch 800, train_loss 0.004988,Time used 0.003472s\n",
      "***************************test_batch 800, test_rmse_loss 0.072956,test_mae_loss 0.054002,test_mape_loss 17.262130,Time used 0.009920s\n",
      "batch 801, train_loss 0.004900,Time used 0.012400s\n",
      "batch 802, train_loss 0.004233,Time used 0.002976s\n",
      "batch 803, train_loss 0.004970,Time used 0.002975s\n",
      "batch 804, train_loss 0.004839,Time used 0.002976s\n",
      "batch 805, train_loss 0.005693,Time used 0.002480s\n",
      "batch 806, train_loss 0.006100,Time used 0.002976s\n",
      "batch 807, train_loss 0.005618,Time used 0.002976s\n",
      "batch 808, train_loss 0.004275,Time used 0.002981s\n",
      "batch 809, train_loss 0.007272,Time used 0.002975s\n",
      "batch 810, train_loss 0.008481,Time used 0.002976s\n",
      "batch 811, train_loss 0.004953,Time used 0.002480s\n",
      "batch 812, train_loss 0.006287,Time used 0.003472s\n",
      "batch 813, train_loss 0.005599,Time used 0.002976s\n",
      "batch 814, train_loss 0.005238,Time used 0.002976s\n",
      "batch 815, train_loss 0.005851,Time used 0.002976s\n",
      "batch 816, train_loss 0.005182,Time used 0.002977s\n",
      "batch 817, train_loss 0.004831,Time used 0.002976s\n",
      "batch 818, train_loss 0.004949,Time used 0.002976s\n",
      "batch 819, train_loss 0.004646,Time used 0.003473s\n",
      "batch 820, train_loss 0.005657,Time used 0.003472s\n",
      "batch 821, train_loss 0.005642,Time used 0.002976s\n",
      "batch 822, train_loss 0.005199,Time used 0.003472s\n",
      "batch 823, train_loss 0.005829,Time used 0.002976s\n",
      "batch 824, train_loss 0.004702,Time used 0.003473s\n",
      "batch 825, train_loss 0.005392,Time used 0.002977s\n",
      "batch 826, train_loss 0.004753,Time used 0.002981s\n",
      "batch 827, train_loss 0.005126,Time used 0.002984s\n",
      "batch 828, train_loss 0.005775,Time used 0.002480s\n",
      "batch 829, train_loss 0.005182,Time used 0.002964s\n",
      "batch 830, train_loss 0.006216,Time used 0.002976s\n",
      "batch 831, train_loss 0.004871,Time used 0.002480s\n",
      "batch 832, train_loss 0.004780,Time used 0.002977s\n",
      "batch 833, train_loss 0.007510,Time used 0.002976s\n",
      "batch 834, train_loss 0.004849,Time used 0.002976s\n",
      "batch 835, train_loss 0.005662,Time used 0.002976s\n",
      "batch 836, train_loss 0.005073,Time used 0.002976s\n",
      "batch 837, train_loss 0.004532,Time used 0.003472s\n",
      "batch 838, train_loss 0.005088,Time used 0.002480s\n",
      "batch 839, train_loss 0.005917,Time used 0.002976s\n",
      "batch 840, train_loss 0.005108,Time used 0.003472s\n",
      "batch 841, train_loss 0.004957,Time used 0.002976s\n",
      "batch 842, train_loss 0.005895,Time used 0.002979s\n",
      "batch 843, train_loss 0.004460,Time used 0.002976s\n",
      "batch 844, train_loss 0.007323,Time used 0.002976s\n",
      "batch 845, train_loss 0.004606,Time used 0.002976s\n",
      "batch 846, train_loss 0.004141,Time used 0.002976s\n",
      "batch 847, train_loss 0.005369,Time used 0.002481s\n",
      "batch 848, train_loss 0.004262,Time used 0.002976s\n",
      "batch 849, train_loss 0.004616,Time used 0.002970s\n",
      "batch 850, train_loss 0.005961,Time used 0.002976s\n",
      "batch 851, train_loss 0.005339,Time used 0.002977s\n",
      "batch 852, train_loss 0.004173,Time used 0.003473s\n",
      "batch 853, train_loss 0.004921,Time used 0.004464s\n",
      "batch 854, train_loss 0.006223,Time used 0.004947s\n",
      "batch 855, train_loss 0.005432,Time used 0.012895s\n",
      "batch 856, train_loss 0.005356,Time used 0.002978s\n",
      "batch 857, train_loss 0.005486,Time used 0.002480s\n",
      "batch 858, train_loss 0.006940,Time used 0.002480s\n",
      "batch 859, train_loss 0.005134,Time used 0.002485s\n",
      "batch 860, train_loss 0.005416,Time used 0.002976s\n",
      "batch 861, train_loss 0.005403,Time used 0.002976s\n",
      "batch 862, train_loss 0.004646,Time used 0.002976s\n",
      "batch 863, train_loss 0.005211,Time used 0.002976s\n",
      "batch 864, train_loss 0.004969,Time used 0.003472s\n",
      "batch 865, train_loss 0.004767,Time used 0.002975s\n",
      "batch 866, train_loss 0.006090,Time used 0.002976s\n",
      "batch 867, train_loss 0.005493,Time used 0.002976s\n",
      "batch 868, train_loss 0.005994,Time used 0.002976s\n",
      "batch 869, train_loss 0.005202,Time used 0.002484s\n",
      "batch 870, train_loss 0.004495,Time used 0.003472s\n",
      "batch 871, train_loss 0.005271,Time used 0.002976s\n",
      "batch 872, train_loss 0.005234,Time used 0.002480s\n",
      "batch 873, train_loss 0.005425,Time used 0.002976s\n",
      "batch 874, train_loss 0.005314,Time used 0.002976s\n",
      "batch 875, train_loss 0.004345,Time used 0.002976s\n",
      "batch 876, train_loss 0.004889,Time used 0.002976s\n",
      "batch 877, train_loss 0.005089,Time used 0.002480s\n",
      "batch 878, train_loss 0.005240,Time used 0.002976s\n",
      "batch 879, train_loss 0.005638,Time used 0.002976s\n",
      "batch 880, train_loss 0.005641,Time used 0.002480s\n",
      "batch 881, train_loss 0.005618,Time used 0.003476s\n",
      "batch 882, train_loss 0.005115,Time used 0.002480s\n",
      "batch 883, train_loss 0.005033,Time used 0.002977s\n",
      "batch 884, train_loss 0.006348,Time used 0.003472s\n",
      "batch 885, train_loss 0.004608,Time used 0.002980s\n",
      "batch 886, train_loss 0.004488,Time used 0.002975s\n",
      "batch 887, train_loss 0.004896,Time used 0.003473s\n",
      "batch 888, train_loss 0.005888,Time used 0.002976s\n",
      "batch 889, train_loss 0.005087,Time used 0.013392s\n",
      "batch 890, train_loss 0.005117,Time used 0.003472s\n",
      "batch 891, train_loss 0.005096,Time used 0.002977s\n",
      "batch 892, train_loss 0.004644,Time used 0.002481s\n",
      "batch 893, train_loss 0.005172,Time used 0.002480s\n",
      "batch 894, train_loss 0.005014,Time used 0.002480s\n",
      "batch 895, train_loss 0.005795,Time used 0.002976s\n",
      "batch 896, train_loss 0.003732,Time used 0.002977s\n",
      "batch 897, train_loss 0.005585,Time used 0.002976s\n",
      "batch 898, train_loss 0.005865,Time used 0.002967s\n",
      "batch 899, train_loss 0.006630,Time used 0.002480s\n",
      "batch 900, train_loss 0.004512,Time used 0.003472s\n",
      "***************************test_batch 900, test_rmse_loss 0.072197,test_mae_loss 0.053687,test_mape_loss 17.710094,Time used 0.010416s\n",
      "batch 901, train_loss 0.005775,Time used 0.002481s\n",
      "batch 902, train_loss 0.005308,Time used 0.002976s\n",
      "batch 903, train_loss 0.005946,Time used 0.002977s\n",
      "batch 904, train_loss 0.004920,Time used 0.002976s\n",
      "batch 905, train_loss 0.006784,Time used 0.003473s\n",
      "batch 906, train_loss 0.005943,Time used 0.002971s\n",
      "batch 907, train_loss 0.004775,Time used 0.002477s\n",
      "batch 908, train_loss 0.005515,Time used 0.002480s\n",
      "batch 909, train_loss 0.004498,Time used 0.003468s\n",
      "batch 910, train_loss 0.004547,Time used 0.003472s\n",
      "batch 911, train_loss 0.005364,Time used 0.002974s\n",
      "batch 912, train_loss 0.005047,Time used 0.002976s\n",
      "batch 913, train_loss 0.005251,Time used 0.002480s\n",
      "batch 914, train_loss 0.005020,Time used 0.002976s\n",
      "batch 915, train_loss 0.006085,Time used 0.002977s\n",
      "batch 916, train_loss 0.004668,Time used 0.002976s\n",
      "batch 917, train_loss 0.005151,Time used 0.003472s\n",
      "batch 918, train_loss 0.003456,Time used 0.002480s\n",
      "batch 919, train_loss 0.004200,Time used 0.002975s\n",
      "batch 920, train_loss 0.005799,Time used 0.002984s\n",
      "batch 921, train_loss 0.005320,Time used 0.002976s\n",
      "batch 922, train_loss 0.004401,Time used 0.002977s\n",
      "batch 923, train_loss 0.005879,Time used 0.002976s\n",
      "batch 924, train_loss 0.005796,Time used 0.002976s\n",
      "batch 925, train_loss 0.004951,Time used 0.002976s\n",
      "batch 926, train_loss 0.006486,Time used 0.002976s\n",
      "batch 927, train_loss 0.004832,Time used 0.002977s\n",
      "batch 928, train_loss 0.006091,Time used 0.002976s\n",
      "batch 929, train_loss 0.005056,Time used 0.002976s\n",
      "batch 930, train_loss 0.005055,Time used 0.003464s\n",
      "batch 931, train_loss 0.004721,Time used 0.002480s\n",
      "batch 932, train_loss 0.005703,Time used 0.002976s\n",
      "batch 933, train_loss 0.006178,Time used 0.002976s\n",
      "batch 934, train_loss 0.006380,Time used 0.002480s\n",
      "batch 935, train_loss 0.005852,Time used 0.002480s\n",
      "batch 936, train_loss 0.004015,Time used 0.002976s\n",
      "batch 937, train_loss 0.004746,Time used 0.002979s\n",
      "batch 938, train_loss 0.005743,Time used 0.002971s\n",
      "batch 939, train_loss 0.004580,Time used 0.002976s\n",
      "batch 940, train_loss 0.004106,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 941, train_loss 0.005730,Time used 0.002479s\n",
      "batch 942, train_loss 0.006269,Time used 0.002976s\n",
      "batch 943, train_loss 0.006598,Time used 0.002977s\n",
      "batch 944, train_loss 0.004651,Time used 0.003469s\n",
      "batch 945, train_loss 0.004459,Time used 0.002480s\n",
      "batch 946, train_loss 0.004600,Time used 0.002480s\n",
      "batch 947, train_loss 0.005845,Time used 0.002971s\n",
      "batch 948, train_loss 0.004714,Time used 0.002976s\n",
      "batch 949, train_loss 0.005554,Time used 0.003472s\n",
      "batch 950, train_loss 0.004313,Time used 0.002976s\n",
      "batch 951, train_loss 0.004403,Time used 0.002971s\n",
      "batch 952, train_loss 0.005698,Time used 0.002480s\n",
      "batch 953, train_loss 0.005452,Time used 0.002480s\n",
      "batch 954, train_loss 0.004056,Time used 0.002976s\n",
      "batch 955, train_loss 0.004732,Time used 0.002976s\n",
      "batch 956, train_loss 0.005144,Time used 0.002976s\n",
      "batch 957, train_loss 0.005352,Time used 0.002480s\n",
      "batch 958, train_loss 0.004391,Time used 0.002976s\n",
      "batch 959, train_loss 0.005253,Time used 0.003472s\n",
      "batch 960, train_loss 0.005514,Time used 0.002978s\n",
      "batch 961, train_loss 0.004403,Time used 0.003473s\n",
      "batch 962, train_loss 0.005509,Time used 0.003472s\n",
      "batch 963, train_loss 0.004532,Time used 0.002976s\n",
      "batch 964, train_loss 0.004940,Time used 0.002480s\n",
      "batch 965, train_loss 0.005680,Time used 0.002976s\n",
      "batch 966, train_loss 0.005773,Time used 0.002975s\n",
      "batch 967, train_loss 0.005325,Time used 0.003968s\n",
      "batch 968, train_loss 0.005433,Time used 0.003472s\n",
      "batch 969, train_loss 0.005260,Time used 0.002980s\n",
      "batch 970, train_loss 0.005368,Time used 0.002976s\n",
      "batch 971, train_loss 0.004842,Time used 0.002976s\n",
      "batch 972, train_loss 0.016366,Time used 0.002976s\n",
      "batch 973, train_loss 0.005554,Time used 0.002976s\n",
      "batch 974, train_loss 0.005128,Time used 0.003472s\n",
      "batch 975, train_loss 0.006236,Time used 0.002976s\n",
      "batch 976, train_loss 0.005622,Time used 0.003472s\n",
      "batch 977, train_loss 0.005712,Time used 0.003481s\n",
      "batch 978, train_loss 0.004958,Time used 0.012896s\n",
      "batch 979, train_loss 0.005132,Time used 0.003472s\n",
      "batch 980, train_loss 0.006156,Time used 0.002976s\n",
      "batch 981, train_loss 0.007231,Time used 0.003472s\n",
      "batch 982, train_loss 0.005430,Time used 0.002975s\n",
      "batch 983, train_loss 0.005610,Time used 0.003472s\n",
      "batch 984, train_loss 0.004611,Time used 0.003472s\n",
      "batch 985, train_loss 0.006056,Time used 0.002976s\n",
      "batch 986, train_loss 0.004722,Time used 0.002480s\n",
      "batch 987, train_loss 0.005872,Time used 0.002976s\n",
      "batch 988, train_loss 0.005216,Time used 0.002976s\n",
      "batch 989, train_loss 0.004889,Time used 0.002976s\n",
      "batch 990, train_loss 0.004434,Time used 0.002480s\n",
      "batch 991, train_loss 0.005836,Time used 0.003473s\n",
      "batch 992, train_loss 0.005052,Time used 0.002976s\n",
      "batch 993, train_loss 0.005359,Time used 0.002976s\n",
      "batch 994, train_loss 0.005341,Time used 0.002976s\n",
      "batch 995, train_loss 0.005512,Time used 0.002975s\n",
      "batch 996, train_loss 0.004565,Time used 0.002976s\n",
      "batch 997, train_loss 0.004287,Time used 0.003472s\n",
      "batch 998, train_loss 0.005540,Time used 0.002976s\n",
      "batch 999, train_loss 0.005364,Time used 0.002971s\n",
      "batch 1000, train_loss 0.004009,Time used 0.003472s\n",
      "***************************test_batch 1000, test_rmse_loss 0.071539,test_mae_loss 0.053247,test_mape_loss 17.534602,Time used 0.010912s\n",
      "batch 1001, train_loss 0.004055,Time used 0.003472s\n",
      "batch 1002, train_loss 0.005248,Time used 0.002976s\n",
      "batch 1003, train_loss 0.004465,Time used 0.002976s\n",
      "batch 1004, train_loss 0.005852,Time used 0.003472s\n",
      "batch 1005, train_loss 0.005430,Time used 0.002480s\n",
      "batch 1006, train_loss 0.006340,Time used 0.002480s\n",
      "batch 1007, train_loss 0.005404,Time used 0.002976s\n",
      "batch 1008, train_loss 0.005679,Time used 0.002976s\n",
      "batch 1009, train_loss 0.005458,Time used 0.003473s\n",
      "batch 1010, train_loss 0.005741,Time used 0.002976s\n",
      "batch 1011, train_loss 0.004258,Time used 0.002480s\n",
      "batch 1012, train_loss 0.005083,Time used 0.002977s\n",
      "batch 1013, train_loss 0.003718,Time used 0.002976s\n",
      "batch 1014, train_loss 0.004782,Time used 0.003969s\n",
      "batch 1015, train_loss 0.004508,Time used 0.003472s\n",
      "batch 1016, train_loss 0.004167,Time used 0.002976s\n",
      "batch 1017, train_loss 0.004774,Time used 0.002967s\n",
      "batch 1018, train_loss 0.005354,Time used 0.002976s\n",
      "batch 1019, train_loss 0.004031,Time used 0.003968s\n",
      "batch 1020, train_loss 0.004479,Time used 0.003472s\n",
      "batch 1021, train_loss 0.005718,Time used 0.002976s\n",
      "batch 1022, train_loss 0.004735,Time used 0.003464s\n",
      "batch 1023, train_loss 0.004941,Time used 0.002976s\n",
      "batch 1024, train_loss 0.005265,Time used 0.002480s\n",
      "batch 1025, train_loss 0.004994,Time used 0.003472s\n",
      "batch 1026, train_loss 0.009551,Time used 0.002976s\n",
      "batch 1027, train_loss 0.004547,Time used 0.002979s\n",
      "batch 1028, train_loss 0.004443,Time used 0.003472s\n",
      "batch 1029, train_loss 0.006022,Time used 0.002976s\n",
      "batch 1030, train_loss 0.004519,Time used 0.002976s\n",
      "batch 1031, train_loss 0.005861,Time used 0.003473s\n",
      "batch 1032, train_loss 0.005621,Time used 0.002976s\n",
      "batch 1033, train_loss 0.005511,Time used 0.002975s\n",
      "batch 1034, train_loss 0.005623,Time used 0.002976s\n",
      "batch 1035, train_loss 0.006854,Time used 0.002975s\n",
      "batch 1036, train_loss 0.004864,Time used 0.002976s\n",
      "batch 1037, train_loss 0.004818,Time used 0.002976s\n",
      "batch 1038, train_loss 0.006205,Time used 0.002977s\n",
      "batch 1039, train_loss 0.004801,Time used 0.002975s\n",
      "batch 1040, train_loss 0.005268,Time used 0.002979s\n",
      "batch 1041, train_loss 0.004977,Time used 0.002972s\n",
      "batch 1042, train_loss 0.003879,Time used 0.002480s\n",
      "batch 1043, train_loss 0.004691,Time used 0.003472s\n",
      "batch 1044, train_loss 0.005231,Time used 0.002975s\n",
      "batch 1045, train_loss 0.005066,Time used 0.002976s\n",
      "batch 1046, train_loss 0.004982,Time used 0.003472s\n",
      "batch 1047, train_loss 0.005765,Time used 0.002972s\n",
      "batch 1048, train_loss 0.005041,Time used 0.002976s\n",
      "batch 1049, train_loss 0.004927,Time used 0.002976s\n",
      "batch 1050, train_loss 0.005096,Time used 0.002976s\n",
      "batch 1051, train_loss 0.005793,Time used 0.002976s\n",
      "batch 1052, train_loss 0.003437,Time used 0.003472s\n",
      "batch 1053, train_loss 0.004536,Time used 0.003472s\n",
      "batch 1054, train_loss 0.005803,Time used 0.002480s\n",
      "batch 1055, train_loss 0.005685,Time used 0.002976s\n",
      "batch 1056, train_loss 0.005382,Time used 0.002967s\n",
      "batch 1057, train_loss 0.006192,Time used 0.002976s\n",
      "batch 1058, train_loss 0.005180,Time used 0.002976s\n",
      "batch 1059, train_loss 0.006165,Time used 0.002976s\n",
      "batch 1060, train_loss 0.004488,Time used 0.002472s\n",
      "batch 1061, train_loss 0.005290,Time used 0.003472s\n",
      "batch 1062, train_loss 0.004045,Time used 0.002976s\n",
      "batch 1063, train_loss 0.005062,Time used 0.003472s\n",
      "batch 1064, train_loss 0.005275,Time used 0.014384s\n",
      "batch 1065, train_loss 0.005059,Time used 0.003816s\n",
      "batch 1066, train_loss 0.005220,Time used 0.002976s\n",
      "batch 1067, train_loss 0.005626,Time used 0.002978s\n",
      "batch 1068, train_loss 0.004183,Time used 0.002976s\n",
      "batch 1069, train_loss 0.004432,Time used 0.002976s\n",
      "batch 1070, train_loss 0.004421,Time used 0.002976s\n",
      "batch 1071, train_loss 0.006131,Time used 0.002976s\n",
      "batch 1072, train_loss 0.005281,Time used 0.002976s\n",
      "batch 1073, train_loss 0.004500,Time used 0.003473s\n",
      "batch 1074, train_loss 0.005605,Time used 0.002977s\n",
      "batch 1075, train_loss 0.004221,Time used 0.004463s\n",
      "batch 1076, train_loss 0.005386,Time used 0.003968s\n",
      "batch 1077, train_loss 0.004137,Time used 0.003472s\n",
      "batch 1078, train_loss 0.004684,Time used 0.014384s\n",
      "batch 1079, train_loss 0.005276,Time used 0.002975s\n",
      "batch 1080, train_loss 0.003966,Time used 0.004464s\n",
      "batch 1081, train_loss 0.004235,Time used 0.002484s\n",
      "batch 1082, train_loss 0.004880,Time used 0.002976s\n",
      "batch 1083, train_loss 0.004771,Time used 0.002480s\n",
      "batch 1084, train_loss 0.004543,Time used 0.003472s\n",
      "batch 1085, train_loss 0.004364,Time used 0.002975s\n",
      "batch 1086, train_loss 0.004315,Time used 0.002976s\n",
      "batch 1087, train_loss 0.005147,Time used 0.002966s\n",
      "batch 1088, train_loss 0.005366,Time used 0.002975s\n",
      "batch 1089, train_loss 0.005682,Time used 0.003472s\n",
      "batch 1090, train_loss 0.004683,Time used 0.003480s\n",
      "batch 1091, train_loss 0.004642,Time used 0.002480s\n",
      "batch 1092, train_loss 0.004574,Time used 0.002976s\n",
      "batch 1093, train_loss 0.005641,Time used 0.003472s\n",
      "batch 1094, train_loss 0.004506,Time used 0.002976s\n",
      "batch 1095, train_loss 0.005784,Time used 0.002979s\n",
      "batch 1096, train_loss 0.004936,Time used 0.003468s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1097, train_loss 0.004778,Time used 0.002976s\n",
      "batch 1098, train_loss 0.004805,Time used 0.003472s\n",
      "batch 1099, train_loss 0.003941,Time used 0.003968s\n",
      "batch 1100, train_loss 0.005867,Time used 0.003473s\n",
      "***************************test_batch 1100, test_rmse_loss 0.070904,test_mae_loss 0.052299,test_mape_loss 16.795571,Time used 0.011408s\n",
      "batch 1101, train_loss 0.004931,Time used 0.002976s\n",
      "batch 1102, train_loss 0.004675,Time used 0.002977s\n",
      "batch 1103, train_loss 0.004929,Time used 0.002976s\n",
      "batch 1104, train_loss 0.005539,Time used 0.004476s\n",
      "batch 1105, train_loss 0.005394,Time used 0.003472s\n",
      "batch 1106, train_loss 0.004709,Time used 0.002976s\n",
      "batch 1107, train_loss 0.004990,Time used 0.003968s\n",
      "batch 1108, train_loss 0.005728,Time used 0.002976s\n",
      "batch 1109, train_loss 0.006342,Time used 0.002975s\n",
      "batch 1110, train_loss 0.005649,Time used 0.002976s\n",
      "batch 1111, train_loss 0.005308,Time used 0.002976s\n",
      "batch 1112, train_loss 0.006119,Time used 0.002480s\n",
      "batch 1113, train_loss 0.005399,Time used 0.002976s\n",
      "batch 1114, train_loss 0.005138,Time used 0.002975s\n",
      "batch 1115, train_loss 0.004787,Time used 0.003968s\n",
      "batch 1116, train_loss 0.003965,Time used 0.002976s\n",
      "batch 1117, train_loss 0.005747,Time used 0.002481s\n",
      "batch 1118, train_loss 0.006011,Time used 0.002480s\n",
      "batch 1119, train_loss 0.005694,Time used 0.003473s\n",
      "batch 1120, train_loss 0.003968,Time used 0.002981s\n",
      "batch 1121, train_loss 0.004740,Time used 0.003473s\n",
      "batch 1122, train_loss 0.004619,Time used 0.002976s\n",
      "batch 1123, train_loss 0.004725,Time used 0.002985s\n",
      "batch 1124, train_loss 0.005420,Time used 0.002480s\n",
      "batch 1125, train_loss 0.005638,Time used 0.002975s\n",
      "batch 1126, train_loss 0.003691,Time used 0.004465s\n",
      "batch 1127, train_loss 0.004473,Time used 0.002976s\n",
      "batch 1128, train_loss 0.005692,Time used 0.002975s\n",
      "batch 1129, train_loss 0.004900,Time used 0.003968s\n",
      "batch 1130, train_loss 0.004487,Time used 0.002480s\n",
      "batch 1131, train_loss 0.005854,Time used 0.002976s\n",
      "batch 1132, train_loss 0.005491,Time used 0.002976s\n",
      "batch 1133, train_loss 0.005611,Time used 0.002975s\n",
      "batch 1134, train_loss 0.004144,Time used 0.002970s\n",
      "batch 1135, train_loss 0.004253,Time used 0.002976s\n",
      "batch 1136, train_loss 0.004589,Time used 0.003472s\n",
      "batch 1137, train_loss 0.006418,Time used 0.002478s\n",
      "batch 1138, train_loss 0.005406,Time used 0.002976s\n",
      "batch 1139, train_loss 0.004158,Time used 0.002975s\n",
      "batch 1140, train_loss 0.004562,Time used 0.003471s\n",
      "batch 1141, train_loss 0.005344,Time used 0.002480s\n",
      "batch 1142, train_loss 0.005539,Time used 0.002977s\n",
      "batch 1143, train_loss 0.004618,Time used 0.002977s\n",
      "batch 1144, train_loss 0.005966,Time used 0.002977s\n",
      "batch 1145, train_loss 0.005936,Time used 0.003472s\n",
      "batch 1146, train_loss 0.005473,Time used 0.003968s\n",
      "batch 1147, train_loss 0.004444,Time used 0.003969s\n",
      "batch 1148, train_loss 0.003861,Time used 0.003472s\n",
      "batch 1149, train_loss 0.004936,Time used 0.013887s\n",
      "batch 1150, train_loss 0.005295,Time used 0.003472s\n",
      "batch 1151, train_loss 0.005093,Time used 0.003472s\n",
      "batch 1152, train_loss 0.005293,Time used 0.003471s\n",
      "batch 1153, train_loss 0.005202,Time used 0.002976s\n",
      "batch 1154, train_loss 0.004874,Time used 0.003471s\n",
      "batch 1155, train_loss 0.005454,Time used 0.002480s\n",
      "batch 1156, train_loss 0.006352,Time used 0.002976s\n",
      "batch 1157, train_loss 0.004658,Time used 0.002977s\n",
      "batch 1158, train_loss 0.005126,Time used 0.002976s\n",
      "batch 1159, train_loss 0.004653,Time used 0.002975s\n",
      "batch 1160, train_loss 0.004642,Time used 0.002976s\n",
      "batch 1161, train_loss 0.004590,Time used 0.002976s\n",
      "batch 1162, train_loss 0.003940,Time used 0.003467s\n",
      "batch 1163, train_loss 0.004917,Time used 0.002480s\n",
      "batch 1164, train_loss 0.005499,Time used 0.002976s\n",
      "batch 1165, train_loss 0.004929,Time used 0.002976s\n",
      "batch 1166, train_loss 0.003906,Time used 0.003472s\n",
      "batch 1167, train_loss 0.005404,Time used 0.003471s\n",
      "batch 1168, train_loss 0.004615,Time used 0.003472s\n",
      "batch 1169, train_loss 0.005177,Time used 0.002976s\n",
      "batch 1170, train_loss 0.005806,Time used 0.003472s\n",
      "batch 1171, train_loss 0.004960,Time used 0.003472s\n",
      "batch 1172, train_loss 0.004689,Time used 0.002480s\n",
      "batch 1173, train_loss 0.004418,Time used 0.002976s\n",
      "batch 1174, train_loss 0.005085,Time used 0.002480s\n",
      "batch 1175, train_loss 0.005532,Time used 0.002976s\n",
      "batch 1176, train_loss 0.004913,Time used 0.002976s\n",
      "batch 1177, train_loss 0.005583,Time used 0.003472s\n",
      "batch 1178, train_loss 0.005482,Time used 0.002976s\n",
      "batch 1179, train_loss 0.004194,Time used 0.002975s\n",
      "batch 1180, train_loss 0.004916,Time used 0.002976s\n",
      "batch 1181, train_loss 0.004545,Time used 0.002484s\n",
      "batch 1182, train_loss 0.004283,Time used 0.002969s\n",
      "batch 1183, train_loss 0.006109,Time used 0.002975s\n",
      "batch 1184, train_loss 0.004414,Time used 0.003472s\n",
      "batch 1185, train_loss 0.006408,Time used 0.003472s\n",
      "batch 1186, train_loss 0.004444,Time used 0.002975s\n",
      "batch 1187, train_loss 0.004083,Time used 0.003472s\n",
      "batch 1188, train_loss 0.003223,Time used 0.003472s\n",
      "batch 1189, train_loss 0.004371,Time used 0.002976s\n",
      "batch 1190, train_loss 0.004900,Time used 0.002976s\n",
      "batch 1191, train_loss 0.005684,Time used 0.002980s\n",
      "batch 1192, train_loss 0.005413,Time used 0.003472s\n",
      "batch 1193, train_loss 0.005150,Time used 0.002976s\n",
      "batch 1194, train_loss 0.004885,Time used 0.002975s\n",
      "batch 1195, train_loss 0.005326,Time used 0.002976s\n",
      "batch 1196, train_loss 0.003175,Time used 0.002976s\n",
      "batch 1197, train_loss 0.005498,Time used 0.002977s\n",
      "batch 1198, train_loss 0.004964,Time used 0.003472s\n",
      "batch 1199, train_loss 0.004761,Time used 0.002976s\n",
      "batch 1200, train_loss 0.004576,Time used 0.002976s\n",
      "***************************test_batch 1200, test_rmse_loss 0.070227,test_mae_loss 0.052281,test_mape_loss 17.558540,Time used 0.010913s\n",
      "batch 1201, train_loss 0.003979,Time used 0.002480s\n",
      "batch 1202, train_loss 0.004829,Time used 0.003473s\n",
      "batch 1203, train_loss 0.005116,Time used 0.003472s\n",
      "batch 1204, train_loss 0.004763,Time used 0.002976s\n",
      "batch 1205, train_loss 0.005266,Time used 0.003472s\n",
      "batch 1206, train_loss 0.005488,Time used 0.002976s\n",
      "batch 1207, train_loss 0.004612,Time used 0.003472s\n",
      "batch 1208, train_loss 0.005590,Time used 0.004050s\n",
      "batch 1209, train_loss 0.005140,Time used 0.002976s\n",
      "batch 1210, train_loss 0.005031,Time used 0.003473s\n",
      "batch 1211, train_loss 0.003982,Time used 0.003472s\n",
      "batch 1212, train_loss 0.005019,Time used 0.002975s\n",
      "batch 1213, train_loss 0.004467,Time used 0.002975s\n",
      "batch 1214, train_loss 0.004427,Time used 0.003472s\n",
      "batch 1215, train_loss 0.004590,Time used 0.002977s\n",
      "batch 1216, train_loss 0.004780,Time used 0.002976s\n",
      "batch 1217, train_loss 0.004213,Time used 0.003471s\n",
      "batch 1218, train_loss 0.004728,Time used 0.002976s\n",
      "batch 1219, train_loss 0.004532,Time used 0.003472s\n",
      "batch 1220, train_loss 0.004548,Time used 0.003976s\n",
      "batch 1221, train_loss 0.005357,Time used 0.002976s\n",
      "batch 1222, train_loss 0.004790,Time used 0.002976s\n",
      "batch 1223, train_loss 0.005018,Time used 0.003968s\n",
      "batch 1224, train_loss 0.004584,Time used 0.003472s\n",
      "batch 1225, train_loss 0.003904,Time used 0.002975s\n",
      "batch 1226, train_loss 0.005004,Time used 0.002976s\n",
      "batch 1227, train_loss 0.005484,Time used 0.002481s\n",
      "batch 1228, train_loss 0.004944,Time used 0.003472s\n",
      "batch 1229, train_loss 0.004420,Time used 0.003472s\n",
      "batch 1230, train_loss 0.005010,Time used 0.003472s\n",
      "batch 1231, train_loss 0.005196,Time used 0.003472s\n",
      "batch 1232, train_loss 0.005240,Time used 0.003472s\n",
      "batch 1233, train_loss 0.006199,Time used 0.015368s\n",
      "batch 1234, train_loss 0.007129,Time used 0.003472s\n",
      "batch 1235, train_loss 0.005695,Time used 0.002977s\n",
      "batch 1236, train_loss 0.004108,Time used 0.002976s\n",
      "batch 1237, train_loss 0.004891,Time used 0.003472s\n",
      "batch 1238, train_loss 0.006665,Time used 0.002976s\n",
      "batch 1239, train_loss 0.004823,Time used 0.002976s\n",
      "batch 1240, train_loss 0.005193,Time used 0.002976s\n",
      "batch 1241, train_loss 0.004642,Time used 0.002976s\n",
      "batch 1242, train_loss 0.006132,Time used 0.002976s\n",
      "batch 1243, train_loss 0.004376,Time used 0.002976s\n",
      "batch 1244, train_loss 0.005175,Time used 0.003472s\n",
      "batch 1245, train_loss 0.004783,Time used 0.002976s\n",
      "batch 1246, train_loss 0.004076,Time used 0.002976s\n",
      "batch 1247, train_loss 0.005143,Time used 0.003472s\n",
      "batch 1248, train_loss 0.004679,Time used 0.003968s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1249, train_loss 0.004703,Time used 0.003472s\n",
      "batch 1250, train_loss 0.005436,Time used 0.002976s\n",
      "batch 1251, train_loss 0.005203,Time used 0.003472s\n",
      "batch 1252, train_loss 0.004377,Time used 0.003472s\n",
      "batch 1253, train_loss 0.005476,Time used 0.003471s\n",
      "batch 1254, train_loss 0.004883,Time used 0.002975s\n",
      "batch 1255, train_loss 0.004720,Time used 0.003472s\n",
      "batch 1256, train_loss 0.003941,Time used 0.002480s\n",
      "batch 1257, train_loss 0.004446,Time used 0.002976s\n",
      "batch 1258, train_loss 0.005032,Time used 0.003471s\n",
      "batch 1259, train_loss 0.005453,Time used 0.003472s\n",
      "batch 1260, train_loss 0.004811,Time used 0.003472s\n",
      "batch 1261, train_loss 0.005928,Time used 0.002976s\n",
      "batch 1262, train_loss 0.005314,Time used 0.002976s\n",
      "batch 1263, train_loss 0.005829,Time used 0.002976s\n",
      "batch 1264, train_loss 0.005398,Time used 0.002977s\n",
      "batch 1265, train_loss 0.005044,Time used 0.002976s\n",
      "batch 1266, train_loss 0.004354,Time used 0.004464s\n",
      "batch 1267, train_loss 0.005141,Time used 0.003472s\n",
      "batch 1268, train_loss 0.005464,Time used 0.002976s\n",
      "batch 1269, train_loss 0.005104,Time used 0.003462s\n",
      "batch 1270, train_loss 0.005728,Time used 0.002975s\n",
      "batch 1271, train_loss 0.004633,Time used 0.002972s\n",
      "batch 1272, train_loss 0.004932,Time used 0.003968s\n",
      "batch 1273, train_loss 0.004052,Time used 0.002976s\n",
      "batch 1274, train_loss 0.004463,Time used 0.002976s\n",
      "batch 1275, train_loss 0.004502,Time used 0.003474s\n",
      "batch 1276, train_loss 0.004812,Time used 0.002977s\n",
      "batch 1277, train_loss 0.004495,Time used 0.004960s\n",
      "batch 1278, train_loss 0.004579,Time used 0.003472s\n",
      "batch 1279, train_loss 0.005304,Time used 0.002976s\n",
      "batch 1280, train_loss 0.003040,Time used 0.002976s\n",
      "batch 1281, train_loss 0.005323,Time used 0.012896s\n",
      "batch 1282, train_loss 0.005191,Time used 0.003480s\n",
      "batch 1283, train_loss 0.004042,Time used 0.002976s\n",
      "batch 1284, train_loss 0.005063,Time used 0.003477s\n",
      "batch 1285, train_loss 0.004425,Time used 0.002975s\n",
      "batch 1286, train_loss 0.004462,Time used 0.003472s\n",
      "batch 1287, train_loss 0.005789,Time used 0.002976s\n",
      "batch 1288, train_loss 0.006160,Time used 0.002976s\n",
      "batch 1289, train_loss 0.005326,Time used 0.003470s\n",
      "batch 1290, train_loss 0.004650,Time used 0.002976s\n",
      "batch 1291, train_loss 0.004476,Time used 0.002976s\n",
      "batch 1292, train_loss 0.004549,Time used 0.003472s\n",
      "batch 1293, train_loss 0.005781,Time used 0.002976s\n",
      "batch 1294, train_loss 0.004520,Time used 0.002976s\n",
      "batch 1295, train_loss 0.004391,Time used 0.002480s\n",
      "batch 1296, train_loss 0.004639,Time used 0.002976s\n",
      "batch 1297, train_loss 0.004352,Time used 0.002971s\n",
      "batch 1298, train_loss 0.005302,Time used 0.003460s\n",
      "batch 1299, train_loss 0.004493,Time used 0.002976s\n",
      "batch 1300, train_loss 0.004720,Time used 0.003472s\n",
      "***************************test_batch 1300, test_rmse_loss 0.069639,test_mae_loss 0.051963,test_mape_loss 17.844849,Time used 0.012400s\n",
      "batch 1301, train_loss 0.005491,Time used 0.003479s\n",
      "batch 1302, train_loss 0.004274,Time used 0.002976s\n",
      "batch 1303, train_loss 0.004071,Time used 0.003463s\n",
      "batch 1304, train_loss 0.004821,Time used 0.003473s\n",
      "batch 1305, train_loss 0.003745,Time used 0.002976s\n",
      "batch 1306, train_loss 0.005203,Time used 0.002963s\n",
      "batch 1307, train_loss 0.005313,Time used 0.002975s\n",
      "batch 1308, train_loss 0.004422,Time used 0.003464s\n",
      "batch 1309, train_loss 0.004505,Time used 0.003464s\n",
      "batch 1310, train_loss 0.005342,Time used 0.002976s\n",
      "batch 1311, train_loss 0.005115,Time used 0.004960s\n",
      "batch 1312, train_loss 0.003993,Time used 0.003968s\n",
      "batch 1313, train_loss 0.005481,Time used 0.003968s\n",
      "batch 1314, train_loss 0.004731,Time used 0.003965s\n",
      "batch 1315, train_loss 0.005311,Time used 0.011904s\n",
      "batch 1316, train_loss 0.005540,Time used 0.003472s\n",
      "batch 1317, train_loss 0.005035,Time used 0.003472s\n",
      "batch 1318, train_loss 0.004482,Time used 0.003469s\n",
      "batch 1319, train_loss 0.004218,Time used 0.002976s\n",
      "batch 1320, train_loss 0.005773,Time used 0.003467s\n",
      "batch 1321, train_loss 0.004528,Time used 0.003468s\n",
      "batch 1322, train_loss 0.005507,Time used 0.003473s\n",
      "batch 1323, train_loss 0.004118,Time used 0.003467s\n",
      "batch 1324, train_loss 0.004546,Time used 0.003480s\n",
      "batch 1325, train_loss 0.005104,Time used 0.003473s\n",
      "batch 1326, train_loss 0.004516,Time used 0.002964s\n",
      "batch 1327, train_loss 0.004626,Time used 0.003472s\n",
      "batch 1328, train_loss 0.004761,Time used 0.003472s\n",
      "batch 1329, train_loss 0.005996,Time used 0.003471s\n",
      "batch 1330, train_loss 0.004682,Time used 0.003472s\n",
      "batch 1331, train_loss 0.004680,Time used 0.002976s\n",
      "batch 1332, train_loss 0.004246,Time used 0.003468s\n",
      "batch 1333, train_loss 0.005556,Time used 0.003968s\n",
      "batch 1334, train_loss 0.004350,Time used 0.004473s\n",
      "batch 1335, train_loss 0.006017,Time used 0.003472s\n",
      "batch 1336, train_loss 0.004352,Time used 0.002976s\n",
      "batch 1337, train_loss 0.004892,Time used 0.002976s\n",
      "batch 1338, train_loss 0.005143,Time used 0.003472s\n",
      "batch 1339, train_loss 0.006743,Time used 0.002978s\n",
      "batch 1340, train_loss 0.004239,Time used 0.002976s\n",
      "batch 1341, train_loss 0.004933,Time used 0.002976s\n",
      "batch 1342, train_loss 0.004750,Time used 0.002972s\n",
      "batch 1343, train_loss 0.004129,Time used 0.002976s\n",
      "batch 1344, train_loss 0.005019,Time used 0.003473s\n",
      "batch 1345, train_loss 0.004962,Time used 0.003473s\n",
      "batch 1346, train_loss 0.005272,Time used 0.002976s\n",
      "batch 1347, train_loss 0.004813,Time used 0.003472s\n",
      "batch 1348, train_loss 0.003961,Time used 0.002476s\n",
      "batch 1349, train_loss 0.004232,Time used 0.002481s\n",
      "batch 1350, train_loss 0.003133,Time used 0.002480s\n",
      "batch 1351, train_loss 0.006359,Time used 0.002971s\n",
      "batch 1352, train_loss 0.004256,Time used 0.002976s\n",
      "batch 1353, train_loss 0.004006,Time used 0.003472s\n",
      "batch 1354, train_loss 0.005032,Time used 0.002976s\n",
      "batch 1355, train_loss 0.004238,Time used 0.002480s\n",
      "batch 1356, train_loss 0.004682,Time used 0.003472s\n",
      "batch 1357, train_loss 0.004226,Time used 0.002976s\n",
      "batch 1358, train_loss 0.004295,Time used 0.002976s\n",
      "batch 1359, train_loss 0.004883,Time used 0.002480s\n",
      "batch 1360, train_loss 0.004276,Time used 0.002975s\n",
      "batch 1361, train_loss 0.004420,Time used 0.003968s\n",
      "batch 1362, train_loss 0.003960,Time used 0.002976s\n",
      "batch 1363, train_loss 0.005309,Time used 0.002983s\n",
      "batch 1364, train_loss 0.003870,Time used 0.002976s\n",
      "batch 1365, train_loss 0.005652,Time used 0.002975s\n",
      "batch 1366, train_loss 0.005430,Time used 0.002482s\n",
      "batch 1367, train_loss 0.004233,Time used 0.002976s\n",
      "batch 1368, train_loss 0.004299,Time used 0.003471s\n",
      "batch 1369, train_loss 0.004342,Time used 0.003481s\n",
      "batch 1370, train_loss 0.004406,Time used 0.002976s\n",
      "batch 1371, train_loss 0.004820,Time used 0.002483s\n",
      "batch 1372, train_loss 0.003597,Time used 0.002976s\n",
      "batch 1373, train_loss 0.004159,Time used 0.003472s\n",
      "batch 1374, train_loss 0.004506,Time used 0.002976s\n",
      "batch 1375, train_loss 0.006166,Time used 0.002976s\n",
      "batch 1376, train_loss 0.004650,Time used 0.002967s\n",
      "batch 1377, train_loss 0.004560,Time used 0.002976s\n",
      "batch 1378, train_loss 0.004518,Time used 0.002975s\n",
      "batch 1379, train_loss 0.004461,Time used 0.002480s\n",
      "batch 1380, train_loss 0.004613,Time used 0.003472s\n",
      "batch 1381, train_loss 0.005686,Time used 0.002968s\n",
      "batch 1382, train_loss 0.005177,Time used 0.002481s\n",
      "batch 1383, train_loss 0.006342,Time used 0.002976s\n",
      "batch 1384, train_loss 0.004902,Time used 0.002479s\n",
      "batch 1385, train_loss 0.004771,Time used 0.002977s\n",
      "batch 1386, train_loss 0.003948,Time used 0.002975s\n",
      "batch 1387, train_loss 0.004640,Time used 0.002976s\n",
      "batch 1388, train_loss 0.003735,Time used 0.003468s\n",
      "batch 1389, train_loss 0.004792,Time used 0.002976s\n",
      "batch 1390, train_loss 0.004759,Time used 0.002479s\n",
      "batch 1391, train_loss 0.004575,Time used 0.002976s\n",
      "batch 1392, train_loss 0.006597,Time used 0.003472s\n",
      "batch 1393, train_loss 0.005222,Time used 0.003471s\n",
      "batch 1394, train_loss 0.005222,Time used 0.002480s\n",
      "batch 1395, train_loss 0.004302,Time used 0.002976s\n",
      "batch 1396, train_loss 0.006118,Time used 0.002480s\n",
      "batch 1397, train_loss 0.004987,Time used 0.003968s\n",
      "batch 1398, train_loss 0.004436,Time used 0.002975s\n",
      "batch 1399, train_loss 0.005011,Time used 0.002976s\n",
      "batch 1400, train_loss 0.005160,Time used 0.012895s\n",
      "***************************test_batch 1400, test_rmse_loss 0.068916,test_mae_loss 0.051030,test_mape_loss 16.817262,Time used 0.012897s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1401, train_loss 0.005713,Time used 0.002976s\n",
      "batch 1402, train_loss 0.004504,Time used 0.003968s\n",
      "batch 1403, train_loss 0.004793,Time used 0.003472s\n",
      "batch 1404, train_loss 0.011674,Time used 0.002976s\n",
      "batch 1405, train_loss 0.004797,Time used 0.002975s\n",
      "batch 1406, train_loss 0.004923,Time used 0.002976s\n",
      "batch 1407, train_loss 0.004288,Time used 0.002481s\n",
      "batch 1408, train_loss 0.005586,Time used 0.002976s\n",
      "batch 1409, train_loss 0.004613,Time used 0.003963s\n",
      "batch 1410, train_loss 0.004864,Time used 0.002976s\n",
      "batch 1411, train_loss 0.004978,Time used 0.002982s\n",
      "batch 1412, train_loss 0.004657,Time used 0.002480s\n",
      "batch 1413, train_loss 0.004407,Time used 0.002975s\n",
      "batch 1414, train_loss 0.004194,Time used 0.002976s\n",
      "batch 1415, train_loss 0.005438,Time used 0.002976s\n",
      "batch 1416, train_loss 0.004275,Time used 0.002480s\n",
      "batch 1417, train_loss 0.004657,Time used 0.002975s\n",
      "batch 1418, train_loss 0.004646,Time used 0.002479s\n",
      "batch 1419, train_loss 0.005281,Time used 0.002976s\n",
      "batch 1420, train_loss 0.005003,Time used 0.003968s\n",
      "batch 1421, train_loss 0.004767,Time used 0.002971s\n",
      "batch 1422, train_loss 0.005007,Time used 0.002976s\n",
      "batch 1423, train_loss 0.004645,Time used 0.002480s\n",
      "batch 1424, train_loss 0.004728,Time used 0.002968s\n",
      "batch 1425, train_loss 0.005189,Time used 0.002977s\n",
      "batch 1426, train_loss 0.005463,Time used 0.002490s\n",
      "batch 1427, train_loss 0.006354,Time used 0.002977s\n",
      "batch 1428, train_loss 0.004593,Time used 0.002480s\n",
      "batch 1429, train_loss 0.004301,Time used 0.002480s\n",
      "batch 1430, train_loss 0.004008,Time used 0.002976s\n",
      "batch 1431, train_loss 0.004604,Time used 0.003472s\n",
      "batch 1432, train_loss 0.004777,Time used 0.002976s\n",
      "batch 1433, train_loss 0.004250,Time used 0.003968s\n",
      "batch 1434, train_loss 0.006382,Time used 0.002976s\n",
      "batch 1435, train_loss 0.004477,Time used 0.002976s\n",
      "batch 1436, train_loss 0.004396,Time used 0.002480s\n",
      "batch 1437, train_loss 0.006694,Time used 0.002976s\n",
      "batch 1438, train_loss 0.004490,Time used 0.002975s\n",
      "batch 1439, train_loss 0.005232,Time used 0.002967s\n",
      "batch 1440, train_loss 0.004174,Time used 0.002979s\n",
      "batch 1441, train_loss 0.004244,Time used 0.002966s\n",
      "batch 1442, train_loss 0.004735,Time used 0.002480s\n",
      "batch 1443, train_loss 0.004239,Time used 0.002966s\n",
      "batch 1444, train_loss 0.004063,Time used 0.003471s\n",
      "batch 1445, train_loss 0.004209,Time used 0.003472s\n",
      "batch 1446, train_loss 0.005184,Time used 0.002976s\n",
      "batch 1447, train_loss 0.005387,Time used 0.002975s\n",
      "batch 1448, train_loss 0.004348,Time used 0.002976s\n",
      "batch 1449, train_loss 0.003936,Time used 0.002480s\n",
      "batch 1450, train_loss 0.004518,Time used 0.002480s\n",
      "batch 1451, train_loss 0.004958,Time used 0.002976s\n",
      "batch 1452, train_loss 0.004198,Time used 0.002976s\n",
      "batch 1453, train_loss 0.004204,Time used 0.002971s\n",
      "batch 1454, train_loss 0.004686,Time used 0.002976s\n",
      "batch 1455, train_loss 0.004297,Time used 0.002980s\n",
      "batch 1456, train_loss 0.005124,Time used 0.002480s\n",
      "batch 1457, train_loss 0.004118,Time used 0.002976s\n",
      "batch 1458, train_loss 0.006413,Time used 0.002976s\n",
      "batch 1459, train_loss 0.005150,Time used 0.002480s\n",
      "batch 1460, train_loss 0.004336,Time used 0.002976s\n",
      "batch 1461, train_loss 0.003601,Time used 0.003472s\n",
      "batch 1462, train_loss 0.006091,Time used 0.002480s\n",
      "batch 1463, train_loss 0.004866,Time used 0.003472s\n",
      "batch 1464, train_loss 0.005755,Time used 0.002976s\n",
      "batch 1465, train_loss 0.004321,Time used 0.002480s\n",
      "batch 1466, train_loss 0.005675,Time used 0.002480s\n",
      "batch 1467, train_loss 0.004648,Time used 0.002977s\n",
      "batch 1468, train_loss 0.005622,Time used 0.002976s\n",
      "batch 1469, train_loss 0.004489,Time used 0.002981s\n",
      "batch 1470, train_loss 0.004809,Time used 0.002976s\n",
      "batch 1471, train_loss 0.004800,Time used 0.003472s\n",
      "batch 1472, train_loss 0.004018,Time used 0.003473s\n",
      "batch 1473, train_loss 0.005016,Time used 0.002479s\n",
      "batch 1474, train_loss 0.005294,Time used 0.002484s\n",
      "batch 1475, train_loss 0.005107,Time used 0.003472s\n",
      "batch 1476, train_loss 0.003618,Time used 0.002480s\n",
      "batch 1477, train_loss 0.004141,Time used 0.002976s\n",
      "batch 1478, train_loss 0.004326,Time used 0.002976s\n",
      "batch 1479, train_loss 0.004020,Time used 0.002480s\n",
      "batch 1480, train_loss 0.004327,Time used 0.003968s\n",
      "batch 1481, train_loss 0.005158,Time used 0.002976s\n",
      "batch 1482, train_loss 0.004933,Time used 0.002976s\n",
      "batch 1483, train_loss 0.003977,Time used 0.002480s\n",
      "batch 1484, train_loss 0.004310,Time used 0.003473s\n",
      "batch 1485, train_loss 0.003778,Time used 0.002976s\n",
      "batch 1486, train_loss 0.004235,Time used 0.002976s\n",
      "batch 1487, train_loss 0.004178,Time used 0.002977s\n",
      "batch 1488, train_loss 0.005633,Time used 0.012400s\n",
      "batch 1489, train_loss 0.004646,Time used 0.003472s\n",
      "batch 1490, train_loss 0.005613,Time used 0.003472s\n",
      "batch 1491, train_loss 0.004345,Time used 0.003472s\n",
      "batch 1492, train_loss 0.004877,Time used 0.003967s\n",
      "batch 1493, train_loss 0.004290,Time used 0.012399s\n",
      "batch 1494, train_loss 0.004154,Time used 0.003473s\n",
      "batch 1495, train_loss 0.004725,Time used 0.002976s\n",
      "batch 1496, train_loss 0.005385,Time used 0.003476s\n",
      "batch 1497, train_loss 0.004218,Time used 0.002972s\n",
      "batch 1498, train_loss 0.004808,Time used 0.002976s\n",
      "batch 1499, train_loss 0.004124,Time used 0.002976s\n",
      "batch 1500, train_loss 0.004989,Time used 0.003472s\n",
      "***************************test_batch 1500, test_rmse_loss 0.068446,test_mae_loss 0.049982,test_mape_loss 15.495582,Time used 0.011899s\n",
      "batch 1501, train_loss 0.005442,Time used 0.002480s\n",
      "batch 1502, train_loss 0.003997,Time used 0.002976s\n",
      "batch 1503, train_loss 0.004556,Time used 0.002976s\n",
      "batch 1504, train_loss 0.004393,Time used 0.002976s\n",
      "batch 1505, train_loss 0.004619,Time used 0.002976s\n",
      "batch 1506, train_loss 0.004761,Time used 0.002976s\n",
      "batch 1507, train_loss 0.005525,Time used 0.003472s\n",
      "batch 1508, train_loss 0.004982,Time used 0.002976s\n",
      "batch 1509, train_loss 0.005796,Time used 0.002480s\n",
      "batch 1510, train_loss 0.004885,Time used 0.002976s\n",
      "batch 1511, train_loss 0.004426,Time used 0.002971s\n",
      "batch 1512, train_loss 0.003170,Time used 0.003472s\n",
      "batch 1513, train_loss 0.004292,Time used 0.002480s\n",
      "batch 1514, train_loss 0.005059,Time used 0.003968s\n",
      "batch 1515, train_loss 0.004776,Time used 0.003472s\n",
      "batch 1516, train_loss 0.005165,Time used 0.003472s\n",
      "batch 1517, train_loss 0.004768,Time used 0.003968s\n",
      "batch 1518, train_loss 0.004466,Time used 0.003471s\n",
      "batch 1519, train_loss 0.004970,Time used 0.003968s\n",
      "batch 1520, train_loss 0.004787,Time used 0.003473s\n",
      "batch 1521, train_loss 0.004877,Time used 0.002975s\n",
      "batch 1522, train_loss 0.005662,Time used 0.002975s\n",
      "batch 1523, train_loss 0.004167,Time used 0.003472s\n",
      "batch 1524, train_loss 0.004658,Time used 0.003471s\n",
      "batch 1525, train_loss 0.005348,Time used 0.003472s\n",
      "batch 1526, train_loss 0.003771,Time used 0.002976s\n",
      "batch 1527, train_loss 0.003925,Time used 0.002976s\n",
      "batch 1528, train_loss 0.003424,Time used 0.002977s\n",
      "batch 1529, train_loss 0.004137,Time used 0.002975s\n",
      "batch 1530, train_loss 0.005307,Time used 0.002976s\n",
      "batch 1531, train_loss 0.005290,Time used 0.003472s\n",
      "batch 1532, train_loss 0.004866,Time used 0.002975s\n",
      "batch 1533, train_loss 0.004635,Time used 0.003478s\n",
      "batch 1534, train_loss 0.004895,Time used 0.002984s\n",
      "batch 1535, train_loss 0.003947,Time used 0.002964s\n",
      "batch 1536, train_loss 0.004249,Time used 0.003475s\n",
      "batch 1537, train_loss 0.004987,Time used 0.003472s\n",
      "batch 1538, train_loss 0.004156,Time used 0.002976s\n",
      "batch 1539, train_loss 0.004917,Time used 0.003472s\n",
      "batch 1540, train_loss 0.004199,Time used 0.002976s\n",
      "batch 1541, train_loss 0.004132,Time used 0.003476s\n",
      "batch 1542, train_loss 0.004326,Time used 0.003467s\n",
      "batch 1543, train_loss 0.003618,Time used 0.002971s\n",
      "batch 1544, train_loss 0.004398,Time used 0.002970s\n",
      "batch 1545, train_loss 0.004489,Time used 0.002976s\n",
      "batch 1546, train_loss 0.005276,Time used 0.002480s\n",
      "batch 1547, train_loss 0.004096,Time used 0.002975s\n",
      "batch 1548, train_loss 0.004558,Time used 0.002976s\n",
      "batch 1549, train_loss 0.005100,Time used 0.002980s\n",
      "batch 1550, train_loss 0.005093,Time used 0.002976s\n",
      "batch 1551, train_loss 0.005228,Time used 0.002974s\n",
      "batch 1552, train_loss 0.004091,Time used 0.002967s\n",
      "batch 1553, train_loss 0.005727,Time used 0.002976s\n",
      "batch 1554, train_loss 0.004269,Time used 0.002967s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1555, train_loss 0.004688,Time used 0.002975s\n",
      "batch 1556, train_loss 0.004342,Time used 0.002983s\n",
      "batch 1557, train_loss 0.004998,Time used 0.002976s\n",
      "batch 1558, train_loss 0.005059,Time used 0.002976s\n",
      "batch 1559, train_loss 0.004033,Time used 0.002976s\n",
      "batch 1560, train_loss 0.004166,Time used 0.002976s\n",
      "batch 1561, train_loss 0.004583,Time used 0.003472s\n",
      "batch 1562, train_loss 0.004866,Time used 0.002976s\n",
      "batch 1563, train_loss 0.005614,Time used 0.002975s\n",
      "batch 1564, train_loss 0.004617,Time used 0.002976s\n",
      "batch 1565, train_loss 0.005182,Time used 0.002976s\n",
      "batch 1566, train_loss 0.004796,Time used 0.003459s\n",
      "batch 1567, train_loss 0.004316,Time used 0.003472s\n",
      "batch 1568, train_loss 0.004728,Time used 0.003472s\n",
      "batch 1569, train_loss 0.005326,Time used 0.012400s\n",
      "batch 1570, train_loss 0.005092,Time used 0.002480s\n",
      "batch 1571, train_loss 0.004077,Time used 0.002976s\n",
      "batch 1572, train_loss 0.004091,Time used 0.003472s\n",
      "batch 1573, train_loss 0.003870,Time used 0.002480s\n",
      "batch 1574, train_loss 0.004727,Time used 0.002976s\n",
      "batch 1575, train_loss 0.004838,Time used 0.002964s\n",
      "batch 1576, train_loss 0.004335,Time used 0.002977s\n",
      "batch 1577, train_loss 0.004879,Time used 0.002480s\n",
      "batch 1578, train_loss 0.005661,Time used 0.003472s\n",
      "batch 1579, train_loss 0.005300,Time used 0.003472s\n",
      "batch 1580, train_loss 0.005596,Time used 0.002975s\n",
      "batch 1581, train_loss 0.004987,Time used 0.002479s\n",
      "batch 1582, train_loss 0.004103,Time used 0.003472s\n",
      "batch 1583, train_loss 0.004729,Time used 0.002975s\n",
      "batch 1584, train_loss 0.005746,Time used 0.003473s\n",
      "batch 1585, train_loss 0.002995,Time used 0.002964s\n",
      "batch 1586, train_loss 0.005513,Time used 0.003472s\n",
      "batch 1587, train_loss 0.004919,Time used 0.002975s\n",
      "batch 1588, train_loss 0.004254,Time used 0.002480s\n",
      "batch 1589, train_loss 0.004302,Time used 0.002976s\n",
      "batch 1590, train_loss 0.004332,Time used 0.002480s\n",
      "batch 1591, train_loss 0.004602,Time used 0.002976s\n",
      "batch 1592, train_loss 0.003617,Time used 0.002480s\n",
      "batch 1593, train_loss 0.004093,Time used 0.002480s\n",
      "batch 1594, train_loss 0.004570,Time used 0.002480s\n",
      "batch 1595, train_loss 0.005013,Time used 0.002976s\n",
      "batch 1596, train_loss 0.004493,Time used 0.002975s\n",
      "batch 1597, train_loss 0.004093,Time used 0.003968s\n",
      "batch 1598, train_loss 0.005122,Time used 0.002976s\n",
      "batch 1599, train_loss 0.005329,Time used 0.002970s\n",
      "batch 1600, train_loss 0.004540,Time used 0.002976s\n",
      "***************************test_batch 1600, test_rmse_loss 0.067621,test_mae_loss 0.049717,test_mape_loss 16.043370,Time used 0.010912s\n",
      "batch 1601, train_loss 0.004220,Time used 0.002976s\n",
      "batch 1602, train_loss 0.004149,Time used 0.002481s\n",
      "batch 1603, train_loss 0.005186,Time used 0.002976s\n",
      "batch 1604, train_loss 0.004326,Time used 0.002975s\n",
      "batch 1605, train_loss 0.003723,Time used 0.003473s\n",
      "batch 1606, train_loss 0.004634,Time used 0.002976s\n",
      "batch 1607, train_loss 0.005001,Time used 0.002481s\n",
      "batch 1608, train_loss 0.003548,Time used 0.002976s\n",
      "batch 1609, train_loss 0.004868,Time used 0.002976s\n",
      "batch 1610, train_loss 0.004525,Time used 0.002975s\n",
      "batch 1611, train_loss 0.004291,Time used 0.003472s\n",
      "batch 1612, train_loss 0.004978,Time used 0.002976s\n",
      "batch 1613, train_loss 0.005580,Time used 0.002976s\n",
      "batch 1614, train_loss 0.004589,Time used 0.002976s\n",
      "batch 1615, train_loss 0.004052,Time used 0.002976s\n",
      "batch 1616, train_loss 0.003091,Time used 0.002978s\n",
      "batch 1617, train_loss 0.004723,Time used 0.002975s\n",
      "batch 1618, train_loss 0.005130,Time used 0.002976s\n",
      "batch 1619, train_loss 0.004942,Time used 0.002976s\n",
      "batch 1620, train_loss 0.004049,Time used 0.002975s\n",
      "batch 1621, train_loss 0.004723,Time used 0.002481s\n",
      "batch 1622, train_loss 0.004585,Time used 0.003472s\n",
      "batch 1623, train_loss 0.003862,Time used 0.002975s\n",
      "batch 1624, train_loss 0.005215,Time used 0.002985s\n",
      "batch 1625, train_loss 0.005135,Time used 0.002480s\n",
      "batch 1626, train_loss 0.004351,Time used 0.002480s\n",
      "batch 1627, train_loss 0.004302,Time used 0.002475s\n",
      "batch 1628, train_loss 0.004100,Time used 0.002976s\n",
      "batch 1629, train_loss 0.004038,Time used 0.003468s\n",
      "batch 1630, train_loss 0.004470,Time used 0.002975s\n",
      "batch 1631, train_loss 0.004366,Time used 0.003472s\n",
      "batch 1632, train_loss 0.005012,Time used 0.002480s\n",
      "batch 1633, train_loss 0.004405,Time used 0.002979s\n",
      "batch 1634, train_loss 0.004626,Time used 0.002976s\n",
      "batch 1635, train_loss 0.004945,Time used 0.004961s\n",
      "batch 1636, train_loss 0.003483,Time used 0.003478s\n",
      "batch 1637, train_loss 0.004798,Time used 0.002976s\n",
      "batch 1638, train_loss 0.004882,Time used 0.003472s\n",
      "batch 1639, train_loss 0.003670,Time used 0.003472s\n",
      "batch 1640, train_loss 0.005009,Time used 0.003466s\n",
      "batch 1641, train_loss 0.003461,Time used 0.003472s\n",
      "batch 1642, train_loss 0.004182,Time used 0.002976s\n",
      "batch 1643, train_loss 0.003764,Time used 0.002976s\n",
      "batch 1644, train_loss 0.004939,Time used 0.003968s\n",
      "batch 1645, train_loss 0.005806,Time used 0.002975s\n",
      "batch 1646, train_loss 0.004058,Time used 0.002986s\n",
      "batch 1647, train_loss 0.005120,Time used 0.002480s\n",
      "batch 1648, train_loss 0.004632,Time used 0.002969s\n",
      "batch 1649, train_loss 0.005382,Time used 0.003472s\n",
      "batch 1650, train_loss 0.004185,Time used 0.002981s\n",
      "batch 1651, train_loss 0.004690,Time used 0.002976s\n",
      "batch 1652, train_loss 0.003857,Time used 0.002975s\n",
      "batch 1653, train_loss 0.005065,Time used 0.002480s\n",
      "batch 1654, train_loss 0.004481,Time used 0.002976s\n",
      "batch 1655, train_loss 0.004019,Time used 0.003968s\n",
      "batch 1656, train_loss 0.003900,Time used 0.003472s\n",
      "batch 1657, train_loss 0.004695,Time used 0.003472s\n",
      "batch 1658, train_loss 0.004739,Time used 0.004962s\n",
      "batch 1659, train_loss 0.005024,Time used 0.003968s\n",
      "batch 1660, train_loss 0.004371,Time used 0.012400s\n",
      "batch 1661, train_loss 0.004710,Time used 0.002976s\n",
      "batch 1662, train_loss 0.005151,Time used 0.003471s\n",
      "batch 1663, train_loss 0.004645,Time used 0.003472s\n",
      "batch 1664, train_loss 0.004814,Time used 0.002968s\n",
      "batch 1665, train_loss 0.005415,Time used 0.002976s\n",
      "batch 1666, train_loss 0.004852,Time used 0.003472s\n",
      "batch 1667, train_loss 0.004862,Time used 0.003473s\n",
      "batch 1668, train_loss 0.003623,Time used 0.002976s\n",
      "batch 1669, train_loss 0.004653,Time used 0.003970s\n",
      "batch 1670, train_loss 0.004791,Time used 0.003968s\n",
      "batch 1671, train_loss 0.004433,Time used 0.003472s\n",
      "batch 1672, train_loss 0.004176,Time used 0.002975s\n",
      "batch 1673, train_loss 0.004772,Time used 0.002976s\n",
      "batch 1674, train_loss 0.003978,Time used 0.002976s\n",
      "batch 1675, train_loss 0.005086,Time used 0.002976s\n",
      "batch 1676, train_loss 0.004270,Time used 0.003472s\n",
      "batch 1677, train_loss 0.004726,Time used 0.003481s\n",
      "batch 1678, train_loss 0.004212,Time used 0.003463s\n",
      "batch 1679, train_loss 0.004510,Time used 0.003470s\n",
      "batch 1680, train_loss 0.004307,Time used 0.003472s\n",
      "batch 1681, train_loss 0.004303,Time used 0.003473s\n",
      "batch 1682, train_loss 0.004735,Time used 0.002969s\n",
      "batch 1683, train_loss 0.004282,Time used 0.003473s\n",
      "batch 1684, train_loss 0.003997,Time used 0.003472s\n",
      "batch 1685, train_loss 0.004013,Time used 0.003472s\n",
      "batch 1686, train_loss 0.004314,Time used 0.002976s\n",
      "batch 1687, train_loss 0.005058,Time used 0.003472s\n",
      "batch 1688, train_loss 0.004043,Time used 0.003472s\n",
      "batch 1689, train_loss 0.004590,Time used 0.003466s\n",
      "batch 1690, train_loss 0.004772,Time used 0.003471s\n",
      "batch 1691, train_loss 0.004335,Time used 0.002976s\n",
      "batch 1692, train_loss 0.004419,Time used 0.003978s\n",
      "batch 1693, train_loss 0.004715,Time used 0.003472s\n",
      "batch 1694, train_loss 0.003878,Time used 0.004961s\n",
      "batch 1695, train_loss 0.003691,Time used 0.002976s\n",
      "batch 1696, train_loss 0.004026,Time used 0.005456s\n",
      "batch 1697, train_loss 0.004196,Time used 0.002973s\n",
      "batch 1698, train_loss 0.004245,Time used 0.014385s\n",
      "batch 1699, train_loss 0.004563,Time used 0.003969s\n",
      "batch 1700, train_loss 0.003831,Time used 0.003472s\n",
      "***************************test_batch 1700, test_rmse_loss 0.067027,test_mae_loss 0.049009,test_mape_loss 15.490167,Time used 0.021824s\n",
      "batch 1701, train_loss 0.005003,Time used 0.002975s\n",
      "batch 1702, train_loss 0.004706,Time used 0.002975s\n",
      "batch 1703, train_loss 0.005595,Time used 0.002480s\n",
      "batch 1704, train_loss 0.004846,Time used 0.002977s\n",
      "batch 1705, train_loss 0.004272,Time used 0.003469s\n",
      "batch 1706, train_loss 0.005042,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1707, train_loss 0.004690,Time used 0.003471s\n",
      "batch 1708, train_loss 0.003482,Time used 0.002976s\n",
      "batch 1709, train_loss 0.003746,Time used 0.002480s\n",
      "batch 1710, train_loss 0.004766,Time used 0.003472s\n",
      "batch 1711, train_loss 0.005650,Time used 0.002976s\n",
      "batch 1712, train_loss 0.004700,Time used 0.002976s\n",
      "batch 1713, train_loss 0.004153,Time used 0.002976s\n",
      "batch 1714, train_loss 0.004414,Time used 0.002976s\n",
      "batch 1715, train_loss 0.004542,Time used 0.002975s\n",
      "batch 1716, train_loss 0.004431,Time used 0.003472s\n",
      "batch 1717, train_loss 0.004103,Time used 0.002976s\n",
      "batch 1718, train_loss 0.004187,Time used 0.003471s\n",
      "batch 1719, train_loss 0.004428,Time used 0.003472s\n",
      "batch 1720, train_loss 0.004984,Time used 0.003472s\n",
      "batch 1721, train_loss 0.003896,Time used 0.003472s\n",
      "batch 1722, train_loss 0.005043,Time used 0.002976s\n",
      "batch 1723, train_loss 0.005606,Time used 0.003473s\n",
      "batch 1724, train_loss 0.005282,Time used 0.003472s\n",
      "batch 1725, train_loss 0.004953,Time used 0.003969s\n",
      "batch 1726, train_loss 0.004199,Time used 0.002976s\n",
      "batch 1727, train_loss 0.005416,Time used 0.002976s\n",
      "batch 1728, train_loss 0.001896,Time used 0.004960s\n",
      "batch 1729, train_loss 0.004658,Time used 0.003968s\n",
      "batch 1730, train_loss 0.004117,Time used 0.003971s\n",
      "batch 1731, train_loss 0.004854,Time used 0.003470s\n",
      "batch 1732, train_loss 0.005878,Time used 0.003472s\n",
      "batch 1733, train_loss 0.003936,Time used 0.002976s\n",
      "batch 1734, train_loss 0.003644,Time used 0.012400s\n",
      "batch 1735, train_loss 0.004690,Time used 0.003472s\n",
      "batch 1736, train_loss 0.004437,Time used 0.002976s\n",
      "batch 1737, train_loss 0.003749,Time used 0.003472s\n",
      "batch 1738, train_loss 0.004571,Time used 0.002976s\n",
      "batch 1739, train_loss 0.004844,Time used 0.005451s\n",
      "batch 1740, train_loss 0.003655,Time used 0.002976s\n",
      "batch 1741, train_loss 0.003998,Time used 0.002480s\n",
      "batch 1742, train_loss 0.004067,Time used 0.002976s\n",
      "batch 1743, train_loss 0.004356,Time used 0.003475s\n",
      "batch 1744, train_loss 0.005768,Time used 0.002976s\n",
      "batch 1745, train_loss 0.004812,Time used 0.003472s\n",
      "batch 1746, train_loss 0.004401,Time used 0.003472s\n",
      "batch 1747, train_loss 0.004405,Time used 0.002977s\n",
      "batch 1748, train_loss 0.004177,Time used 0.003968s\n",
      "batch 1749, train_loss 0.003849,Time used 0.002479s\n",
      "batch 1750, train_loss 0.004584,Time used 0.002976s\n",
      "batch 1751, train_loss 0.004379,Time used 0.002967s\n",
      "batch 1752, train_loss 0.003955,Time used 0.002975s\n",
      "batch 1753, train_loss 0.004303,Time used 0.002976s\n",
      "batch 1754, train_loss 0.004502,Time used 0.003969s\n",
      "batch 1755, train_loss 0.005643,Time used 0.002481s\n",
      "batch 1756, train_loss 0.004929,Time used 0.002976s\n",
      "batch 1757, train_loss 0.004088,Time used 0.003466s\n",
      "batch 1758, train_loss 0.005104,Time used 0.002976s\n",
      "batch 1759, train_loss 0.004496,Time used 0.003472s\n",
      "batch 1760, train_loss 0.004774,Time used 0.003472s\n",
      "batch 1761, train_loss 0.004241,Time used 0.002973s\n",
      "batch 1762, train_loss 0.003724,Time used 0.002976s\n",
      "batch 1763, train_loss 0.005326,Time used 0.002976s\n",
      "batch 1764, train_loss 0.004659,Time used 0.002975s\n",
      "batch 1765, train_loss 0.004108,Time used 0.003472s\n",
      "batch 1766, train_loss 0.004150,Time used 0.002975s\n",
      "batch 1767, train_loss 0.003794,Time used 0.003471s\n",
      "batch 1768, train_loss 0.004583,Time used 0.002976s\n",
      "batch 1769, train_loss 0.004661,Time used 0.002975s\n",
      "batch 1770, train_loss 0.003796,Time used 0.002976s\n",
      "batch 1771, train_loss 0.004487,Time used 0.003472s\n",
      "batch 1772, train_loss 0.005743,Time used 0.002976s\n",
      "batch 1773, train_loss 0.004363,Time used 0.003468s\n",
      "batch 1774, train_loss 0.004510,Time used 0.002976s\n",
      "batch 1775, train_loss 0.004266,Time used 0.002976s\n",
      "batch 1776, train_loss 0.005272,Time used 0.003472s\n",
      "batch 1777, train_loss 0.003489,Time used 0.002480s\n",
      "batch 1778, train_loss 0.003735,Time used 0.003471s\n",
      "batch 1779, train_loss 0.004079,Time used 0.002970s\n",
      "batch 1780, train_loss 0.005051,Time used 0.003472s\n",
      "batch 1781, train_loss 0.004868,Time used 0.002976s\n",
      "batch 1782, train_loss 0.001593,Time used 0.003472s\n",
      "batch 1783, train_loss 0.005097,Time used 0.002975s\n",
      "batch 1784, train_loss 0.004728,Time used 0.002977s\n",
      "batch 1785, train_loss 0.004156,Time used 0.002967s\n",
      "batch 1786, train_loss 0.004484,Time used 0.002975s\n",
      "batch 1787, train_loss 0.004281,Time used 0.002481s\n",
      "batch 1788, train_loss 0.003783,Time used 0.003473s\n",
      "batch 1789, train_loss 0.004016,Time used 0.002976s\n",
      "batch 1790, train_loss 0.006368,Time used 0.002975s\n",
      "batch 1791, train_loss 0.004395,Time used 0.002977s\n",
      "batch 1792, train_loss 0.004174,Time used 0.002975s\n",
      "batch 1793, train_loss 0.004361,Time used 0.003472s\n",
      "batch 1794, train_loss 0.003948,Time used 0.002976s\n",
      "batch 1795, train_loss 0.003632,Time used 0.003472s\n",
      "batch 1796, train_loss 0.003708,Time used 0.003471s\n",
      "batch 1797, train_loss 0.004865,Time used 0.002981s\n",
      "batch 1798, train_loss 0.005028,Time used 0.002976s\n",
      "batch 1799, train_loss 0.005012,Time used 0.002976s\n",
      "batch 1800, train_loss 0.004596,Time used 0.002970s\n",
      "***************************test_batch 1800, test_rmse_loss 0.066581,test_mae_loss 0.048427,test_mape_loss 15.003620,Time used 0.010912s\n",
      "batch 1801, train_loss 0.003392,Time used 0.002977s\n",
      "batch 1802, train_loss 0.004192,Time used 0.002480s\n",
      "batch 1803, train_loss 0.004096,Time used 0.003472s\n",
      "batch 1804, train_loss 0.004920,Time used 0.003473s\n",
      "batch 1805, train_loss 0.004304,Time used 0.002976s\n",
      "batch 1806, train_loss 0.004315,Time used 0.002976s\n",
      "batch 1807, train_loss 0.003754,Time used 0.002483s\n",
      "batch 1808, train_loss 0.004713,Time used 0.002974s\n",
      "batch 1809, train_loss 0.004420,Time used 0.002976s\n",
      "batch 1810, train_loss 0.004985,Time used 0.002976s\n",
      "batch 1811, train_loss 0.003721,Time used 0.002976s\n",
      "batch 1812, train_loss 0.003955,Time used 0.002976s\n",
      "batch 1813, train_loss 0.004592,Time used 0.002975s\n",
      "batch 1814, train_loss 0.004916,Time used 0.002971s\n",
      "batch 1815, train_loss 0.004690,Time used 0.002973s\n",
      "batch 1816, train_loss 0.004347,Time used 0.003477s\n",
      "batch 1817, train_loss 0.004140,Time used 0.003471s\n",
      "batch 1818, train_loss 0.003807,Time used 0.002972s\n",
      "batch 1819, train_loss 0.004961,Time used 0.002976s\n",
      "batch 1820, train_loss 0.006724,Time used 0.002976s\n",
      "batch 1821, train_loss 0.003842,Time used 0.014385s\n",
      "batch 1822, train_loss 0.004764,Time used 0.003466s\n",
      "batch 1823, train_loss 0.005086,Time used 0.002976s\n",
      "batch 1824, train_loss 0.004769,Time used 0.003476s\n",
      "batch 1825, train_loss 0.004649,Time used 0.002976s\n",
      "batch 1826, train_loss 0.005532,Time used 0.002977s\n",
      "batch 1827, train_loss 0.003690,Time used 0.003472s\n",
      "batch 1828, train_loss 0.004298,Time used 0.004463s\n",
      "batch 1829, train_loss 0.004252,Time used 0.002976s\n",
      "batch 1830, train_loss 0.004010,Time used 0.002975s\n",
      "batch 1831, train_loss 0.003377,Time used 0.003472s\n",
      "batch 1832, train_loss 0.005014,Time used 0.002971s\n",
      "batch 1833, train_loss 0.004011,Time used 0.003472s\n",
      "batch 1834, train_loss 0.004402,Time used 0.003473s\n",
      "batch 1835, train_loss 0.003418,Time used 0.003472s\n",
      "batch 1836, train_loss 0.001797,Time used 0.003477s\n",
      "batch 1837, train_loss 0.005351,Time used 0.002481s\n",
      "batch 1838, train_loss 0.003668,Time used 0.002976s\n",
      "batch 1839, train_loss 0.004152,Time used 0.003472s\n",
      "batch 1840, train_loss 0.004469,Time used 0.002976s\n",
      "batch 1841, train_loss 0.003981,Time used 0.002975s\n",
      "batch 1842, train_loss 0.004831,Time used 0.002976s\n",
      "batch 1843, train_loss 0.003529,Time used 0.002976s\n",
      "batch 1844, train_loss 0.004543,Time used 0.002977s\n",
      "batch 1845, train_loss 0.004616,Time used 0.002973s\n",
      "batch 1846, train_loss 0.004734,Time used 0.002976s\n",
      "batch 1847, train_loss 0.004428,Time used 0.003476s\n",
      "batch 1848, train_loss 0.003640,Time used 0.002975s\n",
      "batch 1849, train_loss 0.004584,Time used 0.002976s\n",
      "batch 1850, train_loss 0.003853,Time used 0.002977s\n",
      "batch 1851, train_loss 0.005099,Time used 0.003472s\n",
      "batch 1852, train_loss 0.004031,Time used 0.003473s\n",
      "batch 1853, train_loss 0.004534,Time used 0.002974s\n",
      "batch 1854, train_loss 0.003845,Time used 0.002976s\n",
      "batch 1855, train_loss 0.003922,Time used 0.002977s\n",
      "batch 1856, train_loss 0.004746,Time used 0.003470s\n",
      "batch 1857, train_loss 0.003292,Time used 0.003482s\n",
      "batch 1858, train_loss 0.004627,Time used 0.003472s\n",
      "batch 1859, train_loss 0.003232,Time used 0.003472s\n",
      "batch 1860, train_loss 0.005001,Time used 0.003968s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1861, train_loss 0.003494,Time used 0.002975s\n",
      "batch 1862, train_loss 0.004239,Time used 0.002976s\n",
      "batch 1863, train_loss 0.004929,Time used 0.002972s\n",
      "batch 1864, train_loss 0.003998,Time used 0.002976s\n",
      "batch 1865, train_loss 0.004788,Time used 0.003467s\n",
      "batch 1866, train_loss 0.004251,Time used 0.002977s\n",
      "batch 1867, train_loss 0.003533,Time used 0.003472s\n",
      "batch 1868, train_loss 0.004382,Time used 0.002976s\n",
      "batch 1869, train_loss 0.004053,Time used 0.002976s\n",
      "batch 1870, train_loss 0.006689,Time used 0.003472s\n",
      "batch 1871, train_loss 0.004131,Time used 0.003968s\n",
      "batch 1872, train_loss 0.004235,Time used 0.003472s\n",
      "batch 1873, train_loss 0.005097,Time used 0.004464s\n",
      "batch 1874, train_loss 0.004530,Time used 0.003472s\n",
      "batch 1875, train_loss 0.004187,Time used 0.003481s\n",
      "batch 1876, train_loss 0.004090,Time used 0.002976s\n",
      "batch 1877, train_loss 0.005071,Time used 0.002976s\n",
      "batch 1878, train_loss 0.005216,Time used 0.003968s\n",
      "batch 1879, train_loss 0.004421,Time used 0.002976s\n",
      "batch 1880, train_loss 0.004895,Time used 0.003464s\n",
      "batch 1881, train_loss 0.004235,Time used 0.003472s\n",
      "batch 1882, train_loss 0.004964,Time used 0.002976s\n",
      "batch 1883, train_loss 0.004535,Time used 0.002976s\n",
      "batch 1884, train_loss 0.003645,Time used 0.003473s\n",
      "batch 1885, train_loss 0.004522,Time used 0.003968s\n",
      "batch 1886, train_loss 0.005825,Time used 0.003472s\n",
      "batch 1887, train_loss 0.004406,Time used 0.003968s\n",
      "batch 1888, train_loss 0.003373,Time used 0.003968s\n",
      "batch 1889, train_loss 0.004043,Time used 0.002976s\n",
      "batch 1890, train_loss 0.000876,Time used 0.003472s\n",
      "batch 1891, train_loss 0.003987,Time used 0.003472s\n",
      "batch 1892, train_loss 0.003531,Time used 0.002976s\n",
      "batch 1893, train_loss 0.004527,Time used 0.003474s\n",
      "batch 1894, train_loss 0.003635,Time used 0.003471s\n",
      "batch 1895, train_loss 0.003935,Time used 0.004463s\n",
      "batch 1896, train_loss 0.004389,Time used 0.003968s\n",
      "batch 1897, train_loss 0.005420,Time used 0.003973s\n",
      "batch 1898, train_loss 0.005300,Time used 0.003468s\n",
      "batch 1899, train_loss 0.003688,Time used 0.013888s\n",
      "batch 1900, train_loss 0.004730,Time used 0.002976s\n",
      "***************************test_batch 1900, test_rmse_loss 0.066259,test_mae_loss 0.048046,test_mape_loss 14.631684,Time used 0.012401s\n",
      "batch 1901, train_loss 0.004615,Time used 0.003473s\n",
      "batch 1902, train_loss 0.003039,Time used 0.003472s\n",
      "batch 1903, train_loss 0.003867,Time used 0.012904s\n",
      "batch 1904, train_loss 0.004569,Time used 0.003472s\n",
      "batch 1905, train_loss 0.004944,Time used 0.002976s\n",
      "batch 1906, train_loss 0.004834,Time used 0.003472s\n",
      "batch 1907, train_loss 0.005364,Time used 0.003472s\n",
      "batch 1908, train_loss 0.003500,Time used 0.002976s\n",
      "batch 1909, train_loss 0.004434,Time used 0.003472s\n",
      "batch 1910, train_loss 0.004083,Time used 0.002976s\n",
      "batch 1911, train_loss 0.003649,Time used 0.002976s\n",
      "batch 1912, train_loss 0.004462,Time used 0.002975s\n",
      "batch 1913, train_loss 0.003947,Time used 0.002976s\n",
      "batch 1914, train_loss 0.005355,Time used 0.002480s\n",
      "batch 1915, train_loss 0.003745,Time used 0.003472s\n",
      "batch 1916, train_loss 0.004360,Time used 0.002480s\n",
      "batch 1917, train_loss 0.004069,Time used 0.003472s\n",
      "batch 1918, train_loss 0.004648,Time used 0.002977s\n",
      "batch 1919, train_loss 0.004901,Time used 0.002976s\n",
      "batch 1920, train_loss 0.004401,Time used 0.002976s\n",
      "batch 1921, train_loss 0.005478,Time used 0.003472s\n",
      "batch 1922, train_loss 0.004162,Time used 0.002975s\n",
      "batch 1923, train_loss 0.004825,Time used 0.002976s\n",
      "batch 1924, train_loss 0.004269,Time used 0.002973s\n",
      "batch 1925, train_loss 0.004769,Time used 0.002975s\n",
      "batch 1926, train_loss 0.005592,Time used 0.003472s\n",
      "batch 1927, train_loss 0.004582,Time used 0.002976s\n",
      "batch 1928, train_loss 0.003732,Time used 0.002976s\n",
      "batch 1929, train_loss 0.004199,Time used 0.002976s\n",
      "batch 1930, train_loss 0.004226,Time used 0.003472s\n",
      "batch 1931, train_loss 0.004289,Time used 0.003968s\n",
      "batch 1932, train_loss 0.004399,Time used 0.003473s\n",
      "batch 1933, train_loss 0.004761,Time used 0.002480s\n",
      "batch 1934, train_loss 0.004333,Time used 0.002975s\n",
      "batch 1935, train_loss 0.004614,Time used 0.003472s\n",
      "batch 1936, train_loss 0.004197,Time used 0.003463s\n",
      "batch 1937, train_loss 0.004729,Time used 0.005456s\n",
      "batch 1938, train_loss 0.004472,Time used 0.003472s\n",
      "batch 1939, train_loss 0.003589,Time used 0.003472s\n",
      "batch 1940, train_loss 0.003232,Time used 0.003468s\n",
      "batch 1941, train_loss 0.004981,Time used 0.003472s\n",
      "batch 1942, train_loss 0.003396,Time used 0.002977s\n",
      "batch 1943, train_loss 0.004056,Time used 0.003472s\n",
      "batch 1944, train_loss 0.002893,Time used 0.002976s\n",
      "batch 1945, train_loss 0.004488,Time used 0.002977s\n",
      "batch 1946, train_loss 0.005452,Time used 0.002976s\n",
      "batch 1947, train_loss 0.004567,Time used 0.002976s\n",
      "batch 1948, train_loss 0.004702,Time used 0.002976s\n",
      "batch 1949, train_loss 0.004173,Time used 0.002980s\n",
      "batch 1950, train_loss 0.003998,Time used 0.003471s\n",
      "batch 1951, train_loss 0.004362,Time used 0.003472s\n",
      "batch 1952, train_loss 0.004567,Time used 0.002976s\n",
      "batch 1953, train_loss 0.004387,Time used 0.003472s\n",
      "batch 1954, train_loss 0.004304,Time used 0.002976s\n",
      "batch 1955, train_loss 0.004936,Time used 0.002976s\n",
      "batch 1956, train_loss 0.005614,Time used 0.003472s\n",
      "batch 1957, train_loss 0.005041,Time used 0.002976s\n",
      "batch 1958, train_loss 0.004009,Time used 0.003472s\n",
      "batch 1959, train_loss 0.003873,Time used 0.003464s\n",
      "batch 1960, train_loss 0.005387,Time used 0.003472s\n",
      "batch 1961, train_loss 0.004592,Time used 0.002976s\n",
      "batch 1962, train_loss 0.004525,Time used 0.003472s\n",
      "batch 1963, train_loss 0.003843,Time used 0.002977s\n",
      "batch 1964, train_loss 0.004435,Time used 0.002975s\n",
      "batch 1965, train_loss 0.004874,Time used 0.003476s\n",
      "batch 1966, train_loss 0.004184,Time used 0.003472s\n",
      "batch 1967, train_loss 0.003904,Time used 0.003472s\n",
      "batch 1968, train_loss 0.003519,Time used 0.003471s\n",
      "batch 1969, train_loss 0.004835,Time used 0.003472s\n",
      "batch 1970, train_loss 0.003844,Time used 0.003464s\n",
      "batch 1971, train_loss 0.004732,Time used 0.002976s\n",
      "batch 1972, train_loss 0.002882,Time used 0.003472s\n",
      "batch 1973, train_loss 0.004214,Time used 0.003472s\n",
      "batch 1974, train_loss 0.004088,Time used 0.003472s\n",
      "batch 1975, train_loss 0.003961,Time used 0.003472s\n",
      "batch 1976, train_loss 0.003294,Time used 0.002976s\n",
      "batch 1977, train_loss 0.003672,Time used 0.003470s\n",
      "batch 1978, train_loss 0.004153,Time used 0.003472s\n",
      "batch 1979, train_loss 0.003723,Time used 0.003472s\n",
      "batch 1980, train_loss 0.003770,Time used 0.003473s\n",
      "batch 1981, train_loss 0.003934,Time used 0.002976s\n",
      "batch 1982, train_loss 0.004508,Time used 0.003472s\n",
      "batch 1983, train_loss 0.004289,Time used 0.003472s\n",
      "batch 1984, train_loss 0.004453,Time used 0.002976s\n",
      "batch 1985, train_loss 0.005744,Time used 0.003472s\n",
      "batch 1986, train_loss 0.004024,Time used 0.003472s\n",
      "batch 1987, train_loss 0.004124,Time used 0.002975s\n",
      "batch 1988, train_loss 0.004185,Time used 0.003472s\n",
      "batch 1989, train_loss 0.004256,Time used 0.012899s\n",
      "batch 1990, train_loss 0.005044,Time used 0.002975s\n",
      "batch 1991, train_loss 0.003502,Time used 0.002976s\n",
      "batch 1992, train_loss 0.004621,Time used 0.003473s\n",
      "batch 1993, train_loss 0.004563,Time used 0.002976s\n",
      "batch 1994, train_loss 0.004342,Time used 0.002976s\n",
      "batch 1995, train_loss 0.003391,Time used 0.003472s\n",
      "batch 1996, train_loss 0.004655,Time used 0.003472s\n",
      "batch 1997, train_loss 0.004412,Time used 0.002977s\n",
      "batch 1998, train_loss 0.002134,Time used 0.002976s\n",
      "batch 1999, train_loss 0.003452,Time used 0.002480s\n",
      "batch 2000, train_loss 0.004630,Time used 0.002976s\n",
      "***************************test_batch 2000, test_rmse_loss 0.065677,test_mae_loss 0.047620,test_mape_loss 14.699860,Time used 0.010913s\n",
      "batch 2001, train_loss 0.003120,Time used 0.002975s\n",
      "batch 2002, train_loss 0.004422,Time used 0.003469s\n",
      "batch 2003, train_loss 0.004164,Time used 0.002480s\n",
      "batch 2004, train_loss 0.005258,Time used 0.002977s\n",
      "batch 2005, train_loss 0.004121,Time used 0.002976s\n",
      "batch 2006, train_loss 0.004320,Time used 0.002977s\n",
      "batch 2007, train_loss 0.004666,Time used 0.002976s\n",
      "batch 2008, train_loss 0.004709,Time used 0.002976s\n",
      "batch 2009, train_loss 0.004713,Time used 0.002967s\n",
      "batch 2010, train_loss 0.005084,Time used 0.003472s\n",
      "batch 2011, train_loss 0.004044,Time used 0.002976s\n",
      "batch 2012, train_loss 0.005463,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2013, train_loss 0.003833,Time used 0.003472s\n",
      "batch 2014, train_loss 0.003956,Time used 0.002980s\n",
      "batch 2015, train_loss 0.004468,Time used 0.002976s\n",
      "batch 2016, train_loss 0.004850,Time used 0.002976s\n",
      "batch 2017, train_loss 0.004258,Time used 0.003472s\n",
      "batch 2018, train_loss 0.003859,Time used 0.002971s\n",
      "batch 2019, train_loss 0.004626,Time used 0.002975s\n",
      "batch 2020, train_loss 0.004114,Time used 0.002970s\n",
      "batch 2021, train_loss 0.004783,Time used 0.003460s\n",
      "batch 2022, train_loss 0.004301,Time used 0.002976s\n",
      "batch 2023, train_loss 0.004672,Time used 0.002977s\n",
      "batch 2024, train_loss 0.004391,Time used 0.003471s\n",
      "batch 2025, train_loss 0.004760,Time used 0.003473s\n",
      "batch 2026, train_loss 0.005036,Time used 0.002975s\n",
      "batch 2027, train_loss 0.005813,Time used 0.002481s\n",
      "batch 2028, train_loss 0.004009,Time used 0.002481s\n",
      "batch 2029, train_loss 0.004485,Time used 0.002976s\n",
      "batch 2030, train_loss 0.003762,Time used 0.002976s\n",
      "batch 2031, train_loss 0.004101,Time used 0.002976s\n",
      "batch 2032, train_loss 0.004202,Time used 0.002976s\n",
      "batch 2033, train_loss 0.004168,Time used 0.002971s\n",
      "batch 2034, train_loss 0.003421,Time used 0.002481s\n",
      "batch 2035, train_loss 0.003814,Time used 0.002976s\n",
      "batch 2036, train_loss 0.004239,Time used 0.002480s\n",
      "batch 2037, train_loss 0.004273,Time used 0.002969s\n",
      "batch 2038, train_loss 0.004308,Time used 0.002976s\n",
      "batch 2039, train_loss 0.004585,Time used 0.002976s\n",
      "batch 2040, train_loss 0.003812,Time used 0.002976s\n",
      "batch 2041, train_loss 0.004441,Time used 0.002975s\n",
      "batch 2042, train_loss 0.003988,Time used 0.002977s\n",
      "batch 2043, train_loss 0.004512,Time used 0.002977s\n",
      "batch 2044, train_loss 0.004044,Time used 0.003969s\n",
      "batch 2045, train_loss 0.003404,Time used 0.002976s\n",
      "batch 2046, train_loss 0.003243,Time used 0.002976s\n",
      "batch 2047, train_loss 0.004500,Time used 0.002481s\n",
      "batch 2048, train_loss 0.003747,Time used 0.003478s\n",
      "batch 2049, train_loss 0.003466,Time used 0.002480s\n",
      "batch 2050, train_loss 0.003747,Time used 0.002480s\n",
      "batch 2051, train_loss 0.004726,Time used 0.002977s\n",
      "batch 2052, train_loss 0.007232,Time used 0.003472s\n",
      "batch 2053, train_loss 0.005077,Time used 0.002976s\n",
      "batch 2054, train_loss 0.004191,Time used 0.002977s\n",
      "batch 2055, train_loss 0.004734,Time used 0.002976s\n",
      "batch 2056, train_loss 0.004426,Time used 0.002975s\n",
      "batch 2057, train_loss 0.004105,Time used 0.002975s\n",
      "batch 2058, train_loss 0.004579,Time used 0.003472s\n",
      "batch 2059, train_loss 0.004587,Time used 0.002970s\n",
      "batch 2060, train_loss 0.003789,Time used 0.002976s\n",
      "batch 2061, train_loss 0.004270,Time used 0.002976s\n",
      "batch 2062, train_loss 0.004774,Time used 0.002984s\n",
      "batch 2063, train_loss 0.004132,Time used 0.002972s\n",
      "batch 2064, train_loss 0.004674,Time used 0.003472s\n",
      "batch 2065, train_loss 0.003922,Time used 0.002976s\n",
      "batch 2066, train_loss 0.004457,Time used 0.003472s\n",
      "batch 2067, train_loss 0.004838,Time used 0.003470s\n",
      "batch 2068, train_loss 0.004105,Time used 0.011903s\n",
      "batch 2069, train_loss 0.003893,Time used 0.002976s\n",
      "batch 2070, train_loss 0.004931,Time used 0.003473s\n",
      "batch 2071, train_loss 0.004621,Time used 0.002976s\n",
      "batch 2072, train_loss 0.004661,Time used 0.002979s\n",
      "batch 2073, train_loss 0.004310,Time used 0.002976s\n",
      "batch 2074, train_loss 0.004449,Time used 0.003472s\n",
      "batch 2075, train_loss 0.003477,Time used 0.002480s\n",
      "batch 2076, train_loss 0.003922,Time used 0.002480s\n",
      "batch 2077, train_loss 0.004000,Time used 0.002470s\n",
      "batch 2078, train_loss 0.005452,Time used 0.003472s\n",
      "batch 2079, train_loss 0.004628,Time used 0.002480s\n",
      "batch 2080, train_loss 0.003200,Time used 0.002976s\n",
      "batch 2081, train_loss 0.004023,Time used 0.002970s\n",
      "batch 2082, train_loss 0.003793,Time used 0.002976s\n",
      "batch 2083, train_loss 0.004645,Time used 0.002976s\n",
      "batch 2084, train_loss 0.003258,Time used 0.003472s\n",
      "batch 2085, train_loss 0.003972,Time used 0.002976s\n",
      "batch 2086, train_loss 0.003831,Time used 0.002480s\n",
      "batch 2087, train_loss 0.004150,Time used 0.002481s\n",
      "batch 2088, train_loss 0.004289,Time used 0.002481s\n",
      "batch 2089, train_loss 0.004814,Time used 0.003477s\n",
      "batch 2090, train_loss 0.004586,Time used 0.002976s\n",
      "batch 2091, train_loss 0.003933,Time used 0.002976s\n",
      "batch 2092, train_loss 0.004234,Time used 0.002480s\n",
      "batch 2093, train_loss 0.004541,Time used 0.002977s\n",
      "batch 2094, train_loss 0.003934,Time used 0.002973s\n",
      "batch 2095, train_loss 0.003861,Time used 0.002481s\n",
      "batch 2096, train_loss 0.004430,Time used 0.002976s\n",
      "batch 2097, train_loss 0.003404,Time used 0.002480s\n",
      "batch 2098, train_loss 0.003798,Time used 0.002481s\n",
      "batch 2099, train_loss 0.004132,Time used 0.002480s\n",
      "batch 2100, train_loss 0.003753,Time used 0.002976s\n",
      "***************************test_batch 2100, test_rmse_loss 0.065213,test_mae_loss 0.047782,test_mape_loss 15.544821,Time used 0.010912s\n",
      "batch 2101, train_loss 0.004209,Time used 0.002480s\n",
      "batch 2102, train_loss 0.004671,Time used 0.002976s\n",
      "batch 2103, train_loss 0.004566,Time used 0.002976s\n",
      "batch 2104, train_loss 0.003959,Time used 0.002976s\n",
      "batch 2105, train_loss 0.004012,Time used 0.003472s\n",
      "batch 2106, train_loss 0.002694,Time used 0.003472s\n",
      "batch 2107, train_loss 0.006618,Time used 0.002976s\n",
      "batch 2108, train_loss 0.004703,Time used 0.003472s\n",
      "batch 2109, train_loss 0.004080,Time used 0.002480s\n",
      "batch 2110, train_loss 0.004163,Time used 0.013887s\n",
      "batch 2111, train_loss 0.004759,Time used 0.002976s\n",
      "batch 2112, train_loss 0.004209,Time used 0.002481s\n",
      "batch 2113, train_loss 0.003428,Time used 0.002976s\n",
      "batch 2114, train_loss 0.003475,Time used 0.002975s\n",
      "batch 2115, train_loss 0.003554,Time used 0.002480s\n",
      "batch 2116, train_loss 0.004395,Time used 0.003470s\n",
      "batch 2117, train_loss 0.004499,Time used 0.002972s\n",
      "batch 2118, train_loss 0.003980,Time used 0.002976s\n",
      "batch 2119, train_loss 0.004767,Time used 0.002975s\n",
      "batch 2120, train_loss 0.005096,Time used 0.002976s\n",
      "batch 2121, train_loss 0.004706,Time used 0.003473s\n",
      "batch 2122, train_loss 0.004747,Time used 0.002976s\n",
      "batch 2123, train_loss 0.004831,Time used 0.002480s\n",
      "batch 2124, train_loss 0.004880,Time used 0.002975s\n",
      "batch 2125, train_loss 0.005008,Time used 0.002976s\n",
      "batch 2126, train_loss 0.004198,Time used 0.003472s\n",
      "batch 2127, train_loss 0.004433,Time used 0.002976s\n",
      "batch 2128, train_loss 0.003813,Time used 0.002976s\n",
      "batch 2129, train_loss 0.003702,Time used 0.002480s\n",
      "batch 2130, train_loss 0.004501,Time used 0.002976s\n",
      "batch 2131, train_loss 0.003360,Time used 0.002481s\n",
      "batch 2132, train_loss 0.004679,Time used 0.002480s\n",
      "batch 2133, train_loss 0.004189,Time used 0.002973s\n",
      "batch 2134, train_loss 0.004294,Time used 0.002976s\n",
      "batch 2135, train_loss 0.003439,Time used 0.002967s\n",
      "batch 2136, train_loss 0.003799,Time used 0.003473s\n",
      "batch 2137, train_loss 0.002859,Time used 0.002975s\n",
      "batch 2138, train_loss 0.003346,Time used 0.002976s\n",
      "batch 2139, train_loss 0.003791,Time used 0.003468s\n",
      "batch 2140, train_loss 0.003549,Time used 0.002480s\n",
      "batch 2141, train_loss 0.004242,Time used 0.002976s\n",
      "batch 2142, train_loss 0.003966,Time used 0.002976s\n",
      "batch 2143, train_loss 0.005445,Time used 0.002976s\n",
      "batch 2144, train_loss 0.004782,Time used 0.002976s\n",
      "batch 2145, train_loss 0.003697,Time used 0.002976s\n",
      "batch 2146, train_loss 0.003618,Time used 0.002976s\n",
      "batch 2147, train_loss 0.004391,Time used 0.002976s\n",
      "batch 2148, train_loss 0.004150,Time used 0.002977s\n",
      "batch 2149, train_loss 0.004711,Time used 0.002976s\n",
      "batch 2150, train_loss 0.003996,Time used 0.003472s\n",
      "batch 2151, train_loss 0.004382,Time used 0.002976s\n",
      "batch 2152, train_loss 0.004233,Time used 0.003472s\n",
      "batch 2153, train_loss 0.003482,Time used 0.003968s\n",
      "batch 2154, train_loss 0.004424,Time used 0.002976s\n",
      "batch 2155, train_loss 0.003622,Time used 0.003977s\n",
      "batch 2156, train_loss 0.003658,Time used 0.003968s\n",
      "batch 2157, train_loss 0.004417,Time used 0.002975s\n",
      "batch 2158, train_loss 0.004313,Time used 0.002976s\n",
      "batch 2159, train_loss 0.005333,Time used 0.003472s\n",
      "batch 2160, train_loss 0.002529,Time used 0.003968s\n",
      "batch 2161, train_loss 0.003075,Time used 0.003472s\n",
      "batch 2162, train_loss 0.005678,Time used 0.003472s\n",
      "batch 2163, train_loss 0.003769,Time used 0.012400s\n",
      "batch 2164, train_loss 0.004494,Time used 0.003472s\n",
      "batch 2165, train_loss 0.004846,Time used 0.002976s\n",
      "batch 2166, train_loss 0.004174,Time used 0.003976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2167, train_loss 0.003976,Time used 0.005451s\n",
      "batch 2168, train_loss 0.003655,Time used 0.002976s\n",
      "batch 2169, train_loss 0.004908,Time used 0.003471s\n",
      "batch 2170, train_loss 0.004143,Time used 0.004462s\n",
      "batch 2171, train_loss 0.003797,Time used 0.014875s\n",
      "batch 2172, train_loss 0.003466,Time used 0.002976s\n",
      "batch 2173, train_loss 0.004205,Time used 0.003471s\n",
      "batch 2174, train_loss 0.004603,Time used 0.003968s\n",
      "batch 2175, train_loss 0.003565,Time used 0.003473s\n",
      "batch 2176, train_loss 0.003913,Time used 0.003472s\n",
      "batch 2177, train_loss 0.004197,Time used 0.003472s\n",
      "batch 2178, train_loss 0.004421,Time used 0.003968s\n",
      "batch 2179, train_loss 0.003829,Time used 0.002976s\n",
      "batch 2180, train_loss 0.004618,Time used 0.002977s\n",
      "batch 2181, train_loss 0.004234,Time used 0.003968s\n",
      "batch 2182, train_loss 0.003693,Time used 0.003968s\n",
      "batch 2183, train_loss 0.004713,Time used 0.003968s\n",
      "batch 2184, train_loss 0.003764,Time used 0.003473s\n",
      "batch 2185, train_loss 0.003795,Time used 0.003472s\n",
      "batch 2186, train_loss 0.004697,Time used 0.003472s\n",
      "batch 2187, train_loss 0.004255,Time used 0.003472s\n",
      "batch 2188, train_loss 0.003949,Time used 0.002976s\n",
      "batch 2189, train_loss 0.004477,Time used 0.003472s\n",
      "batch 2190, train_loss 0.004596,Time used 0.003472s\n",
      "batch 2191, train_loss 0.004392,Time used 0.002976s\n",
      "batch 2192, train_loss 0.004387,Time used 0.002976s\n",
      "batch 2193, train_loss 0.003600,Time used 0.002976s\n",
      "batch 2194, train_loss 0.004499,Time used 0.002976s\n",
      "batch 2195, train_loss 0.005474,Time used 0.002480s\n",
      "batch 2196, train_loss 0.004397,Time used 0.003472s\n",
      "batch 2197, train_loss 0.004159,Time used 0.003472s\n",
      "batch 2198, train_loss 0.004195,Time used 0.002967s\n",
      "batch 2199, train_loss 0.003174,Time used 0.002484s\n",
      "batch 2200, train_loss 0.003416,Time used 0.002474s\n",
      "***************************test_batch 2200, test_rmse_loss 0.064693,test_mae_loss 0.046855,test_mape_loss 14.506413,Time used 0.010912s\n",
      "batch 2201, train_loss 0.004962,Time used 0.002480s\n",
      "batch 2202, train_loss 0.004538,Time used 0.002976s\n",
      "batch 2203, train_loss 0.003615,Time used 0.002977s\n",
      "batch 2204, train_loss 0.003560,Time used 0.002976s\n",
      "batch 2205, train_loss 0.004063,Time used 0.002976s\n",
      "batch 2206, train_loss 0.004863,Time used 0.004464s\n",
      "batch 2207, train_loss 0.003576,Time used 0.002976s\n",
      "batch 2208, train_loss 0.003769,Time used 0.002480s\n",
      "batch 2209, train_loss 0.004378,Time used 0.003472s\n",
      "batch 2210, train_loss 0.003870,Time used 0.002480s\n",
      "batch 2211, train_loss 0.004657,Time used 0.002481s\n",
      "batch 2212, train_loss 0.004565,Time used 0.002976s\n",
      "batch 2213, train_loss 0.004664,Time used 0.003469s\n",
      "batch 2214, train_loss 0.006542,Time used 0.003472s\n",
      "batch 2215, train_loss 0.004502,Time used 0.002480s\n",
      "batch 2216, train_loss 0.004305,Time used 0.003472s\n",
      "batch 2217, train_loss 0.006038,Time used 0.003472s\n",
      "batch 2218, train_loss 0.003888,Time used 0.002976s\n",
      "batch 2219, train_loss 0.003268,Time used 0.002972s\n",
      "batch 2220, train_loss 0.003895,Time used 0.002480s\n",
      "batch 2221, train_loss 0.003355,Time used 0.002976s\n",
      "batch 2222, train_loss 0.004599,Time used 0.002973s\n",
      "batch 2223, train_loss 0.003741,Time used 0.002974s\n",
      "batch 2224, train_loss 0.003379,Time used 0.002975s\n",
      "batch 2225, train_loss 0.003983,Time used 0.003472s\n",
      "batch 2226, train_loss 0.004387,Time used 0.002976s\n",
      "batch 2227, train_loss 0.004323,Time used 0.002976s\n",
      "batch 2228, train_loss 0.003932,Time used 0.002976s\n",
      "batch 2229, train_loss 0.004627,Time used 0.003473s\n",
      "batch 2230, train_loss 0.004102,Time used 0.003471s\n",
      "batch 2231, train_loss 0.004509,Time used 0.002976s\n",
      "batch 2232, train_loss 0.003874,Time used 0.002480s\n",
      "batch 2233, train_loss 0.004472,Time used 0.002975s\n",
      "batch 2234, train_loss 0.003694,Time used 0.002976s\n",
      "batch 2235, train_loss 0.004044,Time used 0.003473s\n",
      "batch 2236, train_loss 0.003825,Time used 0.002976s\n",
      "batch 2237, train_loss 0.003612,Time used 0.003472s\n",
      "batch 2238, train_loss 0.005153,Time used 0.003473s\n",
      "batch 2239, train_loss 0.004382,Time used 0.012400s\n",
      "batch 2240, train_loss 0.003505,Time used 0.002976s\n",
      "batch 2241, train_loss 0.003662,Time used 0.002975s\n",
      "batch 2242, train_loss 0.004737,Time used 0.003472s\n",
      "batch 2243, train_loss 0.004020,Time used 0.002976s\n",
      "batch 2244, train_loss 0.006030,Time used 0.003472s\n",
      "batch 2245, train_loss 0.004393,Time used 0.003969s\n",
      "batch 2246, train_loss 0.003782,Time used 0.003969s\n",
      "batch 2247, train_loss 0.004356,Time used 0.003473s\n",
      "batch 2248, train_loss 0.004011,Time used 0.002976s\n",
      "batch 2249, train_loss 0.003521,Time used 0.003472s\n",
      "batch 2250, train_loss 0.004147,Time used 0.003472s\n",
      "batch 2251, train_loss 0.003810,Time used 0.003968s\n",
      "batch 2252, train_loss 0.003440,Time used 0.003472s\n",
      "batch 2253, train_loss 0.004645,Time used 0.003472s\n",
      "batch 2254, train_loss 0.002852,Time used 0.003472s\n",
      "batch 2255, train_loss 0.004674,Time used 0.002972s\n",
      "batch 2256, train_loss 0.005336,Time used 0.003472s\n",
      "batch 2257, train_loss 0.004203,Time used 0.002976s\n",
      "batch 2258, train_loss 0.006389,Time used 0.003472s\n",
      "batch 2259, train_loss 0.003940,Time used 0.003472s\n",
      "batch 2260, train_loss 0.004817,Time used 0.002481s\n",
      "batch 2261, train_loss 0.004076,Time used 0.003975s\n",
      "batch 2262, train_loss 0.004179,Time used 0.003479s\n",
      "batch 2263, train_loss 0.003533,Time used 0.003472s\n",
      "batch 2264, train_loss 0.004003,Time used 0.003472s\n",
      "batch 2265, train_loss 0.004862,Time used 0.003472s\n",
      "batch 2266, train_loss 0.004014,Time used 0.002975s\n",
      "batch 2267, train_loss 0.003400,Time used 0.002976s\n",
      "batch 2268, train_loss 0.006661,Time used 0.003473s\n",
      "batch 2269, train_loss 0.003856,Time used 0.002976s\n",
      "batch 2270, train_loss 0.004046,Time used 0.002976s\n",
      "batch 2271, train_loss 0.005106,Time used 0.002976s\n",
      "batch 2272, train_loss 0.003701,Time used 0.002976s\n",
      "batch 2273, train_loss 0.004524,Time used 0.002976s\n",
      "batch 2274, train_loss 0.004187,Time used 0.003472s\n",
      "batch 2275, train_loss 0.004147,Time used 0.002976s\n",
      "batch 2276, train_loss 0.004232,Time used 0.002976s\n",
      "batch 2277, train_loss 0.004376,Time used 0.002976s\n",
      "batch 2278, train_loss 0.004348,Time used 0.002977s\n",
      "batch 2279, train_loss 0.003602,Time used 0.002970s\n",
      "batch 2280, train_loss 0.004364,Time used 0.002976s\n",
      "batch 2281, train_loss 0.003951,Time used 0.002480s\n",
      "batch 2282, train_loss 0.004297,Time used 0.002976s\n",
      "batch 2283, train_loss 0.004154,Time used 0.002480s\n",
      "batch 2284, train_loss 0.005777,Time used 0.003471s\n",
      "batch 2285, train_loss 0.004660,Time used 0.002977s\n",
      "batch 2286, train_loss 0.003817,Time used 0.002976s\n",
      "batch 2287, train_loss 0.004476,Time used 0.003472s\n",
      "batch 2288, train_loss 0.003448,Time used 0.003471s\n",
      "batch 2289, train_loss 0.004544,Time used 0.002975s\n",
      "batch 2290, train_loss 0.005126,Time used 0.002481s\n",
      "batch 2291, train_loss 0.003634,Time used 0.002977s\n",
      "batch 2292, train_loss 0.003525,Time used 0.002481s\n",
      "batch 2293, train_loss 0.004772,Time used 0.002976s\n",
      "batch 2294, train_loss 0.003517,Time used 0.002987s\n",
      "batch 2295, train_loss 0.004115,Time used 0.002976s\n",
      "batch 2296, train_loss 0.003653,Time used 0.003468s\n",
      "batch 2297, train_loss 0.003653,Time used 0.002976s\n",
      "batch 2298, train_loss 0.003160,Time used 0.003472s\n",
      "batch 2299, train_loss 0.003591,Time used 0.002976s\n",
      "batch 2300, train_loss 0.005664,Time used 0.002976s\n",
      "***************************test_batch 2300, test_rmse_loss 0.064423,test_mae_loss 0.046526,test_mape_loss 14.166618,Time used 0.009920s\n",
      "batch 2301, train_loss 0.005020,Time used 0.002976s\n",
      "batch 2302, train_loss 0.003631,Time used 0.002975s\n",
      "batch 2303, train_loss 0.003884,Time used 0.002976s\n",
      "batch 2304, train_loss 0.003295,Time used 0.002480s\n",
      "batch 2305, train_loss 0.003190,Time used 0.002976s\n",
      "batch 2306, train_loss 0.003891,Time used 0.002967s\n",
      "batch 2307, train_loss 0.004844,Time used 0.002976s\n",
      "batch 2308, train_loss 0.004684,Time used 0.002976s\n",
      "batch 2309, train_loss 0.005658,Time used 0.002481s\n",
      "batch 2310, train_loss 0.004003,Time used 0.003472s\n",
      "batch 2311, train_loss 0.003220,Time used 0.002975s\n",
      "batch 2312, train_loss 0.004463,Time used 0.003472s\n",
      "batch 2313, train_loss 0.003638,Time used 0.004464s\n",
      "batch 2314, train_loss 0.004532,Time used 0.003475s\n",
      "batch 2315, train_loss 0.004188,Time used 0.013888s\n",
      "batch 2316, train_loss 0.003797,Time used 0.003473s\n",
      "batch 2317, train_loss 0.004422,Time used 0.002975s\n",
      "batch 2318, train_loss 0.003597,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2319, train_loss 0.004667,Time used 0.002977s\n",
      "batch 2320, train_loss 0.003742,Time used 0.003472s\n",
      "batch 2321, train_loss 0.003737,Time used 0.003463s\n",
      "batch 2322, train_loss 0.002894,Time used 0.003472s\n",
      "batch 2323, train_loss 0.004519,Time used 0.002976s\n",
      "batch 2324, train_loss 0.004294,Time used 0.003472s\n",
      "batch 2325, train_loss 0.003816,Time used 0.011903s\n",
      "batch 2326, train_loss 0.003826,Time used 0.003472s\n",
      "batch 2327, train_loss 0.004542,Time used 0.002967s\n",
      "batch 2328, train_loss 0.004484,Time used 0.002982s\n",
      "batch 2329, train_loss 0.004396,Time used 0.002976s\n",
      "batch 2330, train_loss 0.005149,Time used 0.002972s\n",
      "batch 2331, train_loss 0.004485,Time used 0.002480s\n",
      "batch 2332, train_loss 0.004339,Time used 0.002971s\n",
      "batch 2333, train_loss 0.004517,Time used 0.003472s\n",
      "batch 2334, train_loss 0.004270,Time used 0.002976s\n",
      "batch 2335, train_loss 0.003323,Time used 0.002480s\n",
      "batch 2336, train_loss 0.003353,Time used 0.002976s\n",
      "batch 2337, train_loss 0.003585,Time used 0.002975s\n",
      "batch 2338, train_loss 0.005631,Time used 0.002976s\n",
      "batch 2339, train_loss 0.003524,Time used 0.002976s\n",
      "batch 2340, train_loss 0.004242,Time used 0.003472s\n",
      "batch 2341, train_loss 0.005440,Time used 0.002975s\n",
      "batch 2342, train_loss 0.004133,Time used 0.002976s\n",
      "batch 2343, train_loss 0.003983,Time used 0.002976s\n",
      "batch 2344, train_loss 0.004116,Time used 0.002480s\n",
      "batch 2345, train_loss 0.004154,Time used 0.002983s\n",
      "batch 2346, train_loss 0.004391,Time used 0.002480s\n",
      "batch 2347, train_loss 0.003880,Time used 0.002983s\n",
      "batch 2348, train_loss 0.004279,Time used 0.002972s\n",
      "batch 2349, train_loss 0.003897,Time used 0.002976s\n",
      "batch 2350, train_loss 0.004402,Time used 0.002976s\n",
      "batch 2351, train_loss 0.003900,Time used 0.002976s\n",
      "batch 2352, train_loss 0.004705,Time used 0.002976s\n",
      "batch 2353, train_loss 0.005234,Time used 0.003472s\n",
      "batch 2354, train_loss 0.004064,Time used 0.002975s\n",
      "batch 2355, train_loss 0.005405,Time used 0.002969s\n",
      "batch 2356, train_loss 0.003440,Time used 0.002975s\n",
      "batch 2357, train_loss 0.003094,Time used 0.002976s\n",
      "batch 2358, train_loss 0.003515,Time used 0.003472s\n",
      "batch 2359, train_loss 0.003975,Time used 0.002981s\n",
      "batch 2360, train_loss 0.004038,Time used 0.002983s\n",
      "batch 2361, train_loss 0.004221,Time used 0.002977s\n",
      "batch 2362, train_loss 0.004255,Time used 0.003473s\n",
      "batch 2363, train_loss 0.003153,Time used 0.002481s\n",
      "batch 2364, train_loss 0.004025,Time used 0.002481s\n",
      "batch 2365, train_loss 0.003513,Time used 0.002480s\n",
      "batch 2366, train_loss 0.004166,Time used 0.002976s\n",
      "batch 2367, train_loss 0.003443,Time used 0.002980s\n",
      "batch 2368, train_loss 0.003312,Time used 0.003472s\n",
      "batch 2369, train_loss 0.004324,Time used 0.003471s\n",
      "batch 2370, train_loss 0.003389,Time used 0.002977s\n",
      "batch 2371, train_loss 0.004595,Time used 0.002977s\n",
      "batch 2372, train_loss 0.004125,Time used 0.002977s\n",
      "batch 2373, train_loss 0.004535,Time used 0.002976s\n",
      "batch 2374, train_loss 0.003145,Time used 0.002977s\n",
      "batch 2375, train_loss 0.003934,Time used 0.004460s\n",
      "batch 2376, train_loss 0.004473,Time used 0.002976s\n",
      "batch 2377, train_loss 0.004669,Time used 0.002480s\n",
      "batch 2378, train_loss 0.003668,Time used 0.002975s\n",
      "batch 2379, train_loss 0.003881,Time used 0.002975s\n",
      "batch 2380, train_loss 0.003608,Time used 0.002976s\n",
      "batch 2381, train_loss 0.003978,Time used 0.002980s\n",
      "batch 2382, train_loss 0.005019,Time used 0.002480s\n",
      "batch 2383, train_loss 0.004187,Time used 0.002480s\n",
      "batch 2384, train_loss 0.003374,Time used 0.003472s\n",
      "batch 2385, train_loss 0.004762,Time used 0.002975s\n",
      "batch 2386, train_loss 0.003343,Time used 0.003472s\n",
      "batch 2387, train_loss 0.003443,Time used 0.002976s\n",
      "batch 2388, train_loss 0.002973,Time used 0.003472s\n",
      "batch 2389, train_loss 0.003844,Time used 0.002976s\n",
      "batch 2390, train_loss 0.004587,Time used 0.002976s\n",
      "batch 2391, train_loss 0.005624,Time used 0.003472s\n",
      "batch 2392, train_loss 0.003503,Time used 0.002976s\n",
      "batch 2393, train_loss 0.003280,Time used 0.003472s\n",
      "batch 2394, train_loss 0.004643,Time used 0.002975s\n",
      "batch 2395, train_loss 0.003841,Time used 0.002477s\n",
      "batch 2396, train_loss 0.003785,Time used 0.002976s\n",
      "batch 2397, train_loss 0.004395,Time used 0.002977s\n",
      "batch 2398, train_loss 0.003826,Time used 0.002976s\n",
      "batch 2399, train_loss 0.005803,Time used 0.002976s\n",
      "batch 2400, train_loss 0.004730,Time used 0.002479s\n",
      "***************************test_batch 2400, test_rmse_loss 0.064053,test_mae_loss 0.046266,test_mape_loss 14.132562,Time used 0.010912s\n",
      "batch 2401, train_loss 0.003691,Time used 0.003482s\n",
      "batch 2402, train_loss 0.003478,Time used 0.002480s\n",
      "batch 2403, train_loss 0.003294,Time used 0.003472s\n",
      "batch 2404, train_loss 0.004853,Time used 0.002976s\n",
      "batch 2405, train_loss 0.004386,Time used 0.002976s\n",
      "batch 2406, train_loss 0.004202,Time used 0.002975s\n",
      "batch 2407, train_loss 0.004169,Time used 0.002976s\n",
      "batch 2408, train_loss 0.002908,Time used 0.002976s\n",
      "batch 2409, train_loss 0.003966,Time used 0.002476s\n",
      "batch 2410, train_loss 0.004109,Time used 0.003968s\n",
      "batch 2411, train_loss 0.003431,Time used 0.003472s\n",
      "batch 2412, train_loss 0.003295,Time used 0.004463s\n",
      "batch 2413, train_loss 0.005095,Time used 0.002490s\n",
      "batch 2414, train_loss 0.005785,Time used 0.012896s\n",
      "batch 2415, train_loss 0.003880,Time used 0.002976s\n",
      "batch 2416, train_loss 0.003696,Time used 0.002975s\n",
      "batch 2417, train_loss 0.003789,Time used 0.003471s\n",
      "batch 2418, train_loss 0.003950,Time used 0.002480s\n",
      "batch 2419, train_loss 0.004298,Time used 0.002480s\n",
      "batch 2420, train_loss 0.004268,Time used 0.002976s\n",
      "batch 2421, train_loss 0.005891,Time used 0.002480s\n",
      "batch 2422, train_loss 0.003956,Time used 0.002976s\n",
      "batch 2423, train_loss 0.004186,Time used 0.002976s\n",
      "batch 2424, train_loss 0.003835,Time used 0.002973s\n",
      "batch 2425, train_loss 0.003874,Time used 0.003480s\n",
      "batch 2426, train_loss 0.003926,Time used 0.002976s\n",
      "batch 2427, train_loss 0.004543,Time used 0.002481s\n",
      "batch 2428, train_loss 0.004369,Time used 0.002976s\n",
      "batch 2429, train_loss 0.003170,Time used 0.002976s\n",
      "batch 2430, train_loss 0.002976,Time used 0.002975s\n",
      "batch 2431, train_loss 0.004307,Time used 0.002480s\n",
      "batch 2432, train_loss 0.004108,Time used 0.002976s\n",
      "batch 2433, train_loss 0.003560,Time used 0.002975s\n",
      "batch 2434, train_loss 0.004136,Time used 0.002976s\n",
      "batch 2435, train_loss 0.003710,Time used 0.002976s\n",
      "batch 2436, train_loss 0.004708,Time used 0.003472s\n",
      "batch 2437, train_loss 0.004017,Time used 0.002481s\n",
      "batch 2438, train_loss 0.006256,Time used 0.002976s\n",
      "batch 2439, train_loss 0.002874,Time used 0.003472s\n",
      "batch 2440, train_loss 0.003504,Time used 0.002976s\n",
      "batch 2441, train_loss 0.004073,Time used 0.002976s\n",
      "batch 2442, train_loss 0.003373,Time used 0.002481s\n",
      "batch 2443, train_loss 0.003576,Time used 0.002976s\n",
      "batch 2444, train_loss 0.003529,Time used 0.003472s\n",
      "batch 2445, train_loss 0.003825,Time used 0.002976s\n",
      "batch 2446, train_loss 0.004764,Time used 0.002480s\n",
      "batch 2447, train_loss 0.004311,Time used 0.002977s\n",
      "batch 2448, train_loss 0.004310,Time used 0.002976s\n",
      "batch 2449, train_loss 0.004419,Time used 0.002978s\n",
      "batch 2450, train_loss 0.004226,Time used 0.002975s\n",
      "batch 2451, train_loss 0.004815,Time used 0.002976s\n",
      "batch 2452, train_loss 0.004092,Time used 0.003472s\n",
      "batch 2453, train_loss 0.003776,Time used 0.002975s\n",
      "batch 2454, train_loss 0.004429,Time used 0.002976s\n",
      "batch 2455, train_loss 0.004350,Time used 0.002976s\n",
      "batch 2456, train_loss 0.004078,Time used 0.002980s\n",
      "batch 2457, train_loss 0.004222,Time used 0.002975s\n",
      "batch 2458, train_loss 0.003763,Time used 0.002976s\n",
      "batch 2459, train_loss 0.004117,Time used 0.002976s\n",
      "batch 2460, train_loss 0.004468,Time used 0.002976s\n",
      "batch 2461, train_loss 0.004508,Time used 0.002976s\n",
      "batch 2462, train_loss 0.003952,Time used 0.003472s\n",
      "batch 2463, train_loss 0.003471,Time used 0.002976s\n",
      "batch 2464, train_loss 0.004264,Time used 0.002977s\n",
      "batch 2465, train_loss 0.005699,Time used 0.003472s\n",
      "batch 2466, train_loss 0.003621,Time used 0.003472s\n",
      "batch 2467, train_loss 0.003807,Time used 0.003473s\n",
      "batch 2468, train_loss 0.004026,Time used 0.002977s\n",
      "batch 2469, train_loss 0.003339,Time used 0.003472s\n",
      "batch 2470, train_loss 0.003816,Time used 0.003472s\n",
      "batch 2471, train_loss 0.003985,Time used 0.002976s\n",
      "batch 2472, train_loss 0.003598,Time used 0.002979s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2473, train_loss 0.003256,Time used 0.003467s\n",
      "batch 2474, train_loss 0.004502,Time used 0.002976s\n",
      "batch 2475, train_loss 0.004842,Time used 0.002481s\n",
      "batch 2476, train_loss 0.003712,Time used 0.002976s\n",
      "batch 2477, train_loss 0.003395,Time used 0.003472s\n",
      "batch 2478, train_loss 0.004446,Time used 0.003472s\n",
      "batch 2479, train_loss 0.004070,Time used 0.002480s\n",
      "batch 2480, train_loss 0.004878,Time used 0.002976s\n",
      "batch 2481, train_loss 0.003993,Time used 0.002980s\n",
      "batch 2482, train_loss 0.003936,Time used 0.002481s\n",
      "batch 2483, train_loss 0.004152,Time used 0.002480s\n",
      "batch 2484, train_loss 0.008660,Time used 0.002976s\n",
      "batch 2485, train_loss 0.003469,Time used 0.002976s\n",
      "batch 2486, train_loss 0.003857,Time used 0.003472s\n",
      "batch 2487, train_loss 0.003805,Time used 0.002977s\n",
      "batch 2488, train_loss 0.003792,Time used 0.002976s\n",
      "batch 2489, train_loss 0.004029,Time used 0.002975s\n",
      "batch 2490, train_loss 0.004049,Time used 0.002976s\n",
      "batch 2491, train_loss 0.003282,Time used 0.003472s\n",
      "batch 2492, train_loss 0.003936,Time used 0.002480s\n",
      "batch 2493, train_loss 0.003705,Time used 0.002976s\n",
      "batch 2494, train_loss 0.003605,Time used 0.002980s\n",
      "batch 2495, train_loss 0.004190,Time used 0.003477s\n",
      "batch 2496, train_loss 0.003531,Time used 0.003472s\n",
      "batch 2497, train_loss 0.004693,Time used 0.003472s\n",
      "batch 2498, train_loss 0.003221,Time used 0.002976s\n",
      "batch 2499, train_loss 0.004282,Time used 0.002480s\n",
      "batch 2500, train_loss 0.003119,Time used 0.002976s\n",
      "***************************test_batch 2500, test_rmse_loss 0.063764,test_mae_loss 0.046170,test_mape_loss 14.378660,Time used 0.013402s\n",
      "batch 2501, train_loss 0.003828,Time used 0.003969s\n",
      "batch 2502, train_loss 0.003354,Time used 0.012401s\n",
      "batch 2503, train_loss 0.003219,Time used 0.002976s\n",
      "batch 2504, train_loss 0.003877,Time used 0.002976s\n",
      "batch 2505, train_loss 0.003989,Time used 0.002481s\n",
      "batch 2506, train_loss 0.004296,Time used 0.002977s\n",
      "batch 2507, train_loss 0.003972,Time used 0.002976s\n",
      "batch 2508, train_loss 0.003435,Time used 0.002976s\n",
      "batch 2509, train_loss 0.004401,Time used 0.002976s\n",
      "batch 2510, train_loss 0.003311,Time used 0.002484s\n",
      "batch 2511, train_loss 0.005199,Time used 0.002976s\n",
      "batch 2512, train_loss 0.004379,Time used 0.002977s\n",
      "batch 2513, train_loss 0.003839,Time used 0.002976s\n",
      "batch 2514, train_loss 0.004786,Time used 0.003968s\n",
      "batch 2515, train_loss 0.005075,Time used 0.002976s\n",
      "batch 2516, train_loss 0.004652,Time used 0.002975s\n",
      "batch 2517, train_loss 0.004367,Time used 0.002975s\n",
      "batch 2518, train_loss 0.004856,Time used 0.002481s\n",
      "batch 2519, train_loss 0.004273,Time used 0.002976s\n",
      "batch 2520, train_loss 0.004876,Time used 0.003468s\n",
      "batch 2521, train_loss 0.004865,Time used 0.003472s\n",
      "batch 2522, train_loss 0.004042,Time used 0.002976s\n",
      "batch 2523, train_loss 0.004331,Time used 0.002976s\n",
      "batch 2524, train_loss 0.004192,Time used 0.003472s\n",
      "batch 2525, train_loss 0.004794,Time used 0.002979s\n",
      "batch 2526, train_loss 0.004450,Time used 0.003472s\n",
      "batch 2527, train_loss 0.004209,Time used 0.002479s\n",
      "batch 2528, train_loss 0.003160,Time used 0.002975s\n",
      "batch 2529, train_loss 0.003469,Time used 0.002976s\n",
      "batch 2530, train_loss 0.003662,Time used 0.002976s\n",
      "batch 2531, train_loss 0.004557,Time used 0.003968s\n",
      "batch 2532, train_loss 0.004889,Time used 0.003472s\n",
      "batch 2533, train_loss 0.004433,Time used 0.002975s\n",
      "batch 2534, train_loss 0.004266,Time used 0.002964s\n",
      "batch 2535, train_loss 0.003735,Time used 0.011904s\n",
      "batch 2536, train_loss 0.004756,Time used 0.002971s\n",
      "batch 2537, train_loss 0.002948,Time used 0.002976s\n",
      "batch 2538, train_loss 0.005351,Time used 0.002480s\n",
      "batch 2539, train_loss 0.003747,Time used 0.002976s\n",
      "batch 2540, train_loss 0.004782,Time used 0.002480s\n",
      "batch 2541, train_loss 0.003921,Time used 0.002975s\n",
      "batch 2542, train_loss 0.004265,Time used 0.002977s\n",
      "batch 2543, train_loss 0.005062,Time used 0.002976s\n",
      "batch 2544, train_loss 0.004487,Time used 0.003472s\n",
      "batch 2545, train_loss 0.003953,Time used 0.003472s\n",
      "batch 2546, train_loss 0.003937,Time used 0.002481s\n",
      "batch 2547, train_loss 0.004117,Time used 0.002976s\n",
      "batch 2548, train_loss 0.004483,Time used 0.002975s\n",
      "batch 2549, train_loss 0.005263,Time used 0.002976s\n",
      "batch 2550, train_loss 0.004030,Time used 0.003472s\n",
      "batch 2551, train_loss 0.003317,Time used 0.002976s\n",
      "batch 2552, train_loss 0.003843,Time used 0.002976s\n",
      "batch 2553, train_loss 0.003788,Time used 0.002967s\n",
      "batch 2554, train_loss 0.004013,Time used 0.002481s\n",
      "batch 2555, train_loss 0.003821,Time used 0.002480s\n",
      "batch 2556, train_loss 0.004231,Time used 0.003472s\n",
      "batch 2557, train_loss 0.004405,Time used 0.002975s\n",
      "batch 2558, train_loss 0.003886,Time used 0.003472s\n",
      "batch 2559, train_loss 0.003715,Time used 0.002975s\n",
      "batch 2560, train_loss 0.004100,Time used 0.002481s\n",
      "batch 2561, train_loss 0.004091,Time used 0.002481s\n",
      "batch 2562, train_loss 0.003615,Time used 0.002480s\n",
      "batch 2563, train_loss 0.003989,Time used 0.002976s\n",
      "batch 2564, train_loss 0.004829,Time used 0.002481s\n",
      "batch 2565, train_loss 0.003448,Time used 0.002481s\n",
      "batch 2566, train_loss 0.003292,Time used 0.002976s\n",
      "batch 2567, train_loss 0.003702,Time used 0.002976s\n",
      "batch 2568, train_loss 0.004221,Time used 0.002975s\n",
      "batch 2569, train_loss 0.004256,Time used 0.003472s\n",
      "batch 2570, train_loss 0.004246,Time used 0.002976s\n",
      "batch 2571, train_loss 0.002882,Time used 0.002480s\n",
      "batch 2572, train_loss 0.005669,Time used 0.002479s\n",
      "batch 2573, train_loss 0.003895,Time used 0.002976s\n",
      "batch 2574, train_loss 0.003826,Time used 0.002976s\n",
      "batch 2575, train_loss 0.004007,Time used 0.002976s\n",
      "batch 2576, train_loss 0.003577,Time used 0.002976s\n",
      "batch 2577, train_loss 0.003298,Time used 0.003472s\n",
      "batch 2578, train_loss 0.003789,Time used 0.002480s\n",
      "batch 2579, train_loss 0.004254,Time used 0.002976s\n",
      "batch 2580, train_loss 0.002991,Time used 0.002977s\n",
      "batch 2581, train_loss 0.003712,Time used 0.003464s\n",
      "batch 2582, train_loss 0.004379,Time used 0.003472s\n",
      "batch 2583, train_loss 0.003901,Time used 0.002480s\n",
      "batch 2584, train_loss 0.004083,Time used 0.002973s\n",
      "batch 2585, train_loss 0.004395,Time used 0.002976s\n",
      "batch 2586, train_loss 0.004034,Time used 0.002975s\n",
      "batch 2587, train_loss 0.004405,Time used 0.002976s\n",
      "batch 2588, train_loss 0.003654,Time used 0.002976s\n",
      "batch 2589, train_loss 0.005117,Time used 0.002976s\n",
      "batch 2590, train_loss 0.004034,Time used 0.002480s\n",
      "batch 2591, train_loss 0.003504,Time used 0.002976s\n",
      "batch 2592, train_loss 0.009033,Time used 0.003473s\n",
      "batch 2593, train_loss 0.004174,Time used 0.003467s\n",
      "batch 2594, train_loss 0.003792,Time used 0.002975s\n",
      "batch 2595, train_loss 0.003736,Time used 0.002976s\n",
      "batch 2596, train_loss 0.004467,Time used 0.002975s\n",
      "batch 2597, train_loss 0.003932,Time used 0.012400s\n",
      "batch 2598, train_loss 0.003876,Time used 0.002976s\n",
      "batch 2599, train_loss 0.004174,Time used 0.002986s\n",
      "batch 2600, train_loss 0.003375,Time used 0.002976s\n",
      "***************************test_batch 2600, test_rmse_loss 0.063544,test_mae_loss 0.045950,test_mape_loss 14.217381,Time used 0.010912s\n",
      "batch 2601, train_loss 0.003195,Time used 0.002976s\n",
      "batch 2602, train_loss 0.004324,Time used 0.003471s\n",
      "batch 2603, train_loss 0.003937,Time used 0.003472s\n",
      "batch 2604, train_loss 0.003619,Time used 0.002971s\n",
      "batch 2605, train_loss 0.003710,Time used 0.003471s\n",
      "batch 2606, train_loss 0.004144,Time used 0.003472s\n",
      "batch 2607, train_loss 0.004021,Time used 0.003472s\n",
      "batch 2608, train_loss 0.003624,Time used 0.002976s\n",
      "batch 2609, train_loss 0.003459,Time used 0.003472s\n",
      "batch 2610, train_loss 0.003893,Time used 0.002976s\n",
      "batch 2611, train_loss 0.003160,Time used 0.003472s\n",
      "batch 2612, train_loss 0.004824,Time used 0.003471s\n",
      "batch 2613, train_loss 0.003622,Time used 0.003472s\n",
      "batch 2614, train_loss 0.004381,Time used 0.003979s\n",
      "batch 2615, train_loss 0.004098,Time used 0.002975s\n",
      "batch 2616, train_loss 0.003642,Time used 0.003472s\n",
      "batch 2617, train_loss 0.004296,Time used 0.003472s\n",
      "batch 2618, train_loss 0.004729,Time used 0.003472s\n",
      "batch 2619, train_loss 0.004249,Time used 0.003472s\n",
      "batch 2620, train_loss 0.004726,Time used 0.003472s\n",
      "batch 2621, train_loss 0.004313,Time used 0.003472s\n",
      "batch 2622, train_loss 0.003287,Time used 0.002982s\n",
      "batch 2623, train_loss 0.003855,Time used 0.003472s\n",
      "batch 2624, train_loss 0.002877,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2625, train_loss 0.004614,Time used 0.003472s\n",
      "batch 2626, train_loss 0.004701,Time used 0.002977s\n",
      "batch 2627, train_loss 0.005119,Time used 0.003471s\n",
      "batch 2628, train_loss 0.003024,Time used 0.003472s\n",
      "batch 2629, train_loss 0.004567,Time used 0.003472s\n",
      "batch 2630, train_loss 0.004234,Time used 0.003473s\n",
      "batch 2631, train_loss 0.004488,Time used 0.003473s\n",
      "batch 2632, train_loss 0.004105,Time used 0.002977s\n",
      "batch 2633, train_loss 0.003678,Time used 0.002976s\n",
      "batch 2634, train_loss 0.003855,Time used 0.002976s\n",
      "batch 2635, train_loss 0.005176,Time used 0.002976s\n",
      "batch 2636, train_loss 0.004783,Time used 0.002976s\n",
      "batch 2637, train_loss 0.004225,Time used 0.002977s\n",
      "batch 2638, train_loss 0.003487,Time used 0.003472s\n",
      "batch 2639, train_loss 0.004539,Time used 0.002976s\n",
      "batch 2640, train_loss 0.003855,Time used 0.002972s\n",
      "batch 2641, train_loss 0.003330,Time used 0.002976s\n",
      "batch 2642, train_loss 0.003503,Time used 0.003478s\n",
      "batch 2643, train_loss 0.005100,Time used 0.002485s\n",
      "batch 2644, train_loss 0.003252,Time used 0.003472s\n",
      "batch 2645, train_loss 0.005260,Time used 0.002976s\n",
      "batch 2646, train_loss 0.001101,Time used 0.002480s\n",
      "batch 2647, train_loss 0.003381,Time used 0.002976s\n",
      "batch 2648, train_loss 0.004830,Time used 0.002975s\n",
      "batch 2649, train_loss 0.004266,Time used 0.002975s\n",
      "batch 2650, train_loss 0.004053,Time used 0.002976s\n",
      "batch 2651, train_loss 0.005579,Time used 0.003470s\n",
      "batch 2652, train_loss 0.003462,Time used 0.004465s\n",
      "batch 2653, train_loss 0.003423,Time used 0.003967s\n",
      "batch 2654, train_loss 0.003521,Time used 0.003477s\n",
      "batch 2655, train_loss 0.004591,Time used 0.003472s\n",
      "batch 2656, train_loss 0.004077,Time used 0.002480s\n",
      "batch 2657, train_loss 0.003845,Time used 0.003472s\n",
      "batch 2658, train_loss 0.002872,Time used 0.003472s\n",
      "batch 2659, train_loss 0.003510,Time used 0.003468s\n",
      "batch 2660, train_loss 0.003412,Time used 0.003967s\n",
      "batch 2661, train_loss 0.005297,Time used 0.002964s\n",
      "batch 2662, train_loss 0.005472,Time used 0.002976s\n",
      "batch 2663, train_loss 0.005251,Time used 0.002976s\n",
      "batch 2664, train_loss 0.003427,Time used 0.002975s\n",
      "batch 2665, train_loss 0.003162,Time used 0.002976s\n",
      "batch 2666, train_loss 0.004125,Time used 0.002479s\n",
      "batch 2667, train_loss 0.003765,Time used 0.002980s\n",
      "batch 2668, train_loss 0.004447,Time used 0.002976s\n",
      "batch 2669, train_loss 0.005361,Time used 0.002489s\n",
      "batch 2670, train_loss 0.003147,Time used 0.002976s\n",
      "batch 2671, train_loss 0.004163,Time used 0.002487s\n",
      "batch 2672, train_loss 0.004152,Time used 0.002976s\n",
      "batch 2673, train_loss 0.004137,Time used 0.003472s\n",
      "batch 2674, train_loss 0.003583,Time used 0.003472s\n",
      "batch 2675, train_loss 0.003453,Time used 0.002977s\n",
      "batch 2676, train_loss 0.004272,Time used 0.003472s\n",
      "batch 2677, train_loss 0.003722,Time used 0.003472s\n",
      "batch 2678, train_loss 0.003673,Time used 0.011903s\n",
      "batch 2679, train_loss 0.003338,Time used 0.003472s\n",
      "batch 2680, train_loss 0.002944,Time used 0.002476s\n",
      "batch 2681, train_loss 0.004081,Time used 0.002481s\n",
      "batch 2682, train_loss 0.003707,Time used 0.002976s\n",
      "batch 2683, train_loss 0.003618,Time used 0.002976s\n",
      "batch 2684, train_loss 0.004676,Time used 0.002481s\n",
      "batch 2685, train_loss 0.004232,Time used 0.002976s\n",
      "batch 2686, train_loss 0.004054,Time used 0.002975s\n",
      "batch 2687, train_loss 0.004321,Time used 0.002976s\n",
      "batch 2688, train_loss 0.004012,Time used 0.002976s\n",
      "batch 2689, train_loss 0.003837,Time used 0.002975s\n",
      "batch 2690, train_loss 0.003783,Time used 0.003472s\n",
      "batch 2691, train_loss 0.004121,Time used 0.002976s\n",
      "batch 2692, train_loss 0.003713,Time used 0.002976s\n",
      "batch 2693, train_loss 0.003801,Time used 0.003472s\n",
      "batch 2694, train_loss 0.004217,Time used 0.002976s\n",
      "batch 2695, train_loss 0.005021,Time used 0.002977s\n",
      "batch 2696, train_loss 0.003420,Time used 0.002480s\n",
      "batch 2697, train_loss 0.004409,Time used 0.002976s\n",
      "batch 2698, train_loss 0.005160,Time used 0.002480s\n",
      "batch 2699, train_loss 0.004202,Time used 0.002976s\n",
      "batch 2700, train_loss 0.003674,Time used 0.002976s\n",
      "***************************test_batch 2700, test_rmse_loss 0.063433,test_mae_loss 0.046008,test_mape_loss 14.538002,Time used 0.010417s\n",
      "batch 2701, train_loss 0.003135,Time used 0.003471s\n",
      "batch 2702, train_loss 0.005143,Time used 0.002980s\n",
      "batch 2703, train_loss 0.004259,Time used 0.002985s\n",
      "batch 2704, train_loss 0.004008,Time used 0.003472s\n",
      "batch 2705, train_loss 0.003909,Time used 0.002480s\n",
      "batch 2706, train_loss 0.004839,Time used 0.002977s\n",
      "batch 2707, train_loss 0.003456,Time used 0.002976s\n",
      "batch 2708, train_loss 0.004342,Time used 0.002976s\n",
      "batch 2709, train_loss 0.004300,Time used 0.002976s\n",
      "batch 2710, train_loss 0.003187,Time used 0.002976s\n",
      "batch 2711, train_loss 0.003830,Time used 0.002976s\n",
      "batch 2712, train_loss 0.003528,Time used 0.002977s\n",
      "batch 2713, train_loss 0.003759,Time used 0.002976s\n",
      "batch 2714, train_loss 0.005670,Time used 0.002967s\n",
      "batch 2715, train_loss 0.003567,Time used 0.003968s\n",
      "batch 2716, train_loss 0.004011,Time used 0.002972s\n",
      "batch 2717, train_loss 0.004769,Time used 0.002976s\n",
      "batch 2718, train_loss 0.003809,Time used 0.002976s\n",
      "batch 2719, train_loss 0.003854,Time used 0.003472s\n",
      "batch 2720, train_loss 0.004251,Time used 0.003481s\n",
      "batch 2721, train_loss 0.004274,Time used 0.003472s\n",
      "batch 2722, train_loss 0.003480,Time used 0.003468s\n",
      "batch 2723, train_loss 0.003132,Time used 0.002976s\n",
      "batch 2724, train_loss 0.003601,Time used 0.003472s\n",
      "batch 2725, train_loss 0.004349,Time used 0.003473s\n",
      "batch 2726, train_loss 0.003931,Time used 0.003472s\n",
      "batch 2727, train_loss 0.003137,Time used 0.003472s\n",
      "batch 2728, train_loss 0.003800,Time used 0.003968s\n",
      "batch 2729, train_loss 0.004128,Time used 0.003471s\n",
      "batch 2730, train_loss 0.003874,Time used 0.002976s\n",
      "batch 2731, train_loss 0.004032,Time used 0.003472s\n",
      "batch 2732, train_loss 0.003967,Time used 0.003472s\n",
      "batch 2733, train_loss 0.005465,Time used 0.003471s\n",
      "batch 2734, train_loss 0.003768,Time used 0.002977s\n",
      "batch 2735, train_loss 0.003186,Time used 0.003472s\n",
      "batch 2736, train_loss 0.003443,Time used 0.002976s\n",
      "batch 2737, train_loss 0.004562,Time used 0.003473s\n",
      "batch 2738, train_loss 0.003951,Time used 0.003472s\n",
      "batch 2739, train_loss 0.003830,Time used 0.002976s\n",
      "batch 2740, train_loss 0.003838,Time used 0.003968s\n",
      "batch 2741, train_loss 0.004232,Time used 0.002976s\n",
      "batch 2742, train_loss 0.004629,Time used 0.003962s\n",
      "batch 2743, train_loss 0.004838,Time used 0.003473s\n",
      "batch 2744, train_loss 0.003852,Time used 0.004464s\n",
      "batch 2745, train_loss 0.003189,Time used 0.003969s\n",
      "batch 2746, train_loss 0.003992,Time used 0.002976s\n",
      "batch 2747, train_loss 0.004121,Time used 0.012400s\n",
      "batch 2748, train_loss 0.003940,Time used 0.002975s\n",
      "batch 2749, train_loss 0.004230,Time used 0.002976s\n",
      "batch 2750, train_loss 0.004663,Time used 0.003472s\n",
      "batch 2751, train_loss 0.003625,Time used 0.003968s\n",
      "batch 2752, train_loss 0.003646,Time used 0.003968s\n",
      "batch 2753, train_loss 0.003630,Time used 0.002972s\n",
      "batch 2754, train_loss 0.002004,Time used 0.003472s\n",
      "batch 2755, train_loss 0.003876,Time used 0.002976s\n",
      "batch 2756, train_loss 0.003635,Time used 0.004470s\n",
      "batch 2757, train_loss 0.003952,Time used 0.003460s\n",
      "batch 2758, train_loss 0.004158,Time used 0.003968s\n",
      "batch 2759, train_loss 0.003904,Time used 0.003476s\n",
      "batch 2760, train_loss 0.002968,Time used 0.005456s\n",
      "batch 2761, train_loss 0.003881,Time used 0.015376s\n",
      "batch 2762, train_loss 0.004272,Time used 0.002976s\n",
      "batch 2763, train_loss 0.004057,Time used 0.003477s\n",
      "batch 2764, train_loss 0.004080,Time used 0.002977s\n",
      "batch 2765, train_loss 0.004858,Time used 0.002976s\n",
      "batch 2766, train_loss 0.003784,Time used 0.003472s\n",
      "batch 2767, train_loss 0.004220,Time used 0.002976s\n",
      "batch 2768, train_loss 0.003245,Time used 0.003472s\n",
      "batch 2769, train_loss 0.004176,Time used 0.002977s\n",
      "batch 2770, train_loss 0.004663,Time used 0.003473s\n",
      "batch 2771, train_loss 0.003876,Time used 0.003968s\n",
      "batch 2772, train_loss 0.004950,Time used 0.002977s\n",
      "batch 2773, train_loss 0.004723,Time used 0.002976s\n",
      "batch 2774, train_loss 0.004571,Time used 0.002976s\n",
      "batch 2775, train_loss 0.003591,Time used 0.003472s\n",
      "batch 2776, train_loss 0.003941,Time used 0.003472s\n",
      "batch 2777, train_loss 0.003938,Time used 0.002976s\n",
      "batch 2778, train_loss 0.004561,Time used 0.003968s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2779, train_loss 0.004372,Time used 0.002968s\n",
      "batch 2780, train_loss 0.003882,Time used 0.002976s\n",
      "batch 2781, train_loss 0.004778,Time used 0.003472s\n",
      "batch 2782, train_loss 0.003365,Time used 0.002984s\n",
      "batch 2783, train_loss 0.003887,Time used 0.002975s\n",
      "batch 2784, train_loss 0.004043,Time used 0.003472s\n",
      "batch 2785, train_loss 0.004147,Time used 0.002976s\n",
      "batch 2786, train_loss 0.003175,Time used 0.002973s\n",
      "batch 2787, train_loss 0.004127,Time used 0.002976s\n",
      "batch 2788, train_loss 0.002974,Time used 0.002975s\n",
      "batch 2789, train_loss 0.003505,Time used 0.003473s\n",
      "batch 2790, train_loss 0.003345,Time used 0.003477s\n",
      "batch 2791, train_loss 0.004023,Time used 0.002480s\n",
      "batch 2792, train_loss 0.003431,Time used 0.002976s\n",
      "batch 2793, train_loss 0.003425,Time used 0.002976s\n",
      "batch 2794, train_loss 0.003708,Time used 0.002976s\n",
      "batch 2795, train_loss 0.003108,Time used 0.002976s\n",
      "batch 2796, train_loss 0.003706,Time used 0.003468s\n",
      "batch 2797, train_loss 0.004490,Time used 0.002975s\n",
      "batch 2798, train_loss 0.004412,Time used 0.002977s\n",
      "batch 2799, train_loss 0.003728,Time used 0.003474s\n",
      "batch 2800, train_loss 0.003956,Time used 0.002984s\n",
      "***************************test_batch 2800, test_rmse_loss 0.063190,test_mae_loss 0.045643,test_mape_loss 14.139511,Time used 0.010421s\n",
      "batch 2801, train_loss 0.004878,Time used 0.002976s\n",
      "batch 2802, train_loss 0.004564,Time used 0.002480s\n",
      "batch 2803, train_loss 0.003910,Time used 0.002976s\n",
      "batch 2804, train_loss 0.003792,Time used 0.002976s\n",
      "batch 2805, train_loss 0.003690,Time used 0.002977s\n",
      "batch 2806, train_loss 0.004479,Time used 0.003472s\n",
      "batch 2807, train_loss 0.004360,Time used 0.002975s\n",
      "batch 2808, train_loss 0.004266,Time used 0.002976s\n",
      "batch 2809, train_loss 0.004085,Time used 0.002480s\n",
      "batch 2810, train_loss 0.004649,Time used 0.002976s\n",
      "batch 2811, train_loss 0.002362,Time used 0.002976s\n",
      "batch 2812, train_loss 0.003584,Time used 0.002976s\n",
      "batch 2813, train_loss 0.005083,Time used 0.002976s\n",
      "batch 2814, train_loss 0.003732,Time used 0.002976s\n",
      "batch 2815, train_loss 0.004344,Time used 0.002477s\n",
      "batch 2816, train_loss 0.003846,Time used 0.003465s\n",
      "batch 2817, train_loss 0.004399,Time used 0.002976s\n",
      "batch 2818, train_loss 0.003671,Time used 0.002965s\n",
      "batch 2819, train_loss 0.003772,Time used 0.002976s\n",
      "batch 2820, train_loss 0.003641,Time used 0.002976s\n",
      "batch 2821, train_loss 0.004217,Time used 0.002480s\n",
      "batch 2822, train_loss 0.004040,Time used 0.002481s\n",
      "batch 2823, train_loss 0.003207,Time used 0.002976s\n",
      "batch 2824, train_loss 0.004317,Time used 0.002480s\n",
      "batch 2825, train_loss 0.004116,Time used 0.002976s\n",
      "batch 2826, train_loss 0.003884,Time used 0.002968s\n",
      "batch 2827, train_loss 0.004275,Time used 0.002976s\n",
      "batch 2828, train_loss 0.003615,Time used 0.003472s\n",
      "batch 2829, train_loss 0.004781,Time used 0.002976s\n",
      "batch 2830, train_loss 0.004839,Time used 0.002976s\n",
      "batch 2831, train_loss 0.004808,Time used 0.002976s\n",
      "batch 2832, train_loss 0.003739,Time used 0.003472s\n",
      "batch 2833, train_loss 0.003804,Time used 0.002976s\n",
      "batch 2834, train_loss 0.003459,Time used 0.003478s\n",
      "batch 2835, train_loss 0.003805,Time used 0.003467s\n",
      "batch 2836, train_loss 0.004246,Time used 0.002976s\n",
      "batch 2837, train_loss 0.003064,Time used 0.002976s\n",
      "batch 2838, train_loss 0.005039,Time used 0.003472s\n",
      "batch 2839, train_loss 0.003670,Time used 0.002975s\n",
      "batch 2840, train_loss 0.003720,Time used 0.003478s\n",
      "batch 2841, train_loss 0.004841,Time used 0.003472s\n",
      "batch 2842, train_loss 0.003698,Time used 0.002977s\n",
      "batch 2843, train_loss 0.003388,Time used 0.002976s\n",
      "batch 2844, train_loss 0.003890,Time used 0.003970s\n",
      "batch 2845, train_loss 0.003328,Time used 0.002480s\n",
      "batch 2846, train_loss 0.003533,Time used 0.013392s\n",
      "batch 2847, train_loss 0.003363,Time used 0.002976s\n",
      "batch 2848, train_loss 0.003906,Time used 0.002976s\n",
      "batch 2849, train_loss 0.004411,Time used 0.003472s\n",
      "batch 2850, train_loss 0.004037,Time used 0.002975s\n",
      "batch 2851, train_loss 0.005150,Time used 0.002976s\n",
      "batch 2852, train_loss 0.003836,Time used 0.002481s\n",
      "batch 2853, train_loss 0.004225,Time used 0.002976s\n",
      "batch 2854, train_loss 0.003910,Time used 0.003472s\n",
      "batch 2855, train_loss 0.003223,Time used 0.002977s\n",
      "batch 2856, train_loss 0.004133,Time used 0.002480s\n",
      "batch 2857, train_loss 0.004087,Time used 0.002480s\n",
      "batch 2858, train_loss 0.003809,Time used 0.002480s\n",
      "batch 2859, train_loss 0.003267,Time used 0.002479s\n",
      "batch 2860, train_loss 0.004687,Time used 0.002972s\n",
      "batch 2861, train_loss 0.004416,Time used 0.003472s\n",
      "batch 2862, train_loss 0.000339,Time used 0.002976s\n",
      "batch 2863, train_loss 0.002945,Time used 0.002976s\n",
      "batch 2864, train_loss 0.004978,Time used 0.002480s\n",
      "batch 2865, train_loss 0.004418,Time used 0.002972s\n",
      "batch 2866, train_loss 0.003074,Time used 0.003473s\n",
      "batch 2867, train_loss 0.003637,Time used 0.003479s\n",
      "batch 2868, train_loss 0.003957,Time used 0.002976s\n",
      "batch 2869, train_loss 0.005017,Time used 0.002976s\n",
      "batch 2870, train_loss 0.004381,Time used 0.002479s\n",
      "batch 2871, train_loss 0.004577,Time used 0.003473s\n",
      "batch 2872, train_loss 0.004734,Time used 0.002976s\n",
      "batch 2873, train_loss 0.005658,Time used 0.002975s\n",
      "batch 2874, train_loss 0.004379,Time used 0.002480s\n",
      "batch 2875, train_loss 0.004266,Time used 0.002976s\n",
      "batch 2876, train_loss 0.003363,Time used 0.002483s\n",
      "batch 2877, train_loss 0.003615,Time used 0.002975s\n",
      "batch 2878, train_loss 0.003454,Time used 0.003472s\n",
      "batch 2879, train_loss 0.004098,Time used 0.003472s\n",
      "batch 2880, train_loss 0.003586,Time used 0.002976s\n",
      "batch 2881, train_loss 0.003225,Time used 0.002480s\n",
      "batch 2882, train_loss 0.003371,Time used 0.002976s\n",
      "batch 2883, train_loss 0.004098,Time used 0.002976s\n",
      "batch 2884, train_loss 0.003575,Time used 0.003471s\n",
      "batch 2885, train_loss 0.004095,Time used 0.002976s\n",
      "batch 2886, train_loss 0.004202,Time used 0.002480s\n",
      "batch 2887, train_loss 0.003574,Time used 0.002975s\n",
      "batch 2888, train_loss 0.004061,Time used 0.002976s\n",
      "batch 2889, train_loss 0.003815,Time used 0.002480s\n",
      "batch 2890, train_loss 0.002946,Time used 0.002977s\n",
      "batch 2891, train_loss 0.003855,Time used 0.002480s\n",
      "batch 2892, train_loss 0.004335,Time used 0.002481s\n",
      "batch 2893, train_loss 0.004163,Time used 0.002976s\n",
      "batch 2894, train_loss 0.003973,Time used 0.002480s\n",
      "batch 2895, train_loss 0.003675,Time used 0.003472s\n",
      "batch 2896, train_loss 0.003667,Time used 0.002975s\n",
      "batch 2897, train_loss 0.003201,Time used 0.002976s\n",
      "batch 2898, train_loss 0.004255,Time used 0.002976s\n",
      "batch 2899, train_loss 0.005305,Time used 0.002976s\n",
      "batch 2900, train_loss 0.004498,Time used 0.002483s\n",
      "***************************test_batch 2900, test_rmse_loss 0.063323,test_mae_loss 0.046221,test_mape_loss 15.204928,Time used 0.010912s\n",
      "batch 2901, train_loss 0.003958,Time used 0.003472s\n",
      "batch 2902, train_loss 0.004443,Time used 0.002981s\n",
      "batch 2903, train_loss 0.004437,Time used 0.003471s\n",
      "batch 2904, train_loss 0.004125,Time used 0.003969s\n",
      "batch 2905, train_loss 0.003163,Time used 0.002975s\n",
      "batch 2906, train_loss 0.005367,Time used 0.003474s\n",
      "batch 2907, train_loss 0.003226,Time used 0.003472s\n",
      "batch 2908, train_loss 0.003858,Time used 0.002976s\n",
      "batch 2909, train_loss 0.003256,Time used 0.002976s\n",
      "batch 2910, train_loss 0.004224,Time used 0.003476s\n",
      "batch 2911, train_loss 0.004652,Time used 0.003473s\n",
      "batch 2912, train_loss 0.003672,Time used 0.002975s\n",
      "batch 2913, train_loss 0.003566,Time used 0.003476s\n",
      "batch 2914, train_loss 0.003121,Time used 0.002969s\n",
      "batch 2915, train_loss 0.003672,Time used 0.002975s\n",
      "batch 2916, train_loss 0.004668,Time used 0.002976s\n",
      "batch 2917, train_loss 0.004411,Time used 0.002975s\n",
      "batch 2918, train_loss 0.004241,Time used 0.002975s\n",
      "batch 2919, train_loss 0.003712,Time used 0.002975s\n",
      "batch 2920, train_loss 0.003770,Time used 0.002970s\n",
      "batch 2921, train_loss 0.003493,Time used 0.002480s\n",
      "batch 2922, train_loss 0.004999,Time used 0.002976s\n",
      "batch 2923, train_loss 0.003300,Time used 0.002976s\n",
      "batch 2924, train_loss 0.003687,Time used 0.002976s\n",
      "batch 2925, train_loss 0.004655,Time used 0.002480s\n",
      "batch 2926, train_loss 0.003908,Time used 0.002976s\n",
      "batch 2927, train_loss 0.004408,Time used 0.002976s\n",
      "batch 2928, train_loss 0.003383,Time used 0.003472s\n",
      "batch 2929, train_loss 0.004108,Time used 0.002976s\n",
      "batch 2930, train_loss 0.003202,Time used 0.002480s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2931, train_loss 0.003762,Time used 0.002976s\n",
      "batch 2932, train_loss 0.003663,Time used 0.003472s\n",
      "batch 2933, train_loss 0.004062,Time used 0.003472s\n",
      "batch 2934, train_loss 0.004078,Time used 0.003473s\n",
      "batch 2935, train_loss 0.003958,Time used 0.004464s\n",
      "batch 2936, train_loss 0.003446,Time used 0.002976s\n",
      "batch 2937, train_loss 0.003711,Time used 0.002976s\n",
      "batch 2938, train_loss 0.003614,Time used 0.012391s\n",
      "batch 2939, train_loss 0.003639,Time used 0.002480s\n",
      "batch 2940, train_loss 0.003294,Time used 0.002976s\n",
      "batch 2941, train_loss 0.003253,Time used 0.002976s\n",
      "batch 2942, train_loss 0.004356,Time used 0.002975s\n",
      "batch 2943, train_loss 0.003746,Time used 0.002975s\n",
      "batch 2944, train_loss 0.004696,Time used 0.002976s\n",
      "batch 2945, train_loss 0.004332,Time used 0.002976s\n",
      "batch 2946, train_loss 0.004568,Time used 0.002976s\n",
      "batch 2947, train_loss 0.004744,Time used 0.002980s\n",
      "batch 2948, train_loss 0.003944,Time used 0.002976s\n",
      "batch 2949, train_loss 0.004444,Time used 0.002976s\n",
      "batch 2950, train_loss 0.003434,Time used 0.002976s\n",
      "batch 2951, train_loss 0.004283,Time used 0.003473s\n",
      "batch 2952, train_loss 0.004239,Time used 0.002480s\n",
      "batch 2953, train_loss 0.004923,Time used 0.002479s\n",
      "batch 2954, train_loss 0.004211,Time used 0.002976s\n",
      "batch 2955, train_loss 0.002730,Time used 0.002972s\n",
      "batch 2956, train_loss 0.003576,Time used 0.002976s\n",
      "batch 2957, train_loss 0.004311,Time used 0.003473s\n",
      "batch 2958, train_loss 0.003501,Time used 0.003472s\n",
      "batch 2959, train_loss 0.003372,Time used 0.003472s\n",
      "batch 2960, train_loss 0.003237,Time used 0.012268s\n",
      "batch 2961, train_loss 0.004088,Time used 0.002976s\n",
      "batch 2962, train_loss 0.004454,Time used 0.002481s\n",
      "batch 2963, train_loss 0.004769,Time used 0.002976s\n",
      "batch 2964, train_loss 0.004260,Time used 0.002976s\n",
      "batch 2965, train_loss 0.004648,Time used 0.002980s\n",
      "batch 2966, train_loss 0.003627,Time used 0.002479s\n",
      "batch 2967, train_loss 0.003892,Time used 0.002976s\n",
      "batch 2968, train_loss 0.004428,Time used 0.003472s\n",
      "batch 2969, train_loss 0.003437,Time used 0.003464s\n",
      "batch 2970, train_loss 0.003543,Time used 0.002976s\n",
      "batch 2971, train_loss 0.004087,Time used 0.002976s\n",
      "batch 2972, train_loss 0.002991,Time used 0.002977s\n",
      "batch 2973, train_loss 0.004423,Time used 0.003472s\n",
      "batch 2974, train_loss 0.004181,Time used 0.002977s\n",
      "batch 2975, train_loss 0.005311,Time used 0.002976s\n",
      "batch 2976, train_loss 0.003830,Time used 0.002977s\n",
      "batch 2977, train_loss 0.003820,Time used 0.002476s\n",
      "batch 2978, train_loss 0.003925,Time used 0.002985s\n",
      "batch 2979, train_loss 0.003290,Time used 0.002480s\n",
      "batch 2980, train_loss 0.003386,Time used 0.002975s\n",
      "batch 2981, train_loss 0.004286,Time used 0.002976s\n",
      "batch 2982, train_loss 0.003704,Time used 0.002975s\n",
      "batch 2983, train_loss 0.003559,Time used 0.002976s\n",
      "batch 2984, train_loss 0.005433,Time used 0.002976s\n",
      "batch 2985, train_loss 0.003178,Time used 0.002975s\n",
      "batch 2986, train_loss 0.003204,Time used 0.002975s\n",
      "batch 2987, train_loss 0.004353,Time used 0.002975s\n",
      "batch 2988, train_loss 0.005751,Time used 0.002484s\n",
      "batch 2989, train_loss 0.003394,Time used 0.002480s\n",
      "batch 2990, train_loss 0.003863,Time used 0.002976s\n",
      "batch 2991, train_loss 0.004095,Time used 0.002979s\n",
      "batch 2992, train_loss 0.004467,Time used 0.002976s\n",
      "batch 2993, train_loss 0.004478,Time used 0.002976s\n",
      "batch 2994, train_loss 0.003787,Time used 0.003472s\n",
      "batch 2995, train_loss 0.003393,Time used 0.002976s\n",
      "batch 2996, train_loss 0.003748,Time used 0.002480s\n",
      "batch 2997, train_loss 0.003764,Time used 0.002976s\n",
      "batch 2998, train_loss 0.004311,Time used 0.002976s\n",
      "batch 2999, train_loss 0.004015,Time used 0.002976s\n",
      "batch 3000, train_loss 0.004950,Time used 0.002972s\n",
      "***************************test_batch 3000, test_rmse_loss 0.062946,test_mae_loss 0.045289,test_mape_loss 13.724610,Time used 0.009920s\n",
      "batch 3001, train_loss 0.003625,Time used 0.002976s\n",
      "batch 3002, train_loss 0.003643,Time used 0.003481s\n",
      "batch 3003, train_loss 0.004006,Time used 0.002980s\n",
      "batch 3004, train_loss 0.004182,Time used 0.002976s\n",
      "batch 3005, train_loss 0.003050,Time used 0.002471s\n",
      "batch 3006, train_loss 0.004977,Time used 0.003968s\n",
      "batch 3007, train_loss 0.003793,Time used 0.002976s\n",
      "batch 3008, train_loss 0.003213,Time used 0.002474s\n",
      "batch 3009, train_loss 0.004522,Time used 0.002976s\n",
      "batch 3010, train_loss 0.003642,Time used 0.002975s\n",
      "batch 3011, train_loss 0.003502,Time used 0.001984s\n",
      "batch 3012, train_loss 0.004126,Time used 0.002480s\n",
      "batch 3013, train_loss 0.004028,Time used 0.003472s\n",
      "batch 3014, train_loss 0.003919,Time used 0.002975s\n",
      "batch 3015, train_loss 0.004716,Time used 0.002481s\n",
      "batch 3016, train_loss 0.002710,Time used 0.002480s\n",
      "batch 3017, train_loss 0.004277,Time used 0.002977s\n",
      "batch 3018, train_loss 0.004826,Time used 0.002976s\n",
      "batch 3019, train_loss 0.003522,Time used 0.003472s\n",
      "batch 3020, train_loss 0.003478,Time used 0.002976s\n",
      "batch 3021, train_loss 0.003679,Time used 0.002971s\n",
      "batch 3022, train_loss 0.003190,Time used 0.002976s\n",
      "batch 3023, train_loss 0.003287,Time used 0.003472s\n",
      "batch 3024, train_loss 0.005115,Time used 0.002976s\n",
      "batch 3025, train_loss 0.003864,Time used 0.003968s\n",
      "batch 3026, train_loss 0.004050,Time used 0.002981s\n",
      "batch 3027, train_loss 0.004232,Time used 0.012400s\n",
      "batch 3028, train_loss 0.003531,Time used 0.002976s\n",
      "batch 3029, train_loss 0.003348,Time used 0.002976s\n",
      "batch 3030, train_loss 0.003391,Time used 0.002972s\n",
      "batch 3031, train_loss 0.003587,Time used 0.002976s\n",
      "batch 3032, train_loss 0.004199,Time used 0.002479s\n",
      "batch 3033, train_loss 0.004090,Time used 0.002977s\n",
      "batch 3034, train_loss 0.005926,Time used 0.002975s\n",
      "batch 3035, train_loss 0.002990,Time used 0.002480s\n",
      "batch 3036, train_loss 0.004452,Time used 0.003472s\n",
      "batch 3037, train_loss 0.003194,Time used 0.003471s\n",
      "batch 3038, train_loss 0.004230,Time used 0.002976s\n",
      "batch 3039, train_loss 0.003855,Time used 0.002967s\n",
      "batch 3040, train_loss 0.003901,Time used 0.002480s\n",
      "batch 3041, train_loss 0.003464,Time used 0.002975s\n",
      "batch 3042, train_loss 0.003041,Time used 0.002976s\n",
      "batch 3043, train_loss 0.004237,Time used 0.002976s\n",
      "batch 3044, train_loss 0.003734,Time used 0.002976s\n",
      "batch 3045, train_loss 0.004199,Time used 0.003472s\n",
      "batch 3046, train_loss 0.003113,Time used 0.002976s\n",
      "batch 3047, train_loss 0.004447,Time used 0.002976s\n",
      "batch 3048, train_loss 0.003930,Time used 0.003470s\n",
      "batch 3049, train_loss 0.003891,Time used 0.002976s\n",
      "batch 3050, train_loss 0.002926,Time used 0.003472s\n",
      "batch 3051, train_loss 0.004000,Time used 0.003472s\n",
      "batch 3052, train_loss 0.003937,Time used 0.002986s\n",
      "batch 3053, train_loss 0.003857,Time used 0.002976s\n",
      "batch 3054, train_loss 0.004721,Time used 0.003471s\n",
      "batch 3055, train_loss 0.003816,Time used 0.002976s\n",
      "batch 3056, train_loss 0.004676,Time used 0.003472s\n",
      "batch 3057, train_loss 0.002833,Time used 0.003471s\n",
      "batch 3058, train_loss 0.004232,Time used 0.002976s\n",
      "batch 3059, train_loss 0.003730,Time used 0.003472s\n",
      "batch 3060, train_loss 0.004075,Time used 0.002976s\n",
      "batch 3061, train_loss 0.004776,Time used 0.003476s\n",
      "batch 3062, train_loss 0.004137,Time used 0.003464s\n",
      "batch 3063, train_loss 0.004193,Time used 0.003471s\n",
      "batch 3064, train_loss 0.004758,Time used 0.003472s\n",
      "batch 3065, train_loss 0.004155,Time used 0.002976s\n",
      "batch 3066, train_loss 0.004297,Time used 0.002971s\n",
      "batch 3067, train_loss 0.004229,Time used 0.002973s\n",
      "batch 3068, train_loss 0.003661,Time used 0.002980s\n",
      "batch 3069, train_loss 0.003724,Time used 0.003473s\n",
      "batch 3070, train_loss 0.003469,Time used 0.003472s\n",
      "batch 3071, train_loss 0.003846,Time used 0.003472s\n",
      "batch 3072, train_loss 0.004519,Time used 0.003472s\n",
      "batch 3073, train_loss 0.003850,Time used 0.002976s\n",
      "batch 3074, train_loss 0.003730,Time used 0.002975s\n",
      "batch 3075, train_loss 0.003626,Time used 0.002976s\n",
      "batch 3076, train_loss 0.003741,Time used 0.002975s\n",
      "batch 3077, train_loss 0.004090,Time used 0.003473s\n",
      "batch 3078, train_loss 0.002606,Time used 0.002976s\n",
      "batch 3079, train_loss 0.004109,Time used 0.002976s\n",
      "batch 3080, train_loss 0.003937,Time used 0.003472s\n",
      "batch 3081, train_loss 0.005028,Time used 0.003968s\n",
      "batch 3082, train_loss 0.004304,Time used 0.002975s\n",
      "batch 3083, train_loss 0.003862,Time used 0.002976s\n",
      "batch 3084, train_loss 0.003607,Time used 0.002977s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3085, train_loss 0.003502,Time used 0.002976s\n",
      "batch 3086, train_loss 0.003950,Time used 0.002975s\n",
      "batch 3087, train_loss 0.004610,Time used 0.003476s\n",
      "batch 3088, train_loss 0.003439,Time used 0.003472s\n",
      "batch 3089, train_loss 0.004263,Time used 0.003472s\n",
      "batch 3090, train_loss 0.004309,Time used 0.002976s\n",
      "batch 3091, train_loss 0.004049,Time used 0.003472s\n",
      "batch 3092, train_loss 0.003474,Time used 0.002968s\n",
      "batch 3093, train_loss 0.004332,Time used 0.002976s\n",
      "batch 3094, train_loss 0.005038,Time used 0.003472s\n",
      "batch 3095, train_loss 0.003942,Time used 0.002480s\n",
      "batch 3096, train_loss 0.003950,Time used 0.003472s\n",
      "batch 3097, train_loss 0.004488,Time used 0.003968s\n",
      "batch 3098, train_loss 0.004036,Time used 0.003472s\n",
      "batch 3099, train_loss 0.003729,Time used 0.003471s\n",
      "batch 3100, train_loss 0.005085,Time used 0.002976s\n",
      "***************************test_batch 3100, test_rmse_loss 0.062974,test_mae_loss 0.045260,test_mape_loss 13.649572,Time used 0.010416s\n",
      "batch 3101, train_loss 0.003657,Time used 0.003472s\n",
      "batch 3102, train_loss 0.003075,Time used 0.002980s\n",
      "batch 3103, train_loss 0.003969,Time used 0.002976s\n",
      "batch 3104, train_loss 0.004128,Time used 0.003968s\n",
      "batch 3105, train_loss 0.003031,Time used 0.003470s\n",
      "batch 3106, train_loss 0.003042,Time used 0.003472s\n",
      "batch 3107, train_loss 0.003901,Time used 0.003473s\n",
      "batch 3108, train_loss 0.003890,Time used 0.003472s\n",
      "batch 3109, train_loss 0.002840,Time used 0.012400s\n",
      "batch 3110, train_loss 0.003079,Time used 0.003472s\n",
      "batch 3111, train_loss 0.004287,Time used 0.002976s\n",
      "batch 3112, train_loss 0.003934,Time used 0.003472s\n",
      "batch 3113, train_loss 0.003922,Time used 0.002976s\n",
      "batch 3114, train_loss 0.003465,Time used 0.002480s\n",
      "batch 3115, train_loss 0.003745,Time used 0.002979s\n",
      "batch 3116, train_loss 0.003718,Time used 0.002976s\n",
      "batch 3117, train_loss 0.005500,Time used 0.003472s\n",
      "batch 3118, train_loss 0.004060,Time used 0.003467s\n",
      "batch 3119, train_loss 0.004119,Time used 0.002976s\n",
      "batch 3120, train_loss 0.004131,Time used 0.003472s\n",
      "batch 3121, train_loss 0.004075,Time used 0.002976s\n",
      "batch 3122, train_loss 0.003758,Time used 0.003465s\n",
      "batch 3123, train_loss 0.003804,Time used 0.002981s\n",
      "batch 3124, train_loss 0.005506,Time used 0.002976s\n",
      "batch 3125, train_loss 0.003675,Time used 0.003472s\n",
      "batch 3126, train_loss 0.003846,Time used 0.003473s\n",
      "batch 3127, train_loss 0.004191,Time used 0.002976s\n",
      "batch 3128, train_loss 0.002722,Time used 0.002977s\n",
      "batch 3129, train_loss 0.003472,Time used 0.002981s\n",
      "batch 3130, train_loss 0.003521,Time used 0.002480s\n",
      "batch 3131, train_loss 0.003931,Time used 0.002976s\n",
      "batch 3132, train_loss 0.002437,Time used 0.002976s\n",
      "batch 3133, train_loss 0.003013,Time used 0.002976s\n",
      "batch 3134, train_loss 0.003944,Time used 0.003968s\n",
      "batch 3135, train_loss 0.004078,Time used 0.002976s\n",
      "batch 3136, train_loss 0.003452,Time used 0.002976s\n",
      "batch 3137, train_loss 0.003613,Time used 0.002975s\n",
      "batch 3138, train_loss 0.004675,Time used 0.002481s\n",
      "batch 3139, train_loss 0.003993,Time used 0.002480s\n",
      "batch 3140, train_loss 0.003469,Time used 0.003472s\n",
      "batch 3141, train_loss 0.003995,Time used 0.002977s\n",
      "batch 3142, train_loss 0.003920,Time used 0.002976s\n",
      "batch 3143, train_loss 0.003101,Time used 0.003472s\n",
      "batch 3144, train_loss 0.003635,Time used 0.003472s\n",
      "batch 3145, train_loss 0.005034,Time used 0.003472s\n",
      "batch 3146, train_loss 0.004718,Time used 0.002977s\n",
      "batch 3147, train_loss 0.003004,Time used 0.003472s\n",
      "batch 3148, train_loss 0.004554,Time used 0.003472s\n",
      "batch 3149, train_loss 0.003327,Time used 0.002976s\n",
      "batch 3150, train_loss 0.004036,Time used 0.002976s\n",
      "batch 3151, train_loss 0.004811,Time used 0.002977s\n",
      "batch 3152, train_loss 0.004120,Time used 0.002976s\n",
      "batch 3153, train_loss 0.003940,Time used 0.003473s\n",
      "batch 3154, train_loss 0.003643,Time used 0.003462s\n",
      "batch 3155, train_loss 0.004132,Time used 0.002976s\n",
      "batch 3156, train_loss 0.004364,Time used 0.002986s\n",
      "batch 3157, train_loss 0.005289,Time used 0.002976s\n",
      "batch 3158, train_loss 0.004627,Time used 0.002970s\n",
      "batch 3159, train_loss 0.003675,Time used 0.002976s\n",
      "batch 3160, train_loss 0.002902,Time used 0.003473s\n",
      "batch 3161, train_loss 0.004483,Time used 0.002975s\n",
      "batch 3162, train_loss 0.003585,Time used 0.002480s\n",
      "batch 3163, train_loss 0.003354,Time used 0.002977s\n",
      "batch 3164, train_loss 0.004543,Time used 0.002977s\n",
      "batch 3165, train_loss 0.003438,Time used 0.002976s\n",
      "batch 3166, train_loss 0.004465,Time used 0.002976s\n",
      "batch 3167, train_loss 0.004072,Time used 0.003472s\n",
      "batch 3168, train_loss 0.003204,Time used 0.002484s\n",
      "batch 3169, train_loss 0.004033,Time used 0.002976s\n",
      "batch 3170, train_loss 0.003431,Time used 0.002479s\n",
      "batch 3171, train_loss 0.003614,Time used 0.002977s\n",
      "batch 3172, train_loss 0.003114,Time used 0.003472s\n",
      "batch 3173, train_loss 0.003334,Time used 0.003473s\n",
      "batch 3174, train_loss 0.004044,Time used 0.002975s\n",
      "batch 3175, train_loss 0.003742,Time used 0.002976s\n",
      "batch 3176, train_loss 0.004495,Time used 0.012400s\n",
      "batch 3177, train_loss 0.003951,Time used 0.002975s\n",
      "batch 3178, train_loss 0.003799,Time used 0.002976s\n",
      "batch 3179, train_loss 0.004580,Time used 0.002975s\n",
      "batch 3180, train_loss 0.003786,Time used 0.002480s\n",
      "batch 3181, train_loss 0.004336,Time used 0.002967s\n",
      "batch 3182, train_loss 0.003366,Time used 0.002975s\n",
      "batch 3183, train_loss 0.004862,Time used 0.002481s\n",
      "batch 3184, train_loss 0.003204,Time used 0.002976s\n",
      "batch 3185, train_loss 0.004179,Time used 0.003472s\n",
      "batch 3186, train_loss 0.002371,Time used 0.003473s\n",
      "batch 3187, train_loss 0.004221,Time used 0.002481s\n",
      "batch 3188, train_loss 0.003192,Time used 0.002977s\n",
      "batch 3189, train_loss 0.004864,Time used 0.002976s\n",
      "batch 3190, train_loss 0.003753,Time used 0.002968s\n",
      "batch 3191, train_loss 0.003490,Time used 0.002969s\n",
      "batch 3192, train_loss 0.003497,Time used 0.003472s\n",
      "batch 3193, train_loss 0.003722,Time used 0.003472s\n",
      "batch 3194, train_loss 0.004146,Time used 0.002976s\n",
      "batch 3195, train_loss 0.003838,Time used 0.003472s\n",
      "batch 3196, train_loss 0.003982,Time used 0.002976s\n",
      "batch 3197, train_loss 0.003357,Time used 0.012887s\n",
      "batch 3198, train_loss 0.004460,Time used 0.002976s\n",
      "batch 3199, train_loss 0.003738,Time used 0.003471s\n",
      "batch 3200, train_loss 0.003964,Time used 0.003472s\n",
      "***************************test_batch 3200, test_rmse_loss 0.062703,test_mae_loss 0.045224,test_mape_loss 14.022220,Time used 0.010416s\n",
      "batch 3201, train_loss 0.004065,Time used 0.002975s\n",
      "batch 3202, train_loss 0.002867,Time used 0.003472s\n",
      "batch 3203, train_loss 0.003562,Time used 0.002480s\n",
      "batch 3204, train_loss 0.004073,Time used 0.003472s\n",
      "batch 3205, train_loss 0.004162,Time used 0.002985s\n",
      "batch 3206, train_loss 0.003558,Time used 0.002481s\n",
      "batch 3207, train_loss 0.004627,Time used 0.002974s\n",
      "batch 3208, train_loss 0.004256,Time used 0.002976s\n",
      "batch 3209, train_loss 0.004920,Time used 0.002975s\n",
      "batch 3210, train_loss 0.003665,Time used 0.002480s\n",
      "batch 3211, train_loss 0.003450,Time used 0.002480s\n",
      "batch 3212, train_loss 0.003259,Time used 0.002971s\n",
      "batch 3213, train_loss 0.003738,Time used 0.002976s\n",
      "batch 3214, train_loss 0.004192,Time used 0.002973s\n",
      "batch 3215, train_loss 0.003976,Time used 0.002977s\n",
      "batch 3216, train_loss 0.003584,Time used 0.002480s\n",
      "batch 3217, train_loss 0.003855,Time used 0.002975s\n",
      "batch 3218, train_loss 0.003845,Time used 0.002480s\n",
      "batch 3219, train_loss 0.004306,Time used 0.003472s\n",
      "batch 3220, train_loss 0.004651,Time used 0.002480s\n",
      "batch 3221, train_loss 0.003327,Time used 0.002481s\n",
      "batch 3222, train_loss 0.004686,Time used 0.002975s\n",
      "batch 3223, train_loss 0.003246,Time used 0.002476s\n",
      "batch 3224, train_loss 0.003938,Time used 0.002486s\n",
      "batch 3225, train_loss 0.004735,Time used 0.003472s\n",
      "batch 3226, train_loss 0.004004,Time used 0.003472s\n",
      "batch 3227, train_loss 0.003829,Time used 0.002976s\n",
      "batch 3228, train_loss 0.003989,Time used 0.002976s\n",
      "batch 3229, train_loss 0.003879,Time used 0.002973s\n",
      "batch 3230, train_loss 0.003920,Time used 0.002971s\n",
      "batch 3231, train_loss 0.003744,Time used 0.002976s\n",
      "batch 3232, train_loss 0.004484,Time used 0.003472s\n",
      "batch 3233, train_loss 0.003271,Time used 0.002976s\n",
      "batch 3234, train_loss 0.003756,Time used 0.002976s\n",
      "batch 3235, train_loss 0.003658,Time used 0.002480s\n",
      "batch 3236, train_loss 0.003287,Time used 0.002977s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3237, train_loss 0.004042,Time used 0.002976s\n",
      "batch 3238, train_loss 0.004342,Time used 0.002976s\n",
      "batch 3239, train_loss 0.004457,Time used 0.002976s\n",
      "batch 3240, train_loss 0.007227,Time used 0.002976s\n",
      "batch 3241, train_loss 0.003909,Time used 0.002976s\n",
      "batch 3242, train_loss 0.003837,Time used 0.002973s\n",
      "batch 3243, train_loss 0.004706,Time used 0.002976s\n",
      "batch 3244, train_loss 0.004062,Time used 0.002976s\n",
      "batch 3245, train_loss 0.004408,Time used 0.002480s\n",
      "batch 3246, train_loss 0.003038,Time used 0.002976s\n",
      "batch 3247, train_loss 0.003471,Time used 0.002975s\n",
      "batch 3248, train_loss 0.003395,Time used 0.002976s\n",
      "batch 3249, train_loss 0.003833,Time used 0.002976s\n",
      "batch 3250, train_loss 0.003992,Time used 0.002975s\n",
      "batch 3251, train_loss 0.004469,Time used 0.002976s\n",
      "batch 3252, train_loss 0.003404,Time used 0.002976s\n",
      "batch 3253, train_loss 0.004783,Time used 0.002480s\n",
      "batch 3254, train_loss 0.003284,Time used 0.002976s\n",
      "batch 3255, train_loss 0.004027,Time used 0.002975s\n",
      "batch 3256, train_loss 0.003791,Time used 0.002976s\n",
      "batch 3257, train_loss 0.004194,Time used 0.002967s\n",
      "batch 3258, train_loss 0.004012,Time used 0.003472s\n",
      "batch 3259, train_loss 0.003512,Time used 0.002980s\n",
      "batch 3260, train_loss 0.003901,Time used 0.002976s\n",
      "batch 3261, train_loss 0.003732,Time used 0.002480s\n",
      "batch 3262, train_loss 0.005389,Time used 0.003473s\n",
      "batch 3263, train_loss 0.003466,Time used 0.002975s\n",
      "batch 3264, train_loss 0.004403,Time used 0.003472s\n",
      "batch 3265, train_loss 0.003512,Time used 0.002971s\n",
      "batch 3266, train_loss 0.003950,Time used 0.002976s\n",
      "batch 3267, train_loss 0.003590,Time used 0.002976s\n",
      "batch 3268, train_loss 0.004345,Time used 0.002976s\n",
      "batch 3269, train_loss 0.003543,Time used 0.003476s\n",
      "batch 3270, train_loss 0.003985,Time used 0.002976s\n",
      "batch 3271, train_loss 0.003232,Time used 0.002976s\n",
      "batch 3272, train_loss 0.003499,Time used 0.002976s\n",
      "batch 3273, train_loss 0.004085,Time used 0.002480s\n",
      "batch 3274, train_loss 0.004074,Time used 0.003472s\n",
      "batch 3275, train_loss 0.003863,Time used 0.002480s\n",
      "batch 3276, train_loss 0.003989,Time used 0.002975s\n",
      "batch 3277, train_loss 0.004869,Time used 0.002976s\n",
      "batch 3278, train_loss 0.003203,Time used 0.003472s\n",
      "batch 3279, train_loss 0.003861,Time used 0.003973s\n",
      "batch 3280, train_loss 0.004057,Time used 0.002976s\n",
      "batch 3281, train_loss 0.003283,Time used 0.002480s\n",
      "batch 3282, train_loss 0.003865,Time used 0.002969s\n",
      "batch 3283, train_loss 0.004449,Time used 0.002480s\n",
      "batch 3284, train_loss 0.004849,Time used 0.003467s\n",
      "batch 3285, train_loss 0.004103,Time used 0.003464s\n",
      "batch 3286, train_loss 0.003772,Time used 0.002985s\n",
      "batch 3287, train_loss 0.003987,Time used 0.003472s\n",
      "batch 3288, train_loss 0.002764,Time used 0.003969s\n",
      "batch 3289, train_loss 0.003422,Time used 0.002481s\n",
      "batch 3290, train_loss 0.003950,Time used 0.012400s\n",
      "batch 3291, train_loss 0.004211,Time used 0.003472s\n",
      "batch 3292, train_loss 0.003739,Time used 0.002480s\n",
      "batch 3293, train_loss 0.004143,Time used 0.002480s\n",
      "batch 3294, train_loss 0.002282,Time used 0.002976s\n",
      "batch 3295, train_loss 0.004478,Time used 0.002976s\n",
      "batch 3296, train_loss 0.002934,Time used 0.002976s\n",
      "batch 3297, train_loss 0.003466,Time used 0.002976s\n",
      "batch 3298, train_loss 0.004309,Time used 0.002972s\n",
      "batch 3299, train_loss 0.003219,Time used 0.002982s\n",
      "batch 3300, train_loss 0.003494,Time used 0.003472s\n",
      "***************************test_batch 3300, test_rmse_loss 0.062631,test_mae_loss 0.045167,test_mape_loss 13.994501,Time used 0.009920s\n",
      "batch 3301, train_loss 0.003461,Time used 0.003473s\n",
      "batch 3302, train_loss 0.004690,Time used 0.002974s\n",
      "batch 3303, train_loss 0.004425,Time used 0.002976s\n",
      "batch 3304, train_loss 0.004513,Time used 0.002976s\n",
      "batch 3305, train_loss 0.004178,Time used 0.002480s\n",
      "batch 3306, train_loss 0.003647,Time used 0.002976s\n",
      "batch 3307, train_loss 0.004394,Time used 0.002976s\n",
      "batch 3308, train_loss 0.004025,Time used 0.002975s\n",
      "batch 3309, train_loss 0.002974,Time used 0.002976s\n",
      "batch 3310, train_loss 0.003735,Time used 0.002976s\n",
      "batch 3311, train_loss 0.003748,Time used 0.002976s\n",
      "batch 3312, train_loss 0.003921,Time used 0.002480s\n",
      "batch 3313, train_loss 0.003554,Time used 0.002976s\n",
      "batch 3314, train_loss 0.004297,Time used 0.002976s\n",
      "batch 3315, train_loss 0.003789,Time used 0.002480s\n",
      "batch 3316, train_loss 0.004698,Time used 0.003968s\n",
      "batch 3317, train_loss 0.004257,Time used 0.002975s\n",
      "batch 3318, train_loss 0.005485,Time used 0.002481s\n",
      "batch 3319, train_loss 0.003432,Time used 0.002480s\n",
      "batch 3320, train_loss 0.005411,Time used 0.002977s\n",
      "batch 3321, train_loss 0.003871,Time used 0.002971s\n",
      "batch 3322, train_loss 0.003247,Time used 0.002480s\n",
      "batch 3323, train_loss 0.003083,Time used 0.002479s\n",
      "batch 3324, train_loss 0.003951,Time used 0.002972s\n",
      "batch 3325, train_loss 0.004494,Time used 0.002476s\n",
      "batch 3326, train_loss 0.003904,Time used 0.002976s\n",
      "batch 3327, train_loss 0.002446,Time used 0.002987s\n",
      "batch 3328, train_loss 0.004378,Time used 0.002976s\n",
      "batch 3329, train_loss 0.003630,Time used 0.002480s\n",
      "batch 3330, train_loss 0.004534,Time used 0.001984s\n",
      "batch 3331, train_loss 0.003736,Time used 0.002975s\n",
      "batch 3332, train_loss 0.004516,Time used 0.002976s\n",
      "batch 3333, train_loss 0.003812,Time used 0.002976s\n",
      "batch 3334, train_loss 0.003606,Time used 0.002974s\n",
      "batch 3335, train_loss 0.003927,Time used 0.002477s\n",
      "batch 3336, train_loss 0.003579,Time used 0.002976s\n",
      "batch 3337, train_loss 0.003971,Time used 0.002481s\n",
      "batch 3338, train_loss 0.003416,Time used 0.002977s\n",
      "batch 3339, train_loss 0.004616,Time used 0.003472s\n",
      "batch 3340, train_loss 0.004435,Time used 0.002481s\n",
      "batch 3341, train_loss 0.003744,Time used 0.002976s\n",
      "batch 3342, train_loss 0.003107,Time used 0.002975s\n",
      "batch 3343, train_loss 0.002980,Time used 0.002473s\n",
      "batch 3344, train_loss 0.004027,Time used 0.002480s\n",
      "batch 3345, train_loss 0.003797,Time used 0.002976s\n",
      "batch 3346, train_loss 0.005404,Time used 0.002971s\n",
      "batch 3347, train_loss 0.003008,Time used 0.002975s\n",
      "batch 3348, train_loss 0.006430,Time used 0.002481s\n",
      "batch 3349, train_loss 0.004030,Time used 0.002976s\n",
      "batch 3350, train_loss 0.004085,Time used 0.003472s\n",
      "batch 3351, train_loss 0.004109,Time used 0.003472s\n",
      "batch 3352, train_loss 0.002940,Time used 0.002480s\n",
      "batch 3353, train_loss 0.004283,Time used 0.002975s\n",
      "batch 3354, train_loss 0.004949,Time used 0.002975s\n",
      "batch 3355, train_loss 0.003587,Time used 0.002976s\n",
      "batch 3356, train_loss 0.003927,Time used 0.002480s\n",
      "batch 3357, train_loss 0.003810,Time used 0.002975s\n",
      "batch 3358, train_loss 0.003544,Time used 0.002977s\n",
      "batch 3359, train_loss 0.004793,Time used 0.002480s\n",
      "batch 3360, train_loss 0.003230,Time used 0.002976s\n",
      "batch 3361, train_loss 0.003474,Time used 0.002480s\n",
      "batch 3362, train_loss 0.003499,Time used 0.003472s\n",
      "batch 3363, train_loss 0.003758,Time used 0.002968s\n",
      "batch 3364, train_loss 0.003753,Time used 0.002976s\n",
      "batch 3365, train_loss 0.004010,Time used 0.002976s\n",
      "batch 3366, train_loss 0.004541,Time used 0.002976s\n",
      "batch 3367, train_loss 0.004327,Time used 0.002976s\n",
      "batch 3368, train_loss 0.003794,Time used 0.003472s\n",
      "batch 3369, train_loss 0.004409,Time used 0.002976s\n",
      "batch 3370, train_loss 0.004424,Time used 0.002976s\n",
      "batch 3371, train_loss 0.003707,Time used 0.002980s\n",
      "batch 3372, train_loss 0.003862,Time used 0.003472s\n",
      "batch 3373, train_loss 0.003066,Time used 0.002976s\n",
      "batch 3374, train_loss 0.003692,Time used 0.002976s\n",
      "batch 3375, train_loss 0.004229,Time used 0.003472s\n",
      "batch 3376, train_loss 0.003610,Time used 0.002975s\n",
      "batch 3377, train_loss 0.003696,Time used 0.013888s\n",
      "batch 3378, train_loss 0.004210,Time used 0.002965s\n",
      "batch 3379, train_loss 0.003578,Time used 0.002976s\n",
      "batch 3380, train_loss 0.003563,Time used 0.002976s\n",
      "batch 3381, train_loss 0.005081,Time used 0.002480s\n",
      "batch 3382, train_loss 0.004326,Time used 0.002976s\n",
      "batch 3383, train_loss 0.003984,Time used 0.003472s\n",
      "batch 3384, train_loss 0.003677,Time used 0.002985s\n",
      "batch 3385, train_loss 0.003448,Time used 0.002976s\n",
      "batch 3386, train_loss 0.004059,Time used 0.003472s\n",
      "batch 3387, train_loss 0.004122,Time used 0.002976s\n",
      "batch 3388, train_loss 0.004407,Time used 0.002479s\n",
      "batch 3389, train_loss 0.003440,Time used 0.002976s\n",
      "batch 3390, train_loss 0.003643,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3391, train_loss 0.003794,Time used 0.002479s\n",
      "batch 3392, train_loss 0.003385,Time used 0.002480s\n",
      "batch 3393, train_loss 0.003454,Time used 0.002976s\n",
      "batch 3394, train_loss 0.003720,Time used 0.002971s\n",
      "batch 3395, train_loss 0.004804,Time used 0.002479s\n",
      "batch 3396, train_loss 0.004143,Time used 0.002976s\n",
      "batch 3397, train_loss 0.003118,Time used 0.002976s\n",
      "batch 3398, train_loss 0.004367,Time used 0.003472s\n",
      "batch 3399, train_loss 0.003696,Time used 0.002976s\n",
      "batch 3400, train_loss 0.004367,Time used 0.002976s\n",
      "***************************test_batch 3400, test_rmse_loss 0.062529,test_mae_loss 0.044984,test_mape_loss 13.703229,Time used 0.022815s\n",
      "batch 3401, train_loss 0.003425,Time used 0.002975s\n",
      "batch 3402, train_loss 0.001228,Time used 0.002981s\n",
      "batch 3403, train_loss 0.003697,Time used 0.002480s\n",
      "batch 3404, train_loss 0.004784,Time used 0.002976s\n",
      "batch 3405, train_loss 0.003913,Time used 0.002976s\n",
      "batch 3406, train_loss 0.003860,Time used 0.003472s\n",
      "batch 3407, train_loss 0.004575,Time used 0.002976s\n",
      "batch 3408, train_loss 0.004464,Time used 0.002976s\n",
      "batch 3409, train_loss 0.003616,Time used 0.002976s\n",
      "batch 3410, train_loss 0.004244,Time used 0.002976s\n",
      "batch 3411, train_loss 0.003207,Time used 0.002976s\n",
      "batch 3412, train_loss 0.004273,Time used 0.003473s\n",
      "batch 3413, train_loss 0.003703,Time used 0.002975s\n",
      "batch 3414, train_loss 0.004541,Time used 0.002480s\n",
      "batch 3415, train_loss 0.003322,Time used 0.002976s\n",
      "batch 3416, train_loss 0.003526,Time used 0.002976s\n",
      "batch 3417, train_loss 0.004218,Time used 0.002971s\n",
      "batch 3418, train_loss 0.003831,Time used 0.002976s\n",
      "batch 3419, train_loss 0.003135,Time used 0.002976s\n",
      "batch 3420, train_loss 0.003690,Time used 0.002969s\n",
      "batch 3421, train_loss 0.003938,Time used 0.002976s\n",
      "batch 3422, train_loss 0.004098,Time used 0.002977s\n",
      "batch 3423, train_loss 0.003885,Time used 0.002972s\n",
      "batch 3424, train_loss 0.004059,Time used 0.002981s\n",
      "batch 3425, train_loss 0.004325,Time used 0.002480s\n",
      "batch 3426, train_loss 0.004790,Time used 0.002976s\n",
      "batch 3427, train_loss 0.003155,Time used 0.002976s\n",
      "batch 3428, train_loss 0.004833,Time used 0.002976s\n",
      "batch 3429, train_loss 0.004091,Time used 0.002975s\n",
      "batch 3430, train_loss 0.003407,Time used 0.002975s\n",
      "batch 3431, train_loss 0.004010,Time used 0.003472s\n",
      "batch 3432, train_loss 0.004278,Time used 0.002976s\n",
      "batch 3433, train_loss 0.003854,Time used 0.003472s\n",
      "batch 3434, train_loss 0.003493,Time used 0.003473s\n",
      "batch 3435, train_loss 0.002932,Time used 0.002976s\n",
      "batch 3436, train_loss 0.004208,Time used 0.003473s\n",
      "batch 3437, train_loss 0.003410,Time used 0.002976s\n",
      "batch 3438, train_loss 0.003705,Time used 0.003467s\n",
      "batch 3439, train_loss 0.004678,Time used 0.003471s\n",
      "batch 3440, train_loss 0.004037,Time used 0.002976s\n",
      "batch 3441, train_loss 0.004376,Time used 0.002976s\n",
      "batch 3442, train_loss 0.004595,Time used 0.003472s\n",
      "batch 3443, train_loss 0.003426,Time used 0.002976s\n",
      "batch 3444, train_loss 0.002720,Time used 0.002480s\n",
      "batch 3445, train_loss 0.002969,Time used 0.003472s\n",
      "batch 3446, train_loss 0.005356,Time used 0.003471s\n",
      "batch 3447, train_loss 0.003430,Time used 0.003472s\n",
      "batch 3448, train_loss 0.003843,Time used 0.002976s\n",
      "batch 3449, train_loss 0.002794,Time used 0.002480s\n",
      "batch 3450, train_loss 0.003306,Time used 0.002976s\n",
      "batch 3451, train_loss 0.003576,Time used 0.002976s\n",
      "batch 3452, train_loss 0.003693,Time used 0.002976s\n",
      "batch 3453, train_loss 0.004507,Time used 0.002975s\n",
      "batch 3454, train_loss 0.003028,Time used 0.002480s\n",
      "batch 3455, train_loss 0.004376,Time used 0.002976s\n",
      "batch 3456, train_loss 0.005542,Time used 0.002480s\n",
      "batch 3457, train_loss 0.003646,Time used 0.002479s\n",
      "batch 3458, train_loss 0.002926,Time used 0.003472s\n",
      "batch 3459, train_loss 0.003288,Time used 0.003968s\n",
      "batch 3460, train_loss 0.003753,Time used 0.002976s\n",
      "batch 3461, train_loss 0.003557,Time used 0.002976s\n",
      "batch 3462, train_loss 0.002725,Time used 0.002977s\n",
      "batch 3463, train_loss 0.003370,Time used 0.011904s\n",
      "batch 3464, train_loss 0.003986,Time used 0.002481s\n",
      "batch 3465, train_loss 0.004383,Time used 0.002985s\n",
      "batch 3466, train_loss 0.004192,Time used 0.002481s\n",
      "batch 3467, train_loss 0.003284,Time used 0.002976s\n",
      "batch 3468, train_loss 0.003798,Time used 0.002976s\n",
      "batch 3469, train_loss 0.004181,Time used 0.002976s\n",
      "batch 3470, train_loss 0.004136,Time used 0.002976s\n",
      "batch 3471, train_loss 0.003783,Time used 0.002481s\n",
      "batch 3472, train_loss 0.003749,Time used 0.002481s\n",
      "batch 3473, train_loss 0.003578,Time used 0.002976s\n",
      "batch 3474, train_loss 0.003730,Time used 0.002975s\n",
      "batch 3475, train_loss 0.003812,Time used 0.002976s\n",
      "batch 3476, train_loss 0.005003,Time used 0.002476s\n",
      "batch 3477, train_loss 0.003420,Time used 0.002976s\n",
      "batch 3478, train_loss 0.003832,Time used 0.002976s\n",
      "batch 3479, train_loss 0.003806,Time used 0.003465s\n",
      "batch 3480, train_loss 0.004675,Time used 0.002481s\n",
      "batch 3481, train_loss 0.003915,Time used 0.002976s\n",
      "batch 3482, train_loss 0.003422,Time used 0.002976s\n",
      "batch 3483, train_loss 0.003697,Time used 0.002972s\n",
      "batch 3484, train_loss 0.004044,Time used 0.002480s\n",
      "batch 3485, train_loss 0.003388,Time used 0.002976s\n",
      "batch 3486, train_loss 0.004342,Time used 0.002480s\n",
      "batch 3487, train_loss 0.003726,Time used 0.002976s\n",
      "batch 3488, train_loss 0.003556,Time used 0.002976s\n",
      "batch 3489, train_loss 0.004802,Time used 0.002975s\n",
      "batch 3490, train_loss 0.004475,Time used 0.002975s\n",
      "batch 3491, train_loss 0.003903,Time used 0.003472s\n",
      "batch 3492, train_loss 0.003908,Time used 0.002976s\n",
      "batch 3493, train_loss 0.003306,Time used 0.003472s\n",
      "batch 3494, train_loss 0.003951,Time used 0.003968s\n",
      "batch 3495, train_loss 0.003482,Time used 0.002976s\n",
      "batch 3496, train_loss 0.004686,Time used 0.004457s\n",
      "batch 3497, train_loss 0.003729,Time used 0.003472s\n",
      "batch 3498, train_loss 0.003179,Time used 0.003473s\n",
      "batch 3499, train_loss 0.004490,Time used 0.002976s\n",
      "batch 3500, train_loss 0.003799,Time used 0.003469s\n",
      "***************************test_batch 3500, test_rmse_loss 0.062612,test_mae_loss 0.044933,test_mape_loss 13.310137,Time used 0.012904s\n",
      "batch 3501, train_loss 0.003671,Time used 0.002974s\n",
      "batch 3502, train_loss 0.003971,Time used 0.003472s\n",
      "batch 3503, train_loss 0.003744,Time used 0.003978s\n",
      "batch 3504, train_loss 0.004460,Time used 0.003476s\n",
      "batch 3505, train_loss 0.004035,Time used 0.003472s\n",
      "batch 3506, train_loss 0.004098,Time used 0.004465s\n",
      "batch 3507, train_loss 0.005250,Time used 0.003472s\n",
      "batch 3508, train_loss 0.004239,Time used 0.003473s\n",
      "batch 3509, train_loss 0.004213,Time used 0.003472s\n",
      "batch 3510, train_loss 0.004423,Time used 0.003472s\n",
      "batch 3511, train_loss 0.004419,Time used 0.002964s\n",
      "batch 3512, train_loss 0.003379,Time used 0.003472s\n",
      "batch 3513, train_loss 0.003956,Time used 0.002967s\n",
      "batch 3514, train_loss 0.004205,Time used 0.003473s\n",
      "batch 3515, train_loss 0.003893,Time used 0.002976s\n",
      "batch 3516, train_loss 0.004162,Time used 0.002976s\n",
      "batch 3517, train_loss 0.003101,Time used 0.002977s\n",
      "batch 3518, train_loss 0.003424,Time used 0.002976s\n",
      "batch 3519, train_loss 0.003234,Time used 0.003472s\n",
      "batch 3520, train_loss 0.004030,Time used 0.003472s\n",
      "batch 3521, train_loss 0.003963,Time used 0.002976s\n",
      "batch 3522, train_loss 0.003912,Time used 0.002977s\n",
      "batch 3523, train_loss 0.004393,Time used 0.002976s\n",
      "batch 3524, train_loss 0.003465,Time used 0.002976s\n",
      "batch 3525, train_loss 0.004151,Time used 0.002975s\n",
      "batch 3526, train_loss 0.002651,Time used 0.002976s\n",
      "batch 3527, train_loss 0.003527,Time used 0.002977s\n",
      "batch 3528, train_loss 0.004577,Time used 0.003469s\n",
      "batch 3529, train_loss 0.003729,Time used 0.002977s\n",
      "batch 3530, train_loss 0.004066,Time used 0.002975s\n",
      "batch 3531, train_loss 0.003775,Time used 0.002976s\n",
      "batch 3532, train_loss 0.004366,Time used 0.002981s\n",
      "batch 3533, train_loss 0.005623,Time used 0.002976s\n",
      "batch 3534, train_loss 0.003695,Time used 0.003472s\n",
      "batch 3535, train_loss 0.003723,Time used 0.002480s\n",
      "batch 3536, train_loss 0.004151,Time used 0.003472s\n",
      "batch 3537, train_loss 0.003774,Time used 0.002976s\n",
      "batch 3538, train_loss 0.003787,Time used 0.003472s\n",
      "batch 3539, train_loss 0.004408,Time used 0.003467s\n",
      "batch 3540, train_loss 0.004088,Time used 0.003472s\n",
      "batch 3541, train_loss 0.003615,Time used 0.003472s\n",
      "batch 3542, train_loss 0.004462,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3543, train_loss 0.004053,Time used 0.002976s\n",
      "batch 3544, train_loss 0.004024,Time used 0.003473s\n",
      "batch 3545, train_loss 0.003437,Time used 0.002976s\n",
      "batch 3546, train_loss 0.003548,Time used 0.002976s\n",
      "batch 3547, train_loss 0.003271,Time used 0.002980s\n",
      "batch 3548, train_loss 0.003462,Time used 0.005456s\n",
      "batch 3549, train_loss 0.004624,Time used 0.003966s\n",
      "batch 3550, train_loss 0.003181,Time used 0.002976s\n",
      "batch 3551, train_loss 0.003284,Time used 0.013391s\n",
      "batch 3552, train_loss 0.004732,Time used 0.002976s\n",
      "batch 3553, train_loss 0.004236,Time used 0.002985s\n",
      "batch 3554, train_loss 0.003509,Time used 0.003472s\n",
      "batch 3555, train_loss 0.003522,Time used 0.003968s\n",
      "batch 3556, train_loss 0.004134,Time used 0.002972s\n",
      "batch 3557, train_loss 0.003412,Time used 0.002976s\n",
      "batch 3558, train_loss 0.003110,Time used 0.003968s\n",
      "batch 3559, train_loss 0.003666,Time used 0.003472s\n",
      "batch 3560, train_loss 0.003920,Time used 0.003968s\n",
      "batch 3561, train_loss 0.003856,Time used 0.003471s\n",
      "batch 3562, train_loss 0.005261,Time used 0.003471s\n",
      "batch 3563, train_loss 0.003482,Time used 0.002480s\n",
      "batch 3564, train_loss 0.004499,Time used 0.003478s\n",
      "batch 3565, train_loss 0.004179,Time used 0.002968s\n",
      "batch 3566, train_loss 0.003420,Time used 0.002976s\n",
      "batch 3567, train_loss 0.003071,Time used 0.003472s\n",
      "batch 3568, train_loss 0.004920,Time used 0.003472s\n",
      "batch 3569, train_loss 0.004577,Time used 0.002976s\n",
      "batch 3570, train_loss 0.003683,Time used 0.002976s\n",
      "batch 3571, train_loss 0.003937,Time used 0.003973s\n",
      "batch 3572, train_loss 0.003245,Time used 0.002973s\n",
      "batch 3573, train_loss 0.004034,Time used 0.003968s\n",
      "batch 3574, train_loss 0.003473,Time used 0.002976s\n",
      "batch 3575, train_loss 0.004338,Time used 0.002977s\n",
      "batch 3576, train_loss 0.004525,Time used 0.002976s\n",
      "batch 3577, train_loss 0.004380,Time used 0.002967s\n",
      "batch 3578, train_loss 0.003396,Time used 0.003968s\n",
      "batch 3579, train_loss 0.004110,Time used 0.002976s\n",
      "batch 3580, train_loss 0.004036,Time used 0.003468s\n",
      "batch 3581, train_loss 0.003660,Time used 0.003472s\n",
      "batch 3582, train_loss 0.003084,Time used 0.003472s\n",
      "batch 3583, train_loss 0.005016,Time used 0.002971s\n",
      "batch 3584, train_loss 0.003886,Time used 0.003968s\n",
      "batch 3585, train_loss 0.005508,Time used 0.003472s\n",
      "batch 3586, train_loss 0.003538,Time used 0.003472s\n",
      "batch 3587, train_loss 0.003396,Time used 0.003968s\n",
      "batch 3588, train_loss 0.003812,Time used 0.003472s\n",
      "batch 3589, train_loss 0.003815,Time used 0.003968s\n",
      "batch 3590, train_loss 0.004664,Time used 0.003468s\n",
      "batch 3591, train_loss 0.005024,Time used 0.003474s\n",
      "batch 3592, train_loss 0.003516,Time used 0.002975s\n",
      "batch 3593, train_loss 0.004078,Time used 0.002977s\n",
      "batch 3594, train_loss 0.003805,Time used 0.002976s\n",
      "batch 3595, train_loss 0.003243,Time used 0.002975s\n",
      "batch 3596, train_loss 0.003759,Time used 0.003472s\n",
      "batch 3597, train_loss 0.002886,Time used 0.003467s\n",
      "batch 3598, train_loss 0.004073,Time used 0.002976s\n",
      "batch 3599, train_loss 0.003408,Time used 0.003472s\n",
      "batch 3600, train_loss 0.003452,Time used 0.004464s\n",
      "***************************test_batch 3600, test_rmse_loss 0.062407,test_mae_loss 0.045017,test_mape_loss 13.994584,Time used 0.011904s\n",
      "batch 3601, train_loss 0.003131,Time used 0.012399s\n",
      "batch 3602, train_loss 0.004475,Time used 0.002975s\n",
      "batch 3603, train_loss 0.003734,Time used 0.002976s\n",
      "batch 3604, train_loss 0.004711,Time used 0.002976s\n",
      "batch 3605, train_loss 0.003679,Time used 0.003472s\n",
      "batch 3606, train_loss 0.003735,Time used 0.003963s\n",
      "batch 3607, train_loss 0.003408,Time used 0.002977s\n",
      "batch 3608, train_loss 0.003066,Time used 0.002976s\n",
      "batch 3609, train_loss 0.003588,Time used 0.003472s\n",
      "batch 3610, train_loss 0.003332,Time used 0.003472s\n",
      "batch 3611, train_loss 0.004132,Time used 0.002984s\n",
      "batch 3612, train_loss 0.003708,Time used 0.002979s\n",
      "batch 3613, train_loss 0.004465,Time used 0.003474s\n",
      "batch 3614, train_loss 0.003820,Time used 0.002471s\n",
      "batch 3615, train_loss 0.004167,Time used 0.002977s\n",
      "batch 3616, train_loss 0.003604,Time used 0.002977s\n",
      "batch 3617, train_loss 0.004431,Time used 0.002975s\n",
      "batch 3618, train_loss 0.002680,Time used 0.002968s\n",
      "batch 3619, train_loss 0.003757,Time used 0.002976s\n",
      "batch 3620, train_loss 0.003623,Time used 0.002967s\n",
      "batch 3621, train_loss 0.003774,Time used 0.002976s\n",
      "batch 3622, train_loss 0.003014,Time used 0.002979s\n",
      "batch 3623, train_loss 0.003826,Time used 0.003968s\n",
      "batch 3624, train_loss 0.003561,Time used 0.003469s\n",
      "batch 3625, train_loss 0.003342,Time used 0.002480s\n",
      "batch 3626, train_loss 0.003501,Time used 0.002976s\n",
      "batch 3627, train_loss 0.004304,Time used 0.003968s\n",
      "batch 3628, train_loss 0.003993,Time used 0.003472s\n",
      "batch 3629, train_loss 0.003599,Time used 0.003968s\n",
      "batch 3630, train_loss 0.003040,Time used 0.003968s\n",
      "batch 3631, train_loss 0.003490,Time used 0.011904s\n",
      "batch 3632, train_loss 0.003494,Time used 0.003473s\n",
      "batch 3633, train_loss 0.003165,Time used 0.003468s\n",
      "batch 3634, train_loss 0.004342,Time used 0.002976s\n",
      "batch 3635, train_loss 0.004172,Time used 0.003473s\n",
      "batch 3636, train_loss 0.003912,Time used 0.002976s\n",
      "batch 3637, train_loss 0.003665,Time used 0.002976s\n",
      "batch 3638, train_loss 0.005563,Time used 0.003472s\n",
      "batch 3639, train_loss 0.003237,Time used 0.002975s\n",
      "batch 3640, train_loss 0.004273,Time used 0.003472s\n",
      "batch 3641, train_loss 0.004350,Time used 0.002480s\n",
      "batch 3642, train_loss 0.003506,Time used 0.003472s\n",
      "batch 3643, train_loss 0.002958,Time used 0.002480s\n",
      "batch 3644, train_loss 0.003605,Time used 0.002976s\n",
      "batch 3645, train_loss 0.003504,Time used 0.003472s\n",
      "batch 3646, train_loss 0.005128,Time used 0.002976s\n",
      "batch 3647, train_loss 0.003497,Time used 0.002481s\n",
      "batch 3648, train_loss 0.003683,Time used 0.002976s\n",
      "batch 3649, train_loss 0.004563,Time used 0.002976s\n",
      "batch 3650, train_loss 0.003558,Time used 0.002480s\n",
      "batch 3651, train_loss 0.004192,Time used 0.002976s\n",
      "batch 3652, train_loss 0.004772,Time used 0.002975s\n",
      "batch 3653, train_loss 0.004185,Time used 0.002976s\n",
      "batch 3654, train_loss 0.003698,Time used 0.002975s\n",
      "batch 3655, train_loss 0.003870,Time used 0.002976s\n",
      "batch 3656, train_loss 0.005470,Time used 0.003472s\n",
      "batch 3657, train_loss 0.004382,Time used 0.002974s\n",
      "batch 3658, train_loss 0.003219,Time used 0.002976s\n",
      "batch 3659, train_loss 0.003921,Time used 0.002976s\n",
      "batch 3660, train_loss 0.004248,Time used 0.002970s\n",
      "batch 3661, train_loss 0.003931,Time used 0.002972s\n",
      "batch 3662, train_loss 0.003835,Time used 0.002480s\n",
      "batch 3663, train_loss 0.004835,Time used 0.002976s\n",
      "batch 3664, train_loss 0.003529,Time used 0.002979s\n",
      "batch 3665, train_loss 0.004252,Time used 0.002976s\n",
      "batch 3666, train_loss 0.003151,Time used 0.002480s\n",
      "batch 3667, train_loss 0.003088,Time used 0.002976s\n",
      "batch 3668, train_loss 0.004191,Time used 0.003472s\n",
      "batch 3669, train_loss 0.004648,Time used 0.002976s\n",
      "batch 3670, train_loss 0.003253,Time used 0.002479s\n",
      "batch 3671, train_loss 0.003537,Time used 0.002976s\n",
      "batch 3672, train_loss 0.002374,Time used 0.002976s\n",
      "batch 3673, train_loss 0.005757,Time used 0.002480s\n",
      "batch 3674, train_loss 0.005047,Time used 0.002976s\n",
      "batch 3675, train_loss 0.003791,Time used 0.002481s\n",
      "batch 3676, train_loss 0.003371,Time used 0.002976s\n",
      "batch 3677, train_loss 0.003606,Time used 0.002480s\n",
      "batch 3678, train_loss 0.003911,Time used 0.002977s\n",
      "batch 3679, train_loss 0.003674,Time used 0.002972s\n",
      "batch 3680, train_loss 0.003986,Time used 0.003472s\n",
      "batch 3681, train_loss 0.004186,Time used 0.002976s\n",
      "batch 3682, train_loss 0.003665,Time used 0.002976s\n",
      "batch 3683, train_loss 0.003528,Time used 0.002480s\n",
      "batch 3684, train_loss 0.003521,Time used 0.002481s\n",
      "batch 3685, train_loss 0.003654,Time used 0.002976s\n",
      "batch 3686, train_loss 0.003219,Time used 0.002976s\n",
      "batch 3687, train_loss 0.004190,Time used 0.002975s\n",
      "batch 3688, train_loss 0.003338,Time used 0.002480s\n",
      "batch 3689, train_loss 0.004196,Time used 0.002975s\n",
      "batch 3690, train_loss 0.003870,Time used 0.003472s\n",
      "batch 3691, train_loss 0.004103,Time used 0.003472s\n",
      "batch 3692, train_loss 0.003479,Time used 0.003469s\n",
      "batch 3693, train_loss 0.004307,Time used 0.003472s\n",
      "batch 3694, train_loss 0.004336,Time used 0.002976s\n",
      "batch 3695, train_loss 0.003926,Time used 0.002976s\n",
      "batch 3696, train_loss 0.003994,Time used 0.002481s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3697, train_loss 0.003962,Time used 0.002976s\n",
      "batch 3698, train_loss 0.003972,Time used 0.003472s\n",
      "batch 3699, train_loss 0.003695,Time used 0.002976s\n",
      "batch 3700, train_loss 0.003674,Time used 0.002976s\n",
      "***************************test_batch 3700, test_rmse_loss 0.062299,test_mae_loss 0.044768,test_mape_loss 13.480255,Time used 0.010416s\n",
      "batch 3701, train_loss 0.003971,Time used 0.002976s\n",
      "batch 3702, train_loss 0.002863,Time used 0.002976s\n",
      "batch 3703, train_loss 0.003647,Time used 0.002976s\n",
      "batch 3704, train_loss 0.004184,Time used 0.002976s\n",
      "batch 3705, train_loss 0.002721,Time used 0.002468s\n",
      "batch 3706, train_loss 0.003167,Time used 0.002976s\n",
      "batch 3707, train_loss 0.003470,Time used 0.002976s\n",
      "batch 3708, train_loss 0.004090,Time used 0.002989s\n",
      "batch 3709, train_loss 0.004131,Time used 0.002976s\n",
      "batch 3710, train_loss 0.004033,Time used 0.002976s\n",
      "batch 3711, train_loss 0.003558,Time used 0.002480s\n",
      "batch 3712, train_loss 0.002943,Time used 0.002976s\n",
      "batch 3713, train_loss 0.004232,Time used 0.003472s\n",
      "batch 3714, train_loss 0.003480,Time used 0.003472s\n",
      "batch 3715, train_loss 0.004148,Time used 0.003955s\n",
      "batch 3716, train_loss 0.004258,Time used 0.002976s\n",
      "batch 3717, train_loss 0.004342,Time used 0.014384s\n",
      "batch 3718, train_loss 0.004040,Time used 0.002475s\n",
      "batch 3719, train_loss 0.003911,Time used 0.002976s\n",
      "batch 3720, train_loss 0.004139,Time used 0.002976s\n",
      "batch 3721, train_loss 0.004166,Time used 0.002976s\n",
      "batch 3722, train_loss 0.003449,Time used 0.003473s\n",
      "batch 3723, train_loss 0.003348,Time used 0.002967s\n",
      "batch 3724, train_loss 0.004461,Time used 0.002975s\n",
      "batch 3725, train_loss 0.003735,Time used 0.002480s\n",
      "batch 3726, train_loss 0.007144,Time used 0.003481s\n",
      "batch 3727, train_loss 0.004077,Time used 0.002976s\n",
      "batch 3728, train_loss 0.004698,Time used 0.002976s\n",
      "batch 3729, train_loss 0.003477,Time used 0.003475s\n",
      "batch 3730, train_loss 0.004602,Time used 0.003473s\n",
      "batch 3731, train_loss 0.004244,Time used 0.002976s\n",
      "batch 3732, train_loss 0.004722,Time used 0.002480s\n",
      "batch 3733, train_loss 0.003416,Time used 0.002975s\n",
      "batch 3734, train_loss 0.003258,Time used 0.002976s\n",
      "batch 3735, train_loss 0.004181,Time used 0.003976s\n",
      "batch 3736, train_loss 0.003534,Time used 0.002975s\n",
      "batch 3737, train_loss 0.003602,Time used 0.002976s\n",
      "batch 3738, train_loss 0.003672,Time used 0.002976s\n",
      "batch 3739, train_loss 0.003831,Time used 0.002976s\n",
      "batch 3740, train_loss 0.004426,Time used 0.003472s\n",
      "batch 3741, train_loss 0.003623,Time used 0.002975s\n",
      "batch 3742, train_loss 0.004371,Time used 0.002976s\n",
      "batch 3743, train_loss 0.003646,Time used 0.002976s\n",
      "batch 3744, train_loss 0.004012,Time used 0.002480s\n",
      "batch 3745, train_loss 0.004026,Time used 0.002976s\n",
      "batch 3746, train_loss 0.003378,Time used 0.002976s\n",
      "batch 3747, train_loss 0.004112,Time used 0.002975s\n",
      "batch 3748, train_loss 0.004923,Time used 0.002976s\n",
      "batch 3749, train_loss 0.003695,Time used 0.002977s\n",
      "batch 3750, train_loss 0.002878,Time used 0.002976s\n",
      "batch 3751, train_loss 0.003915,Time used 0.002976s\n",
      "batch 3752, train_loss 0.004715,Time used 0.003472s\n",
      "batch 3753, train_loss 0.004856,Time used 0.002483s\n",
      "batch 3754, train_loss 0.003093,Time used 0.002976s\n",
      "batch 3755, train_loss 0.003716,Time used 0.002977s\n",
      "batch 3756, train_loss 0.003313,Time used 0.002481s\n",
      "batch 3757, train_loss 0.003416,Time used 0.003473s\n",
      "batch 3758, train_loss 0.003168,Time used 0.004456s\n",
      "batch 3759, train_loss 0.003805,Time used 0.002976s\n",
      "batch 3760, train_loss 0.002857,Time used 0.003472s\n",
      "batch 3761, train_loss 0.003360,Time used 0.002977s\n",
      "batch 3762, train_loss 0.004037,Time used 0.002976s\n",
      "batch 3763, train_loss 0.003715,Time used 0.002975s\n",
      "batch 3764, train_loss 0.004057,Time used 0.002976s\n",
      "batch 3765, train_loss 0.004480,Time used 0.002480s\n",
      "batch 3766, train_loss 0.003737,Time used 0.002480s\n",
      "batch 3767, train_loss 0.003489,Time used 0.002976s\n",
      "batch 3768, train_loss 0.003561,Time used 0.002976s\n",
      "batch 3769, train_loss 0.003497,Time used 0.003472s\n",
      "batch 3770, train_loss 0.003793,Time used 0.003472s\n",
      "batch 3771, train_loss 0.003867,Time used 0.003472s\n",
      "batch 3772, train_loss 0.004141,Time used 0.002976s\n",
      "batch 3773, train_loss 0.004664,Time used 0.002975s\n",
      "batch 3774, train_loss 0.003654,Time used 0.002976s\n",
      "batch 3775, train_loss 0.003962,Time used 0.002976s\n",
      "batch 3776, train_loss 0.003808,Time used 0.002967s\n",
      "batch 3777, train_loss 0.004351,Time used 0.002976s\n",
      "batch 3778, train_loss 0.003887,Time used 0.002480s\n",
      "batch 3779, train_loss 0.005049,Time used 0.002481s\n",
      "batch 3780, train_loss 0.005349,Time used 0.002976s\n",
      "batch 3781, train_loss 0.003191,Time used 0.002976s\n",
      "batch 3782, train_loss 0.003304,Time used 0.002976s\n",
      "batch 3783, train_loss 0.004214,Time used 0.002977s\n",
      "batch 3784, train_loss 0.003350,Time used 0.002481s\n",
      "batch 3785, train_loss 0.003751,Time used 0.002976s\n",
      "batch 3786, train_loss 0.004104,Time used 0.003471s\n",
      "batch 3787, train_loss 0.004413,Time used 0.003472s\n",
      "batch 3788, train_loss 0.003573,Time used 0.002980s\n",
      "batch 3789, train_loss 0.004956,Time used 0.003472s\n",
      "batch 3790, train_loss 0.003291,Time used 0.003968s\n",
      "batch 3791, train_loss 0.003612,Time used 0.003472s\n",
      "batch 3792, train_loss 0.003549,Time used 0.004456s\n",
      "batch 3793, train_loss 0.003659,Time used 0.003472s\n",
      "batch 3794, train_loss 0.003777,Time used 0.003472s\n",
      "batch 3795, train_loss 0.003372,Time used 0.002976s\n",
      "batch 3796, train_loss 0.003691,Time used 0.003479s\n",
      "batch 3797, train_loss 0.003541,Time used 0.003972s\n",
      "batch 3798, train_loss 0.004712,Time used 0.003971s\n",
      "batch 3799, train_loss 0.004927,Time used 0.003472s\n",
      "batch 3800, train_loss 0.003596,Time used 0.003472s\n",
      "***************************test_batch 3800, test_rmse_loss 0.062225,test_mae_loss 0.044763,test_mape_loss 13.642294,Time used 0.013888s\n",
      "batch 3801, train_loss 0.003506,Time used 0.004456s\n",
      "batch 3802, train_loss 0.004197,Time used 0.003472s\n",
      "batch 3803, train_loss 0.004812,Time used 0.003469s\n",
      "batch 3804, train_loss 0.003570,Time used 0.011972s\n",
      "batch 3805, train_loss 0.003724,Time used 0.004464s\n",
      "batch 3806, train_loss 0.004347,Time used 0.003473s\n",
      "batch 3807, train_loss 0.003786,Time used 0.003472s\n",
      "batch 3808, train_loss 0.004253,Time used 0.014880s\n",
      "batch 3809, train_loss 0.003633,Time used 0.004464s\n",
      "batch 3810, train_loss 0.003819,Time used 0.002480s\n",
      "batch 3811, train_loss 0.003613,Time used 0.002976s\n",
      "batch 3812, train_loss 0.003657,Time used 0.002976s\n",
      "batch 3813, train_loss 0.003355,Time used 0.002975s\n",
      "batch 3814, train_loss 0.004680,Time used 0.002976s\n",
      "batch 3815, train_loss 0.003816,Time used 0.002975s\n",
      "batch 3816, train_loss 0.003899,Time used 0.002971s\n",
      "batch 3817, train_loss 0.004220,Time used 0.002976s\n",
      "batch 3818, train_loss 0.004488,Time used 0.002480s\n",
      "batch 3819, train_loss 0.003580,Time used 0.003472s\n",
      "batch 3820, train_loss 0.003886,Time used 0.002976s\n",
      "batch 3821, train_loss 0.003643,Time used 0.002976s\n",
      "batch 3822, train_loss 0.004276,Time used 0.003472s\n",
      "batch 3823, train_loss 0.003663,Time used 0.002976s\n",
      "batch 3824, train_loss 0.003435,Time used 0.002976s\n",
      "batch 3825, train_loss 0.003388,Time used 0.002480s\n",
      "batch 3826, train_loss 0.003407,Time used 0.002975s\n",
      "batch 3827, train_loss 0.004748,Time used 0.002981s\n",
      "batch 3828, train_loss 0.003567,Time used 0.002976s\n",
      "batch 3829, train_loss 0.002978,Time used 0.002971s\n",
      "batch 3830, train_loss 0.003624,Time used 0.002976s\n",
      "batch 3831, train_loss 0.003399,Time used 0.002975s\n",
      "batch 3832, train_loss 0.003840,Time used 0.003472s\n",
      "batch 3833, train_loss 0.004859,Time used 0.002976s\n",
      "batch 3834, train_loss 0.001518,Time used 0.002976s\n",
      "batch 3835, train_loss 0.003915,Time used 0.003472s\n",
      "batch 3836, train_loss 0.004224,Time used 0.002975s\n",
      "batch 3837, train_loss 0.003651,Time used 0.002972s\n",
      "batch 3838, train_loss 0.003922,Time used 0.003472s\n",
      "batch 3839, train_loss 0.002850,Time used 0.002976s\n",
      "batch 3840, train_loss 0.003943,Time used 0.003472s\n",
      "batch 3841, train_loss 0.003553,Time used 0.002968s\n",
      "batch 3842, train_loss 0.003328,Time used 0.002975s\n",
      "batch 3843, train_loss 0.003503,Time used 0.002968s\n",
      "batch 3844, train_loss 0.003488,Time used 0.002976s\n",
      "batch 3845, train_loss 0.005056,Time used 0.003472s\n",
      "batch 3846, train_loss 0.004899,Time used 0.002977s\n",
      "batch 3847, train_loss 0.003092,Time used 0.002976s\n",
      "batch 3848, train_loss 0.005396,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3849, train_loss 0.003536,Time used 0.002976s\n",
      "batch 3850, train_loss 0.004312,Time used 0.003464s\n",
      "batch 3851, train_loss 0.004325,Time used 0.002976s\n",
      "batch 3852, train_loss 0.003803,Time used 0.002975s\n",
      "batch 3853, train_loss 0.003691,Time used 0.003461s\n",
      "batch 3854, train_loss 0.003612,Time used 0.003472s\n",
      "batch 3855, train_loss 0.003402,Time used 0.002980s\n",
      "batch 3856, train_loss 0.003464,Time used 0.002976s\n",
      "batch 3857, train_loss 0.004439,Time used 0.002976s\n",
      "batch 3858, train_loss 0.003874,Time used 0.003472s\n",
      "batch 3859, train_loss 0.004216,Time used 0.003472s\n",
      "batch 3860, train_loss 0.003345,Time used 0.003472s\n",
      "batch 3861, train_loss 0.003667,Time used 0.003467s\n",
      "batch 3862, train_loss 0.004266,Time used 0.002976s\n",
      "batch 3863, train_loss 0.003502,Time used 0.002976s\n",
      "batch 3864, train_loss 0.003467,Time used 0.002976s\n",
      "batch 3865, train_loss 0.002847,Time used 0.002977s\n",
      "batch 3866, train_loss 0.004135,Time used 0.002977s\n",
      "batch 3867, train_loss 0.004214,Time used 0.003471s\n",
      "batch 3868, train_loss 0.004735,Time used 0.003473s\n",
      "batch 3869, train_loss 0.004409,Time used 0.002976s\n",
      "batch 3870, train_loss 0.004020,Time used 0.003471s\n",
      "batch 3871, train_loss 0.003725,Time used 0.002976s\n",
      "batch 3872, train_loss 0.004201,Time used 0.003472s\n",
      "batch 3873, train_loss 0.003678,Time used 0.002480s\n",
      "batch 3874, train_loss 0.003950,Time used 0.003968s\n",
      "batch 3875, train_loss 0.003772,Time used 0.002976s\n",
      "batch 3876, train_loss 0.004078,Time used 0.003472s\n",
      "batch 3877, train_loss 0.004348,Time used 0.002977s\n",
      "batch 3878, train_loss 0.003818,Time used 0.003968s\n",
      "batch 3879, train_loss 0.004148,Time used 0.002976s\n",
      "batch 3880, train_loss 0.003684,Time used 0.003472s\n",
      "batch 3881, train_loss 0.004420,Time used 0.003968s\n",
      "batch 3882, train_loss 0.003498,Time used 0.002981s\n",
      "batch 3883, train_loss 0.003143,Time used 0.003472s\n",
      "batch 3884, train_loss 0.003894,Time used 0.006447s\n",
      "batch 3885, train_loss 0.003778,Time used 0.003968s\n",
      "batch 3886, train_loss 0.003319,Time used 0.003473s\n",
      "batch 3887, train_loss 0.003208,Time used 0.013889s\n",
      "batch 3888, train_loss 0.002142,Time used 0.003473s\n",
      "batch 3889, train_loss 0.003675,Time used 0.002976s\n",
      "batch 3890, train_loss 0.004038,Time used 0.003964s\n",
      "batch 3891, train_loss 0.003470,Time used 0.003472s\n",
      "batch 3892, train_loss 0.003837,Time used 0.003472s\n",
      "batch 3893, train_loss 0.003274,Time used 0.003472s\n",
      "batch 3894, train_loss 0.003933,Time used 0.003968s\n",
      "batch 3895, train_loss 0.003128,Time used 0.002985s\n",
      "batch 3896, train_loss 0.003342,Time used 0.003472s\n",
      "batch 3897, train_loss 0.003541,Time used 0.002976s\n",
      "batch 3898, train_loss 0.003452,Time used 0.003971s\n",
      "batch 3899, train_loss 0.003374,Time used 0.003471s\n",
      "batch 3900, train_loss 0.005572,Time used 0.002976s\n",
      "***************************test_batch 3900, test_rmse_loss 0.062544,test_mae_loss 0.045251,test_mape_loss 14.339155,Time used 0.011408s\n",
      "batch 3901, train_loss 0.004394,Time used 0.002976s\n",
      "batch 3902, train_loss 0.004797,Time used 0.002480s\n",
      "batch 3903, train_loss 0.004655,Time used 0.002975s\n",
      "batch 3904, train_loss 0.003217,Time used 0.002481s\n",
      "batch 3905, train_loss 0.003011,Time used 0.003968s\n",
      "batch 3906, train_loss 0.003579,Time used 0.003473s\n",
      "batch 3907, train_loss 0.003922,Time used 0.003467s\n",
      "batch 3908, train_loss 0.004053,Time used 0.003965s\n",
      "batch 3909, train_loss 0.003695,Time used 0.002968s\n",
      "batch 3910, train_loss 0.003453,Time used 0.002976s\n",
      "batch 3911, train_loss 0.003795,Time used 0.002975s\n",
      "batch 3912, train_loss 0.003726,Time used 0.003473s\n",
      "batch 3913, train_loss 0.003820,Time used 0.003473s\n",
      "batch 3914, train_loss 0.003301,Time used 0.002969s\n",
      "batch 3915, train_loss 0.004790,Time used 0.003472s\n",
      "batch 3916, train_loss 0.004186,Time used 0.002976s\n",
      "batch 3917, train_loss 0.003629,Time used 0.002975s\n",
      "batch 3918, train_loss 0.003811,Time used 0.003472s\n",
      "batch 3919, train_loss 0.004279,Time used 0.002971s\n",
      "batch 3920, train_loss 0.003814,Time used 0.002976s\n",
      "batch 3921, train_loss 0.004748,Time used 0.003472s\n",
      "batch 3922, train_loss 0.004073,Time used 0.002976s\n",
      "batch 3923, train_loss 0.004757,Time used 0.002976s\n",
      "batch 3924, train_loss 0.003852,Time used 0.002976s\n",
      "batch 3925, train_loss 0.003548,Time used 0.002964s\n",
      "batch 3926, train_loss 0.002908,Time used 0.002976s\n",
      "batch 3927, train_loss 0.004753,Time used 0.003472s\n",
      "batch 3928, train_loss 0.003471,Time used 0.003969s\n",
      "batch 3929, train_loss 0.003719,Time used 0.002975s\n",
      "batch 3930, train_loss 0.004447,Time used 0.002964s\n",
      "batch 3931, train_loss 0.003349,Time used 0.002976s\n",
      "batch 3932, train_loss 0.003728,Time used 0.002976s\n",
      "batch 3933, train_loss 0.003705,Time used 0.002975s\n",
      "batch 3934, train_loss 0.003136,Time used 0.002976s\n",
      "batch 3935, train_loss 0.004576,Time used 0.002975s\n",
      "batch 3936, train_loss 0.004428,Time used 0.002976s\n",
      "batch 3937, train_loss 0.003901,Time used 0.002480s\n",
      "batch 3938, train_loss 0.003821,Time used 0.003473s\n",
      "batch 3939, train_loss 0.003671,Time used 0.002976s\n",
      "batch 3940, train_loss 0.003214,Time used 0.002968s\n",
      "batch 3941, train_loss 0.003369,Time used 0.002480s\n",
      "batch 3942, train_loss 0.002872,Time used 0.003473s\n",
      "batch 3943, train_loss 0.003790,Time used 0.002480s\n",
      "batch 3944, train_loss 0.003127,Time used 0.003968s\n",
      "batch 3945, train_loss 0.003813,Time used 0.003472s\n",
      "batch 3946, train_loss 0.003809,Time used 0.002976s\n",
      "batch 3947, train_loss 0.003509,Time used 0.002976s\n",
      "batch 3948, train_loss 0.005106,Time used 0.003472s\n",
      "batch 3949, train_loss 0.003019,Time used 0.002976s\n",
      "batch 3950, train_loss 0.003820,Time used 0.003472s\n",
      "batch 3951, train_loss 0.003222,Time used 0.002976s\n",
      "batch 3952, train_loss 0.003964,Time used 0.002480s\n",
      "batch 3953, train_loss 0.003882,Time used 0.002976s\n",
      "batch 3954, train_loss 0.003300,Time used 0.003471s\n",
      "batch 3955, train_loss 0.003342,Time used 0.002968s\n",
      "batch 3956, train_loss 0.003473,Time used 0.002976s\n",
      "batch 3957, train_loss 0.004653,Time used 0.003472s\n",
      "batch 3958, train_loss 0.003422,Time used 0.003472s\n",
      "batch 3959, train_loss 0.004403,Time used 0.003471s\n",
      "batch 3960, train_loss 0.004476,Time used 0.003472s\n",
      "batch 3961, train_loss 0.004532,Time used 0.002976s\n",
      "batch 3962, train_loss 0.004475,Time used 0.002976s\n",
      "batch 3963, train_loss 0.004351,Time used 0.003969s\n",
      "batch 3964, train_loss 0.003384,Time used 0.004465s\n",
      "batch 3965, train_loss 0.003781,Time used 0.003472s\n",
      "batch 3966, train_loss 0.003830,Time used 0.014880s\n",
      "batch 3967, train_loss 0.003147,Time used 0.002977s\n",
      "batch 3968, train_loss 0.004075,Time used 0.003472s\n",
      "batch 3969, train_loss 0.004245,Time used 0.003472s\n",
      "batch 3970, train_loss 0.004021,Time used 0.002976s\n",
      "batch 3971, train_loss 0.003132,Time used 0.003472s\n",
      "batch 3972, train_loss 0.004105,Time used 0.002976s\n",
      "batch 3973, train_loss 0.004187,Time used 0.002975s\n",
      "batch 3974, train_loss 0.003267,Time used 0.002976s\n",
      "batch 3975, train_loss 0.003576,Time used 0.002976s\n",
      "batch 3976, train_loss 0.004268,Time used 0.002976s\n",
      "batch 3977, train_loss 0.003807,Time used 0.002976s\n",
      "batch 3978, train_loss 0.003375,Time used 0.003472s\n",
      "batch 3979, train_loss 0.004332,Time used 0.002976s\n",
      "batch 3980, train_loss 0.004138,Time used 0.002975s\n",
      "batch 3981, train_loss 0.003382,Time used 0.002979s\n",
      "batch 3982, train_loss 0.004781,Time used 0.003472s\n",
      "batch 3983, train_loss 0.003891,Time used 0.003473s\n",
      "batch 3984, train_loss 0.003674,Time used 0.002480s\n",
      "batch 3985, train_loss 0.003602,Time used 0.003473s\n",
      "batch 3986, train_loss 0.003672,Time used 0.003472s\n",
      "batch 3987, train_loss 0.003659,Time used 0.003968s\n",
      "batch 3988, train_loss 0.003662,Time used 0.003472s\n",
      "batch 3989, train_loss 0.003243,Time used 0.002976s\n",
      "batch 3990, train_loss 0.004166,Time used 0.003469s\n",
      "batch 3991, train_loss 0.004077,Time used 0.002976s\n",
      "batch 3992, train_loss 0.003645,Time used 0.003472s\n",
      "batch 3993, train_loss 0.003604,Time used 0.003968s\n",
      "batch 3994, train_loss 0.004153,Time used 0.002976s\n",
      "batch 3995, train_loss 0.003809,Time used 0.003472s\n",
      "batch 3996, train_loss 0.002684,Time used 0.002480s\n",
      "batch 3997, train_loss 0.003607,Time used 0.002479s\n",
      "batch 3998, train_loss 0.003492,Time used 0.003969s\n",
      "batch 3999, train_loss 0.004094,Time used 0.002976s\n",
      "batch 4000, train_loss 0.004595,Time used 0.002976s\n",
      "***************************test_batch 4000, test_rmse_loss 0.062144,test_mae_loss 0.044829,test_mape_loss 13.956444,Time used 0.010913s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4001, train_loss 0.005409,Time used 0.003471s\n",
      "batch 4002, train_loss 0.002818,Time used 0.002982s\n",
      "batch 4003, train_loss 0.003979,Time used 0.002976s\n",
      "batch 4004, train_loss 0.003308,Time used 0.002976s\n",
      "batch 4005, train_loss 0.004232,Time used 0.002976s\n",
      "batch 4006, train_loss 0.004054,Time used 0.002970s\n",
      "batch 4007, train_loss 0.003883,Time used 0.003467s\n",
      "batch 4008, train_loss 0.003503,Time used 0.002976s\n",
      "batch 4009, train_loss 0.003784,Time used 0.003477s\n",
      "batch 4010, train_loss 0.003395,Time used 0.003968s\n",
      "batch 4011, train_loss 0.004528,Time used 0.003968s\n",
      "batch 4012, train_loss 0.004646,Time used 0.003968s\n",
      "batch 4013, train_loss 0.003279,Time used 0.003472s\n",
      "batch 4014, train_loss 0.003339,Time used 0.003472s\n",
      "batch 4015, train_loss 0.004984,Time used 0.011904s\n",
      "batch 4016, train_loss 0.003760,Time used 0.003472s\n",
      "batch 4017, train_loss 0.003643,Time used 0.003472s\n",
      "batch 4018, train_loss 0.003334,Time used 0.003472s\n",
      "batch 4019, train_loss 0.003770,Time used 0.002975s\n",
      "batch 4020, train_loss 0.003272,Time used 0.002976s\n",
      "batch 4021, train_loss 0.003471,Time used 0.002976s\n",
      "batch 4022, train_loss 0.003574,Time used 0.002976s\n",
      "batch 4023, train_loss 0.003484,Time used 0.002480s\n",
      "batch 4024, train_loss 0.003985,Time used 0.002972s\n",
      "batch 4025, train_loss 0.003800,Time used 0.002480s\n",
      "batch 4026, train_loss 0.004217,Time used 0.002976s\n",
      "batch 4027, train_loss 0.003675,Time used 0.002976s\n",
      "batch 4028, train_loss 0.003658,Time used 0.003472s\n",
      "batch 4029, train_loss 0.003250,Time used 0.002976s\n",
      "batch 4030, train_loss 0.003202,Time used 0.002480s\n",
      "batch 4031, train_loss 0.004477,Time used 0.002976s\n",
      "batch 4032, train_loss 0.004194,Time used 0.002480s\n",
      "batch 4033, train_loss 0.003508,Time used 0.003473s\n",
      "batch 4034, train_loss 0.003130,Time used 0.002976s\n",
      "batch 4035, train_loss 0.003613,Time used 0.002976s\n",
      "batch 4036, train_loss 0.003662,Time used 0.002976s\n",
      "batch 4037, train_loss 0.003037,Time used 0.002976s\n",
      "batch 4038, train_loss 0.003928,Time used 0.002976s\n",
      "batch 4039, train_loss 0.005216,Time used 0.002971s\n",
      "batch 4040, train_loss 0.002917,Time used 0.002976s\n",
      "batch 4041, train_loss 0.003796,Time used 0.003472s\n",
      "batch 4042, train_loss 0.003765,Time used 0.002976s\n",
      "batch 4043, train_loss 0.004855,Time used 0.002976s\n",
      "batch 4044, train_loss 0.003735,Time used 0.002976s\n",
      "batch 4045, train_loss 0.003810,Time used 0.002480s\n",
      "batch 4046, train_loss 0.004332,Time used 0.003465s\n",
      "batch 4047, train_loss 0.004503,Time used 0.002976s\n",
      "batch 4048, train_loss 0.003447,Time used 0.002976s\n",
      "batch 4049, train_loss 0.004101,Time used 0.002976s\n",
      "batch 4050, train_loss 0.004062,Time used 0.002976s\n",
      "batch 4051, train_loss 0.003701,Time used 0.002975s\n",
      "batch 4052, train_loss 0.003271,Time used 0.004463s\n",
      "batch 4053, train_loss 0.003575,Time used 0.014384s\n",
      "batch 4054, train_loss 0.003688,Time used 0.002976s\n",
      "batch 4055, train_loss 0.003669,Time used 0.002977s\n",
      "batch 4056, train_loss 0.004215,Time used 0.002976s\n",
      "batch 4057, train_loss 0.004374,Time used 0.002975s\n",
      "batch 4058, train_loss 0.002594,Time used 0.002976s\n",
      "batch 4059, train_loss 0.003432,Time used 0.002976s\n",
      "batch 4060, train_loss 0.004196,Time used 0.002980s\n",
      "batch 4061, train_loss 0.003159,Time used 0.002976s\n",
      "batch 4062, train_loss 0.004568,Time used 0.002977s\n",
      "batch 4063, train_loss 0.003659,Time used 0.002976s\n",
      "batch 4064, train_loss 0.004141,Time used 0.002976s\n",
      "batch 4065, train_loss 0.003444,Time used 0.002976s\n",
      "batch 4066, train_loss 0.003234,Time used 0.002976s\n",
      "batch 4067, train_loss 0.003302,Time used 0.002976s\n",
      "batch 4068, train_loss 0.004594,Time used 0.002964s\n",
      "batch 4069, train_loss 0.004036,Time used 0.003472s\n",
      "batch 4070, train_loss 0.003402,Time used 0.002976s\n",
      "batch 4071, train_loss 0.004125,Time used 0.002976s\n",
      "batch 4072, train_loss 0.004548,Time used 0.002976s\n",
      "batch 4073, train_loss 0.003556,Time used 0.003471s\n",
      "batch 4074, train_loss 0.002774,Time used 0.002480s\n",
      "batch 4075, train_loss 0.004192,Time used 0.002480s\n",
      "batch 4076, train_loss 0.004073,Time used 0.002480s\n",
      "batch 4077, train_loss 0.004564,Time used 0.002968s\n",
      "batch 4078, train_loss 0.004106,Time used 0.002976s\n",
      "batch 4079, train_loss 0.003622,Time used 0.002976s\n",
      "batch 4080, train_loss 0.004277,Time used 0.003472s\n",
      "batch 4081, train_loss 0.003779,Time used 0.003472s\n",
      "batch 4082, train_loss 0.003132,Time used 0.002976s\n",
      "batch 4083, train_loss 0.003689,Time used 0.002480s\n",
      "batch 4084, train_loss 0.003972,Time used 0.002976s\n",
      "batch 4085, train_loss 0.003564,Time used 0.002973s\n",
      "batch 4086, train_loss 0.003101,Time used 0.003472s\n",
      "batch 4087, train_loss 0.004041,Time used 0.003472s\n",
      "batch 4088, train_loss 0.003730,Time used 0.002977s\n",
      "batch 4089, train_loss 0.003196,Time used 0.003472s\n",
      "batch 4090, train_loss 0.003280,Time used 0.003469s\n",
      "batch 4091, train_loss 0.003680,Time used 0.002976s\n",
      "batch 4092, train_loss 0.005127,Time used 0.002976s\n",
      "batch 4093, train_loss 0.004578,Time used 0.002976s\n",
      "batch 4094, train_loss 0.005447,Time used 0.002976s\n",
      "batch 4095, train_loss 0.003965,Time used 0.002974s\n",
      "batch 4096, train_loss 0.003525,Time used 0.003468s\n",
      "batch 4097, train_loss 0.004236,Time used 0.003963s\n",
      "batch 4098, train_loss 0.004638,Time used 0.002973s\n",
      "batch 4099, train_loss 0.003750,Time used 0.002979s\n",
      "batch 4100, train_loss 0.003550,Time used 0.002971s\n",
      "***************************test_batch 4100, test_rmse_loss 0.062044,test_mae_loss 0.044554,test_mape_loss 13.337444,Time used 0.012896s\n",
      "batch 4101, train_loss 0.004772,Time used 0.003968s\n",
      "batch 4102, train_loss 0.003818,Time used 0.002977s\n",
      "batch 4103, train_loss 0.002866,Time used 0.002976s\n",
      "batch 4104, train_loss 0.001799,Time used 0.002977s\n",
      "batch 4105, train_loss 0.004090,Time used 0.002975s\n",
      "batch 4106, train_loss 0.004255,Time used 0.002976s\n",
      "batch 4107, train_loss 0.003603,Time used 0.003464s\n",
      "batch 4108, train_loss 0.003866,Time used 0.003473s\n",
      "batch 4109, train_loss 0.004291,Time used 0.003472s\n",
      "batch 4110, train_loss 0.003325,Time used 0.003969s\n",
      "batch 4111, train_loss 0.005272,Time used 0.002976s\n",
      "batch 4112, train_loss 0.003834,Time used 0.002975s\n",
      "batch 4113, train_loss 0.004126,Time used 0.002480s\n",
      "batch 4114, train_loss 0.003974,Time used 0.002976s\n",
      "batch 4115, train_loss 0.002955,Time used 0.003472s\n",
      "batch 4116, train_loss 0.003974,Time used 0.003473s\n",
      "batch 4117, train_loss 0.003711,Time used 0.002976s\n",
      "batch 4118, train_loss 0.004929,Time used 0.002980s\n",
      "batch 4119, train_loss 0.004056,Time used 0.003968s\n",
      "batch 4120, train_loss 0.003297,Time used 0.002479s\n",
      "batch 4121, train_loss 0.003893,Time used 0.002976s\n",
      "batch 4122, train_loss 0.003994,Time used 0.002976s\n",
      "batch 4123, train_loss 0.004449,Time used 0.002472s\n",
      "batch 4124, train_loss 0.003882,Time used 0.002484s\n",
      "batch 4125, train_loss 0.003721,Time used 0.002481s\n",
      "batch 4126, train_loss 0.004792,Time used 0.002985s\n",
      "batch 4127, train_loss 0.003991,Time used 0.002976s\n",
      "batch 4128, train_loss 0.003394,Time used 0.002480s\n",
      "batch 4129, train_loss 0.003316,Time used 0.002480s\n",
      "batch 4130, train_loss 0.003131,Time used 0.002976s\n",
      "batch 4131, train_loss 0.003657,Time used 0.002976s\n",
      "batch 4132, train_loss 0.003132,Time used 0.002976s\n",
      "batch 4133, train_loss 0.003329,Time used 0.003472s\n",
      "batch 4134, train_loss 0.004046,Time used 0.002976s\n",
      "batch 4135, train_loss 0.004163,Time used 0.002976s\n",
      "batch 4136, train_loss 0.003183,Time used 0.003964s\n",
      "batch 4137, train_loss 0.003503,Time used 0.002976s\n",
      "batch 4138, train_loss 0.003621,Time used 0.012400s\n",
      "batch 4139, train_loss 0.003388,Time used 0.002976s\n",
      "batch 4140, train_loss 0.004193,Time used 0.002481s\n",
      "batch 4141, train_loss 0.004095,Time used 0.002976s\n",
      "batch 4142, train_loss 0.004106,Time used 0.002976s\n",
      "batch 4143, train_loss 0.002917,Time used 0.003468s\n",
      "batch 4144, train_loss 0.003777,Time used 0.002481s\n",
      "batch 4145, train_loss 0.003738,Time used 0.002976s\n",
      "batch 4146, train_loss 0.003877,Time used 0.002468s\n",
      "batch 4147, train_loss 0.005063,Time used 0.002480s\n",
      "batch 4148, train_loss 0.003238,Time used 0.002976s\n",
      "batch 4149, train_loss 0.003450,Time used 0.002976s\n",
      "batch 4150, train_loss 0.003632,Time used 0.002976s\n",
      "batch 4151, train_loss 0.003865,Time used 0.002976s\n",
      "batch 4152, train_loss 0.003496,Time used 0.002975s\n",
      "batch 4153, train_loss 0.004509,Time used 0.003473s\n",
      "batch 4154, train_loss 0.003364,Time used 0.002975s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4155, train_loss 0.003641,Time used 0.002976s\n",
      "batch 4156, train_loss 0.003715,Time used 0.003472s\n",
      "batch 4157, train_loss 0.003416,Time used 0.002980s\n",
      "batch 4158, train_loss 0.001970,Time used 0.003472s\n",
      "batch 4159, train_loss 0.003411,Time used 0.002977s\n",
      "batch 4160, train_loss 0.004076,Time used 0.003472s\n",
      "batch 4161, train_loss 0.003305,Time used 0.002976s\n",
      "batch 4162, train_loss 0.004636,Time used 0.002972s\n",
      "batch 4163, train_loss 0.003823,Time used 0.002479s\n",
      "batch 4164, train_loss 0.003260,Time used 0.002976s\n",
      "batch 4165, train_loss 0.003710,Time used 0.002975s\n",
      "batch 4166, train_loss 0.003571,Time used 0.003472s\n",
      "batch 4167, train_loss 0.003286,Time used 0.002976s\n",
      "batch 4168, train_loss 0.003368,Time used 0.003467s\n",
      "batch 4169, train_loss 0.004802,Time used 0.002976s\n",
      "batch 4170, train_loss 0.003469,Time used 0.002976s\n",
      "batch 4171, train_loss 0.003628,Time used 0.003472s\n",
      "batch 4172, train_loss 0.003980,Time used 0.002976s\n",
      "batch 4173, train_loss 0.003843,Time used 0.002475s\n",
      "batch 4174, train_loss 0.003711,Time used 0.002976s\n",
      "batch 4175, train_loss 0.003914,Time used 0.002976s\n",
      "batch 4176, train_loss 0.005454,Time used 0.002976s\n",
      "batch 4177, train_loss 0.004380,Time used 0.002976s\n",
      "batch 4178, train_loss 0.003016,Time used 0.002976s\n",
      "batch 4179, train_loss 0.004455,Time used 0.002976s\n",
      "batch 4180, train_loss 0.003767,Time used 0.002480s\n",
      "batch 4181, train_loss 0.003446,Time used 0.002976s\n",
      "batch 4182, train_loss 0.003337,Time used 0.002976s\n",
      "batch 4183, train_loss 0.004195,Time used 0.002977s\n",
      "batch 4184, train_loss 0.003513,Time used 0.002976s\n",
      "batch 4185, train_loss 0.004135,Time used 0.002976s\n",
      "batch 4186, train_loss 0.003868,Time used 0.002976s\n",
      "batch 4187, train_loss 0.003211,Time used 0.002976s\n",
      "batch 4188, train_loss 0.003105,Time used 0.002975s\n",
      "batch 4189, train_loss 0.005127,Time used 0.002976s\n",
      "batch 4190, train_loss 0.003793,Time used 0.002976s\n",
      "batch 4191, train_loss 0.004060,Time used 0.002479s\n",
      "batch 4192, train_loss 0.003348,Time used 0.002480s\n",
      "batch 4193, train_loss 0.004185,Time used 0.002976s\n",
      "batch 4194, train_loss 0.004001,Time used 0.002976s\n",
      "batch 4195, train_loss 0.004055,Time used 0.002976s\n",
      "batch 4196, train_loss 0.004788,Time used 0.002977s\n",
      "batch 4197, train_loss 0.003214,Time used 0.002976s\n",
      "batch 4198, train_loss 0.003237,Time used 0.002480s\n",
      "batch 4199, train_loss 0.003832,Time used 0.003472s\n",
      "batch 4200, train_loss 0.004158,Time used 0.002976s\n",
      "***************************test_batch 4200, test_rmse_loss 0.061976,test_mae_loss 0.044511,test_mape_loss 13.370045,Time used 0.010416s\n",
      "batch 4201, train_loss 0.003915,Time used 0.002987s\n",
      "batch 4202, train_loss 0.004037,Time used 0.002480s\n",
      "batch 4203, train_loss 0.003802,Time used 0.003472s\n",
      "batch 4204, train_loss 0.004328,Time used 0.003472s\n",
      "batch 4205, train_loss 0.003040,Time used 0.002976s\n",
      "batch 4206, train_loss 0.003766,Time used 0.002976s\n",
      "batch 4207, train_loss 0.003863,Time used 0.002976s\n",
      "batch 4208, train_loss 0.003400,Time used 0.002480s\n",
      "batch 4209, train_loss 0.003261,Time used 0.002976s\n",
      "batch 4210, train_loss 0.003839,Time used 0.002975s\n",
      "batch 4211, train_loss 0.003331,Time used 0.002976s\n",
      "batch 4212, train_loss 0.004865,Time used 0.003473s\n",
      "batch 4213, train_loss 0.004634,Time used 0.002480s\n",
      "batch 4214, train_loss 0.004347,Time used 0.002980s\n",
      "batch 4215, train_loss 0.003398,Time used 0.002976s\n",
      "batch 4216, train_loss 0.003889,Time used 0.003472s\n",
      "batch 4217, train_loss 0.003936,Time used 0.002976s\n",
      "batch 4218, train_loss 0.003763,Time used 0.002976s\n",
      "batch 4219, train_loss 0.003908,Time used 0.002976s\n",
      "batch 4220, train_loss 0.004599,Time used 0.002979s\n",
      "batch 4221, train_loss 0.003815,Time used 0.002976s\n",
      "batch 4222, train_loss 0.003977,Time used 0.004464s\n",
      "batch 4223, train_loss 0.003244,Time used 0.003968s\n",
      "batch 4224, train_loss 0.003260,Time used 0.002976s\n",
      "batch 4225, train_loss 0.004146,Time used 0.003476s\n",
      "batch 4226, train_loss 0.005029,Time used 0.002976s\n",
      "batch 4227, train_loss 0.003733,Time used 0.012400s\n",
      "batch 4228, train_loss 0.003455,Time used 0.003473s\n",
      "batch 4229, train_loss 0.003900,Time used 0.003472s\n",
      "batch 4230, train_loss 0.003775,Time used 0.011904s\n",
      "batch 4231, train_loss 0.004102,Time used 0.002976s\n",
      "batch 4232, train_loss 0.003598,Time used 0.003472s\n",
      "batch 4233, train_loss 0.003861,Time used 0.003472s\n",
      "batch 4234, train_loss 0.003003,Time used 0.002971s\n",
      "batch 4235, train_loss 0.005111,Time used 0.003473s\n",
      "batch 4236, train_loss 0.003975,Time used 0.002480s\n",
      "batch 4237, train_loss 0.003187,Time used 0.002479s\n",
      "batch 4238, train_loss 0.003463,Time used 0.002977s\n",
      "batch 4239, train_loss 0.003325,Time used 0.002976s\n",
      "batch 4240, train_loss 0.003703,Time used 0.002977s\n",
      "batch 4241, train_loss 0.003780,Time used 0.002975s\n",
      "batch 4242, train_loss 0.003624,Time used 0.002976s\n",
      "batch 4243, train_loss 0.004108,Time used 0.002979s\n",
      "batch 4244, train_loss 0.004510,Time used 0.002973s\n",
      "batch 4245, train_loss 0.004740,Time used 0.002975s\n",
      "batch 4246, train_loss 0.003343,Time used 0.002976s\n",
      "batch 4247, train_loss 0.004174,Time used 0.002976s\n",
      "batch 4248, train_loss 0.003730,Time used 0.002976s\n",
      "batch 4249, train_loss 0.003028,Time used 0.002975s\n",
      "batch 4250, train_loss 0.003486,Time used 0.002976s\n",
      "batch 4251, train_loss 0.003179,Time used 0.002976s\n",
      "batch 4252, train_loss 0.003836,Time used 0.002480s\n",
      "batch 4253, train_loss 0.003814,Time used 0.002975s\n",
      "batch 4254, train_loss 0.003682,Time used 0.002968s\n",
      "batch 4255, train_loss 0.003417,Time used 0.002977s\n",
      "batch 4256, train_loss 0.004386,Time used 0.002983s\n",
      "batch 4257, train_loss 0.003558,Time used 0.002976s\n",
      "batch 4258, train_loss 0.004134,Time used 0.003472s\n",
      "batch 4259, train_loss 0.002964,Time used 0.002975s\n",
      "batch 4260, train_loss 0.003305,Time used 0.003471s\n",
      "batch 4261, train_loss 0.003509,Time used 0.003473s\n",
      "batch 4262, train_loss 0.004161,Time used 0.002976s\n",
      "batch 4263, train_loss 0.003152,Time used 0.002976s\n",
      "batch 4264, train_loss 0.004059,Time used 0.003472s\n",
      "batch 4265, train_loss 0.003774,Time used 0.002975s\n",
      "batch 4266, train_loss 0.004745,Time used 0.002480s\n",
      "batch 4267, train_loss 0.003920,Time used 0.002969s\n",
      "batch 4268, train_loss 0.003615,Time used 0.002976s\n",
      "batch 4269, train_loss 0.003964,Time used 0.003472s\n",
      "batch 4270, train_loss 0.003799,Time used 0.002480s\n",
      "batch 4271, train_loss 0.004133,Time used 0.003472s\n",
      "batch 4272, train_loss 0.003857,Time used 0.002976s\n",
      "batch 4273, train_loss 0.003502,Time used 0.003472s\n",
      "batch 4274, train_loss 0.004717,Time used 0.003464s\n",
      "batch 4275, train_loss 0.003691,Time used 0.002479s\n",
      "batch 4276, train_loss 0.003574,Time used 0.002976s\n",
      "batch 4277, train_loss 0.003347,Time used 0.002976s\n",
      "batch 4278, train_loss 0.003834,Time used 0.003469s\n",
      "batch 4279, train_loss 0.003672,Time used 0.002976s\n",
      "batch 4280, train_loss 0.003828,Time used 0.002976s\n",
      "batch 4281, train_loss 0.003667,Time used 0.003472s\n",
      "batch 4282, train_loss 0.003247,Time used 0.002480s\n",
      "batch 4283, train_loss 0.004283,Time used 0.003460s\n",
      "batch 4284, train_loss 0.002833,Time used 0.003464s\n",
      "batch 4285, train_loss 0.003969,Time used 0.003472s\n",
      "batch 4286, train_loss 0.004257,Time used 0.003472s\n",
      "batch 4287, train_loss 0.003293,Time used 0.002976s\n",
      "batch 4288, train_loss 0.003881,Time used 0.002976s\n",
      "batch 4289, train_loss 0.003391,Time used 0.003472s\n",
      "batch 4290, train_loss 0.004083,Time used 0.002976s\n",
      "batch 4291, train_loss 0.003491,Time used 0.002976s\n",
      "batch 4292, train_loss 0.004056,Time used 0.002980s\n",
      "batch 4293, train_loss 0.003532,Time used 0.002481s\n",
      "batch 4294, train_loss 0.004592,Time used 0.002976s\n",
      "batch 4295, train_loss 0.004040,Time used 0.002976s\n",
      "batch 4296, train_loss 0.004264,Time used 0.003471s\n",
      "batch 4297, train_loss 0.004520,Time used 0.003472s\n",
      "batch 4298, train_loss 0.003115,Time used 0.002976s\n",
      "batch 4299, train_loss 0.003831,Time used 0.002975s\n",
      "batch 4300, train_loss 0.003401,Time used 0.002480s\n",
      "***************************test_batch 4300, test_rmse_loss 0.061920,test_mae_loss 0.044593,test_mape_loss 13.711191,Time used 0.010913s\n",
      "batch 4301, train_loss 0.002897,Time used 0.002977s\n",
      "batch 4302, train_loss 0.003537,Time used 0.002477s\n",
      "batch 4303, train_loss 0.004068,Time used 0.002476s\n",
      "batch 4304, train_loss 0.003702,Time used 0.002976s\n",
      "batch 4305, train_loss 0.003247,Time used 0.003470s\n",
      "batch 4306, train_loss 0.004718,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4307, train_loss 0.003292,Time used 0.003472s\n",
      "batch 4308, train_loss 0.005155,Time used 0.002985s\n",
      "batch 4309, train_loss 0.003055,Time used 0.002976s\n",
      "batch 4310, train_loss 0.003997,Time used 0.002976s\n",
      "batch 4311, train_loss 0.004451,Time used 0.003473s\n",
      "batch 4312, train_loss 0.003678,Time used 0.003471s\n",
      "batch 4313, train_loss 0.003849,Time used 0.014384s\n",
      "batch 4314, train_loss 0.003941,Time used 0.003472s\n",
      "batch 4315, train_loss 0.003629,Time used 0.002977s\n",
      "batch 4316, train_loss 0.003724,Time used 0.003471s\n",
      "batch 4317, train_loss 0.004605,Time used 0.002976s\n",
      "batch 4318, train_loss 0.004048,Time used 0.002480s\n",
      "batch 4319, train_loss 0.004039,Time used 0.003472s\n",
      "batch 4320, train_loss 0.002868,Time used 0.002976s\n",
      "batch 4321, train_loss 0.003714,Time used 0.002480s\n",
      "batch 4322, train_loss 0.004780,Time used 0.002976s\n",
      "batch 4323, train_loss 0.004164,Time used 0.002976s\n",
      "batch 4324, train_loss 0.004297,Time used 0.002976s\n",
      "batch 4325, train_loss 0.004789,Time used 0.002977s\n",
      "batch 4326, train_loss 0.003607,Time used 0.003473s\n",
      "batch 4327, train_loss 0.003259,Time used 0.003472s\n",
      "batch 4328, train_loss 0.004522,Time used 0.002976s\n",
      "batch 4329, train_loss 0.003289,Time used 0.003472s\n",
      "batch 4330, train_loss 0.003609,Time used 0.002972s\n",
      "batch 4331, train_loss 0.003262,Time used 0.002976s\n",
      "batch 4332, train_loss 0.003122,Time used 0.003473s\n",
      "batch 4333, train_loss 0.004122,Time used 0.002976s\n",
      "batch 4334, train_loss 0.003914,Time used 0.002976s\n",
      "batch 4335, train_loss 0.003606,Time used 0.002983s\n",
      "batch 4336, train_loss 0.004153,Time used 0.003472s\n",
      "batch 4337, train_loss 0.003858,Time used 0.002975s\n",
      "batch 4338, train_loss 0.003312,Time used 0.002977s\n",
      "batch 4339, train_loss 0.004232,Time used 0.002976s\n",
      "batch 4340, train_loss 0.003607,Time used 0.002976s\n",
      "batch 4341, train_loss 0.004314,Time used 0.002976s\n",
      "batch 4342, train_loss 0.003487,Time used 0.003473s\n",
      "batch 4343, train_loss 0.003374,Time used 0.002975s\n",
      "batch 4344, train_loss 0.003608,Time used 0.003472s\n",
      "batch 4345, train_loss 0.003647,Time used 0.002977s\n",
      "batch 4346, train_loss 0.003323,Time used 0.003472s\n",
      "batch 4347, train_loss 0.003833,Time used 0.002976s\n",
      "batch 4348, train_loss 0.003312,Time used 0.002976s\n",
      "batch 4349, train_loss 0.003493,Time used 0.003959s\n",
      "batch 4350, train_loss 0.003458,Time used 0.003472s\n",
      "batch 4351, train_loss 0.004090,Time used 0.002976s\n",
      "batch 4352, train_loss 0.003338,Time used 0.002976s\n",
      "batch 4353, train_loss 0.004510,Time used 0.002977s\n",
      "batch 4354, train_loss 0.004153,Time used 0.002976s\n",
      "batch 4355, train_loss 0.004000,Time used 0.002972s\n",
      "batch 4356, train_loss 0.004020,Time used 0.003485s\n",
      "batch 4357, train_loss 0.003426,Time used 0.002980s\n",
      "batch 4358, train_loss 0.003641,Time used 0.002480s\n",
      "batch 4359, train_loss 0.003877,Time used 0.003472s\n",
      "batch 4360, train_loss 0.004509,Time used 0.002976s\n",
      "batch 4361, train_loss 0.003937,Time used 0.003473s\n",
      "batch 4362, train_loss 0.003228,Time used 0.003968s\n",
      "batch 4363, train_loss 0.005296,Time used 0.002976s\n",
      "batch 4364, train_loss 0.003814,Time used 0.003472s\n",
      "batch 4365, train_loss 0.003987,Time used 0.002977s\n",
      "batch 4366, train_loss 0.003053,Time used 0.003472s\n",
      "batch 4367, train_loss 0.003868,Time used 0.002976s\n",
      "batch 4368, train_loss 0.003755,Time used 0.003472s\n",
      "batch 4369, train_loss 0.003817,Time used 0.002976s\n",
      "batch 4370, train_loss 0.003281,Time used 0.003460s\n",
      "batch 4371, train_loss 0.003830,Time used 0.002976s\n",
      "batch 4372, train_loss 0.003165,Time used 0.003472s\n",
      "batch 4373, train_loss 0.004768,Time used 0.003483s\n",
      "batch 4374, train_loss 0.000517,Time used 0.003472s\n",
      "batch 4375, train_loss 0.003731,Time used 0.002479s\n",
      "batch 4376, train_loss 0.003108,Time used 0.002977s\n",
      "batch 4377, train_loss 0.003364,Time used 0.003472s\n",
      "batch 4378, train_loss 0.002956,Time used 0.002976s\n",
      "batch 4379, train_loss 0.003809,Time used 0.003472s\n",
      "batch 4380, train_loss 0.004004,Time used 0.002975s\n",
      "batch 4381, train_loss 0.003917,Time used 0.003472s\n",
      "batch 4382, train_loss 0.004047,Time used 0.002976s\n",
      "batch 4383, train_loss 0.003364,Time used 0.002975s\n",
      "batch 4384, train_loss 0.003173,Time used 0.002481s\n",
      "batch 4385, train_loss 0.004649,Time used 0.003473s\n",
      "batch 4386, train_loss 0.003448,Time used 0.002976s\n",
      "batch 4387, train_loss 0.003852,Time used 0.002975s\n",
      "batch 4388, train_loss 0.003456,Time used 0.002976s\n",
      "batch 4389, train_loss 0.003129,Time used 0.002976s\n",
      "batch 4390, train_loss 0.004416,Time used 0.002977s\n",
      "batch 4391, train_loss 0.003585,Time used 0.002975s\n",
      "batch 4392, train_loss 0.004509,Time used 0.002977s\n",
      "batch 4393, train_loss 0.004412,Time used 0.002976s\n",
      "batch 4394, train_loss 0.003853,Time used 0.003968s\n",
      "batch 4395, train_loss 0.004195,Time used 0.002975s\n",
      "batch 4396, train_loss 0.003956,Time used 0.003472s\n",
      "batch 4397, train_loss 0.003495,Time used 0.003477s\n",
      "batch 4398, train_loss 0.004179,Time used 0.012895s\n",
      "batch 4399, train_loss 0.004138,Time used 0.002975s\n",
      "batch 4400, train_loss 0.003741,Time used 0.003472s\n",
      "***************************test_batch 4400, test_rmse_loss 0.061991,test_mae_loss 0.044617,test_mape_loss 13.713329,Time used 0.010416s\n",
      "batch 4401, train_loss 0.004422,Time used 0.002480s\n",
      "batch 4402, train_loss 0.003993,Time used 0.002975s\n",
      "batch 4403, train_loss 0.003319,Time used 0.002976s\n",
      "batch 4404, train_loss 0.004036,Time used 0.002976s\n",
      "batch 4405, train_loss 0.005490,Time used 0.002976s\n",
      "batch 4406, train_loss 0.004484,Time used 0.002976s\n",
      "batch 4407, train_loss 0.003697,Time used 0.002976s\n",
      "batch 4408, train_loss 0.004061,Time used 0.002976s\n",
      "batch 4409, train_loss 0.004012,Time used 0.003472s\n",
      "batch 4410, train_loss 0.002278,Time used 0.003473s\n",
      "batch 4411, train_loss 0.004102,Time used 0.002480s\n",
      "batch 4412, train_loss 0.003496,Time used 0.002976s\n",
      "batch 4413, train_loss 0.004501,Time used 0.002484s\n",
      "batch 4414, train_loss 0.004453,Time used 0.003472s\n",
      "batch 4415, train_loss 0.003674,Time used 0.003472s\n",
      "batch 4416, train_loss 0.003033,Time used 0.002976s\n",
      "batch 4417, train_loss 0.002912,Time used 0.002976s\n",
      "batch 4418, train_loss 0.004358,Time used 0.002480s\n",
      "batch 4419, train_loss 0.003609,Time used 0.002481s\n",
      "batch 4420, train_loss 0.004186,Time used 0.002480s\n",
      "batch 4421, train_loss 0.004012,Time used 0.002480s\n",
      "batch 4422, train_loss 0.003190,Time used 0.002976s\n",
      "batch 4423, train_loss 0.003163,Time used 0.002480s\n",
      "batch 4424, train_loss 0.003318,Time used 0.002975s\n",
      "batch 4425, train_loss 0.004095,Time used 0.002480s\n",
      "batch 4426, train_loss 0.003505,Time used 0.002967s\n",
      "batch 4427, train_loss 0.003327,Time used 0.002976s\n",
      "batch 4428, train_loss 0.006972,Time used 0.003472s\n",
      "batch 4429, train_loss 0.003191,Time used 0.002480s\n",
      "batch 4430, train_loss 0.003112,Time used 0.002485s\n",
      "batch 4431, train_loss 0.004038,Time used 0.002975s\n",
      "batch 4432, train_loss 0.004112,Time used 0.002976s\n",
      "batch 4433, train_loss 0.004142,Time used 0.002975s\n",
      "batch 4434, train_loss 0.004340,Time used 0.002976s\n",
      "batch 4435, train_loss 0.003590,Time used 0.002976s\n",
      "batch 4436, train_loss 0.004503,Time used 0.002480s\n",
      "batch 4437, train_loss 0.004309,Time used 0.002976s\n",
      "batch 4438, train_loss 0.003182,Time used 0.003472s\n",
      "batch 4439, train_loss 0.003256,Time used 0.003968s\n",
      "batch 4440, train_loss 0.004191,Time used 0.014385s\n",
      "batch 4441, train_loss 0.003573,Time used 0.002976s\n",
      "batch 4442, train_loss 0.004207,Time used 0.002976s\n",
      "batch 4443, train_loss 0.004160,Time used 0.002480s\n",
      "batch 4444, train_loss 0.003194,Time used 0.002483s\n",
      "batch 4445, train_loss 0.003495,Time used 0.002976s\n",
      "batch 4446, train_loss 0.003645,Time used 0.002977s\n",
      "batch 4447, train_loss 0.004518,Time used 0.003472s\n",
      "batch 4448, train_loss 0.003766,Time used 0.002976s\n",
      "batch 4449, train_loss 0.005182,Time used 0.002976s\n",
      "batch 4450, train_loss 0.003359,Time used 0.002976s\n",
      "batch 4451, train_loss 0.004672,Time used 0.002480s\n",
      "batch 4452, train_loss 0.004126,Time used 0.002976s\n",
      "batch 4453, train_loss 0.003722,Time used 0.002976s\n",
      "batch 4454, train_loss 0.003340,Time used 0.002976s\n",
      "batch 4455, train_loss 0.004526,Time used 0.002973s\n",
      "batch 4456, train_loss 0.003846,Time used 0.002977s\n",
      "batch 4457, train_loss 0.004804,Time used 0.002976s\n",
      "batch 4458, train_loss 0.003402,Time used 0.002976s\n",
      "batch 4459, train_loss 0.003578,Time used 0.002976s\n",
      "batch 4460, train_loss 0.004115,Time used 0.002972s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4461, train_loss 0.004073,Time used 0.002976s\n",
      "batch 4462, train_loss 0.003061,Time used 0.002976s\n",
      "batch 4463, train_loss 0.003312,Time used 0.002976s\n",
      "batch 4464, train_loss 0.002957,Time used 0.003472s\n",
      "batch 4465, train_loss 0.003623,Time used 0.002976s\n",
      "batch 4466, train_loss 0.003107,Time used 0.002976s\n",
      "batch 4467, train_loss 0.003712,Time used 0.002976s\n",
      "batch 4468, train_loss 0.003652,Time used 0.002976s\n",
      "batch 4469, train_loss 0.003615,Time used 0.002975s\n",
      "batch 4470, train_loss 0.003729,Time used 0.003472s\n",
      "batch 4471, train_loss 0.003274,Time used 0.002976s\n",
      "batch 4472, train_loss 0.003209,Time used 0.002977s\n",
      "batch 4473, train_loss 0.004472,Time used 0.002480s\n",
      "batch 4474, train_loss 0.004349,Time used 0.002976s\n",
      "batch 4475, train_loss 0.003768,Time used 0.002976s\n",
      "batch 4476, train_loss 0.004357,Time used 0.002977s\n",
      "batch 4477, train_loss 0.003905,Time used 0.002976s\n",
      "batch 4478, train_loss 0.003688,Time used 0.002976s\n",
      "batch 4479, train_loss 0.002681,Time used 0.002976s\n",
      "batch 4480, train_loss 0.003967,Time used 0.002479s\n",
      "batch 4481, train_loss 0.003222,Time used 0.002489s\n",
      "batch 4482, train_loss 0.003301,Time used 0.002976s\n",
      "batch 4483, train_loss 0.003589,Time used 0.002972s\n",
      "batch 4484, train_loss 0.004276,Time used 0.002976s\n",
      "batch 4485, train_loss 0.003114,Time used 0.002976s\n",
      "batch 4486, train_loss 0.003405,Time used 0.003472s\n",
      "batch 4487, train_loss 0.003566,Time used 0.003968s\n",
      "batch 4488, train_loss 0.004286,Time used 0.011408s\n",
      "batch 4489, train_loss 0.003161,Time used 0.002976s\n",
      "batch 4490, train_loss 0.003723,Time used 0.002977s\n",
      "batch 4491, train_loss 0.003027,Time used 0.002976s\n",
      "batch 4492, train_loss 0.003454,Time used 0.002976s\n",
      "batch 4493, train_loss 0.005149,Time used 0.002976s\n",
      "batch 4494, train_loss 0.003546,Time used 0.002975s\n",
      "batch 4495, train_loss 0.003538,Time used 0.002975s\n",
      "batch 4496, train_loss 0.003162,Time used 0.002976s\n",
      "batch 4497, train_loss 0.003888,Time used 0.002976s\n",
      "batch 4498, train_loss 0.004427,Time used 0.002976s\n",
      "batch 4499, train_loss 0.003346,Time used 0.003968s\n",
      "batch 4500, train_loss 0.002950,Time used 0.002480s\n",
      "***************************test_batch 4500, test_rmse_loss 0.062067,test_mae_loss 0.044870,test_mape_loss 14.181010,Time used 0.011408s\n",
      "batch 4501, train_loss 0.003673,Time used 0.003464s\n",
      "batch 4502, train_loss 0.003285,Time used 0.002975s\n",
      "batch 4503, train_loss 0.003355,Time used 0.002970s\n",
      "batch 4504, train_loss 0.003537,Time used 0.003470s\n",
      "batch 4505, train_loss 0.004980,Time used 0.003473s\n",
      "batch 4506, train_loss 0.003760,Time used 0.003472s\n",
      "batch 4507, train_loss 0.003992,Time used 0.002976s\n",
      "batch 4508, train_loss 0.004159,Time used 0.002977s\n",
      "batch 4509, train_loss 0.005529,Time used 0.002976s\n",
      "batch 4510, train_loss 0.003389,Time used 0.002477s\n",
      "batch 4511, train_loss 0.003655,Time used 0.002480s\n",
      "batch 4512, train_loss 0.003618,Time used 0.002480s\n",
      "batch 4513, train_loss 0.003781,Time used 0.003472s\n",
      "batch 4514, train_loss 0.005047,Time used 0.002481s\n",
      "batch 4515, train_loss 0.003747,Time used 0.002975s\n",
      "batch 4516, train_loss 0.003376,Time used 0.002976s\n",
      "batch 4517, train_loss 0.005245,Time used 0.002976s\n",
      "batch 4518, train_loss 0.004243,Time used 0.002976s\n",
      "batch 4519, train_loss 0.004745,Time used 0.002976s\n",
      "batch 4520, train_loss 0.002843,Time used 0.002472s\n",
      "batch 4521, train_loss 0.003502,Time used 0.002976s\n",
      "batch 4522, train_loss 0.003792,Time used 0.002976s\n",
      "batch 4523, train_loss 0.004166,Time used 0.002480s\n",
      "batch 4524, train_loss 0.003588,Time used 0.002480s\n",
      "batch 4525, train_loss 0.003867,Time used 0.002975s\n",
      "batch 4526, train_loss 0.003929,Time used 0.002976s\n",
      "batch 4527, train_loss 0.003636,Time used 0.002976s\n",
      "batch 4528, train_loss 0.003589,Time used 0.002975s\n",
      "batch 4529, train_loss 0.002828,Time used 0.002480s\n",
      "batch 4530, train_loss 0.002912,Time used 0.002975s\n",
      "batch 4531, train_loss 0.003612,Time used 0.002976s\n",
      "batch 4532, train_loss 0.003448,Time used 0.002985s\n",
      "batch 4533, train_loss 0.004119,Time used 0.002976s\n",
      "batch 4534, train_loss 0.004653,Time used 0.002480s\n",
      "batch 4535, train_loss 0.003705,Time used 0.002479s\n",
      "batch 4536, train_loss 0.005447,Time used 0.002971s\n",
      "batch 4537, train_loss 0.003803,Time used 0.002976s\n",
      "batch 4538, train_loss 0.003981,Time used 0.003472s\n",
      "batch 4539, train_loss 0.003041,Time used 0.003468s\n",
      "batch 4540, train_loss 0.004517,Time used 0.002976s\n",
      "batch 4541, train_loss 0.003662,Time used 0.002985s\n",
      "batch 4542, train_loss 0.004149,Time used 0.002976s\n",
      "batch 4543, train_loss 0.004171,Time used 0.004456s\n",
      "batch 4544, train_loss 0.003528,Time used 0.003968s\n",
      "batch 4545, train_loss 0.003480,Time used 0.003476s\n",
      "batch 4546, train_loss 0.003619,Time used 0.003472s\n",
      "batch 4547, train_loss 0.003828,Time used 0.003472s\n",
      "batch 4548, train_loss 0.004193,Time used 0.003968s\n",
      "batch 4549, train_loss 0.004194,Time used 0.002976s\n",
      "batch 4550, train_loss 0.004371,Time used 0.002976s\n",
      "batch 4551, train_loss 0.003580,Time used 0.002976s\n",
      "batch 4552, train_loss 0.003934,Time used 0.002976s\n",
      "batch 4553, train_loss 0.003197,Time used 0.003472s\n",
      "batch 4554, train_loss 0.003582,Time used 0.002480s\n",
      "batch 4555, train_loss 0.003519,Time used 0.002976s\n",
      "batch 4556, train_loss 0.003753,Time used 0.002480s\n",
      "batch 4557, train_loss 0.004270,Time used 0.002976s\n",
      "batch 4558, train_loss 0.003678,Time used 0.002976s\n",
      "batch 4559, train_loss 0.003077,Time used 0.002976s\n",
      "batch 4560, train_loss 0.004653,Time used 0.002976s\n",
      "batch 4561, train_loss 0.003351,Time used 0.002976s\n",
      "batch 4562, train_loss 0.004929,Time used 0.002985s\n",
      "batch 4563, train_loss 0.004056,Time used 0.003461s\n",
      "batch 4564, train_loss 0.003328,Time used 0.002479s\n",
      "batch 4565, train_loss 0.003227,Time used 0.002975s\n",
      "batch 4566, train_loss 0.002834,Time used 0.003472s\n",
      "batch 4567, train_loss 0.003826,Time used 0.002976s\n",
      "batch 4568, train_loss 0.003497,Time used 0.003468s\n",
      "batch 4569, train_loss 0.003877,Time used 0.002976s\n",
      "batch 4570, train_loss 0.003447,Time used 0.002976s\n",
      "batch 4571, train_loss 0.003216,Time used 0.003472s\n",
      "batch 4572, train_loss 0.004072,Time used 0.003472s\n",
      "batch 4573, train_loss 0.004217,Time used 0.002976s\n",
      "batch 4574, train_loss 0.003519,Time used 0.003968s\n",
      "batch 4575, train_loss 0.003904,Time used 0.002976s\n",
      "batch 4576, train_loss 0.004409,Time used 0.003473s\n",
      "batch 4577, train_loss 0.004658,Time used 0.014380s\n",
      "batch 4578, train_loss 0.003463,Time used 0.002970s\n",
      "batch 4579, train_loss 0.004234,Time used 0.002481s\n",
      "batch 4580, train_loss 0.003845,Time used 0.002976s\n",
      "batch 4581, train_loss 0.003674,Time used 0.002976s\n",
      "batch 4582, train_loss 0.004431,Time used 0.002973s\n",
      "batch 4583, train_loss 0.003344,Time used 0.002976s\n",
      "batch 4584, train_loss 0.003701,Time used 0.002975s\n",
      "batch 4585, train_loss 0.002920,Time used 0.003472s\n",
      "batch 4586, train_loss 0.003663,Time used 0.002977s\n",
      "batch 4587, train_loss 0.004078,Time used 0.002975s\n",
      "batch 4588, train_loss 0.004971,Time used 0.002976s\n",
      "batch 4589, train_loss 0.003147,Time used 0.002479s\n",
      "batch 4590, train_loss 0.004136,Time used 0.003471s\n",
      "batch 4591, train_loss 0.003766,Time used 0.002976s\n",
      "batch 4592, train_loss 0.003648,Time used 0.003473s\n",
      "batch 4593, train_loss 0.003632,Time used 0.002976s\n",
      "batch 4594, train_loss 0.003546,Time used 0.002976s\n",
      "batch 4595, train_loss 0.002880,Time used 0.002976s\n",
      "batch 4596, train_loss 0.003774,Time used 0.002973s\n",
      "batch 4597, train_loss 0.003094,Time used 0.002480s\n",
      "batch 4598, train_loss 0.003888,Time used 0.002976s\n",
      "batch 4599, train_loss 0.004047,Time used 0.002967s\n",
      "batch 4600, train_loss 0.004050,Time used 0.002976s\n",
      "***************************test_batch 4600, test_rmse_loss 0.061746,test_mae_loss 0.044326,test_mape_loss 13.262924,Time used 0.011904s\n",
      "batch 4601, train_loss 0.003538,Time used 0.003472s\n",
      "batch 4602, train_loss 0.003240,Time used 0.002973s\n",
      "batch 4603, train_loss 0.003685,Time used 0.002976s\n",
      "batch 4604, train_loss 0.003901,Time used 0.002975s\n",
      "batch 4605, train_loss 0.003592,Time used 0.002479s\n",
      "batch 4606, train_loss 0.003903,Time used 0.002976s\n",
      "batch 4607, train_loss 0.003554,Time used 0.002475s\n",
      "batch 4608, train_loss 0.004828,Time used 0.002976s\n",
      "batch 4609, train_loss 0.003550,Time used 0.002975s\n",
      "batch 4610, train_loss 0.003542,Time used 0.002975s\n",
      "batch 4611, train_loss 0.004285,Time used 0.002476s\n",
      "batch 4612, train_loss 0.003162,Time used 0.002477s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4613, train_loss 0.003522,Time used 0.002977s\n",
      "batch 4614, train_loss 0.002860,Time used 0.002976s\n",
      "batch 4615, train_loss 0.003157,Time used 0.002976s\n",
      "batch 4616, train_loss 0.003979,Time used 0.002976s\n",
      "batch 4617, train_loss 0.003933,Time used 0.002976s\n",
      "batch 4618, train_loss 0.003604,Time used 0.002976s\n",
      "batch 4619, train_loss 0.003827,Time used 0.003472s\n",
      "batch 4620, train_loss 0.004083,Time used 0.002479s\n",
      "batch 4621, train_loss 0.006660,Time used 0.002976s\n",
      "batch 4622, train_loss 0.004144,Time used 0.002974s\n",
      "batch 4623, train_loss 0.003722,Time used 0.002975s\n",
      "batch 4624, train_loss 0.005026,Time used 0.002976s\n",
      "batch 4625, train_loss 0.003541,Time used 0.002975s\n",
      "batch 4626, train_loss 0.003594,Time used 0.002972s\n",
      "batch 4627, train_loss 0.002969,Time used 0.003470s\n",
      "batch 4628, train_loss 0.004325,Time used 0.002972s\n",
      "batch 4629, train_loss 0.004212,Time used 0.002976s\n",
      "batch 4630, train_loss 0.003996,Time used 0.003473s\n",
      "batch 4631, train_loss 0.004125,Time used 0.003472s\n",
      "batch 4632, train_loss 0.003322,Time used 0.002976s\n",
      "batch 4633, train_loss 0.003873,Time used 0.003968s\n",
      "batch 4634, train_loss 0.003948,Time used 0.002976s\n",
      "batch 4635, train_loss 0.004826,Time used 0.003472s\n",
      "batch 4636, train_loss 0.003755,Time used 0.003472s\n",
      "batch 4637, train_loss 0.003436,Time used 0.002972s\n",
      "batch 4638, train_loss 0.004347,Time used 0.002976s\n",
      "batch 4639, train_loss 0.004100,Time used 0.002976s\n",
      "batch 4640, train_loss 0.002834,Time used 0.002480s\n",
      "batch 4641, train_loss 0.003521,Time used 0.003472s\n",
      "batch 4642, train_loss 0.003074,Time used 0.002970s\n",
      "batch 4643, train_loss 0.002889,Time used 0.002976s\n",
      "batch 4644, train_loss 0.003696,Time used 0.002976s\n",
      "batch 4645, train_loss 0.004155,Time used 0.002479s\n",
      "batch 4646, train_loss 0.003229,Time used 0.003472s\n",
      "batch 4647, train_loss 0.004548,Time used 0.002976s\n",
      "batch 4648, train_loss 0.003293,Time used 0.002977s\n",
      "batch 4649, train_loss 0.003694,Time used 0.003473s\n",
      "batch 4650, train_loss 0.003760,Time used 0.002976s\n",
      "batch 4651, train_loss 0.005319,Time used 0.002976s\n",
      "batch 4652, train_loss 0.003361,Time used 0.002480s\n",
      "batch 4653, train_loss 0.003435,Time used 0.003472s\n",
      "batch 4654, train_loss 0.003953,Time used 0.002481s\n",
      "batch 4655, train_loss 0.004413,Time used 0.002985s\n",
      "batch 4656, train_loss 0.004162,Time used 0.003467s\n",
      "batch 4657, train_loss 0.003997,Time used 0.002976s\n",
      "batch 4658, train_loss 0.003223,Time used 0.003472s\n",
      "batch 4659, train_loss 0.003427,Time used 0.003472s\n",
      "batch 4660, train_loss 0.003894,Time used 0.012400s\n",
      "batch 4661, train_loss 0.003655,Time used 0.002976s\n",
      "batch 4662, train_loss 0.003326,Time used 0.003472s\n",
      "batch 4663, train_loss 0.003765,Time used 0.002976s\n",
      "batch 4664, train_loss 0.004528,Time used 0.014384s\n",
      "batch 4665, train_loss 0.003339,Time used 0.003463s\n",
      "batch 4666, train_loss 0.004178,Time used 0.002976s\n",
      "batch 4667, train_loss 0.004745,Time used 0.002480s\n",
      "batch 4668, train_loss 0.004357,Time used 0.002976s\n",
      "batch 4669, train_loss 0.004097,Time used 0.002976s\n",
      "batch 4670, train_loss 0.004408,Time used 0.002976s\n",
      "batch 4671, train_loss 0.003181,Time used 0.002976s\n",
      "batch 4672, train_loss 0.003315,Time used 0.002972s\n",
      "batch 4673, train_loss 0.004613,Time used 0.002976s\n",
      "batch 4674, train_loss 0.002946,Time used 0.002980s\n",
      "batch 4675, train_loss 0.003807,Time used 0.002976s\n",
      "batch 4676, train_loss 0.003454,Time used 0.002976s\n",
      "batch 4677, train_loss 0.003432,Time used 0.003472s\n",
      "batch 4678, train_loss 0.003557,Time used 0.002975s\n",
      "batch 4679, train_loss 0.003995,Time used 0.002976s\n",
      "batch 4680, train_loss 0.003173,Time used 0.002480s\n",
      "batch 4681, train_loss 0.003321,Time used 0.002485s\n",
      "batch 4682, train_loss 0.003177,Time used 0.003472s\n",
      "batch 4683, train_loss 0.003108,Time used 0.002976s\n",
      "batch 4684, train_loss 0.003919,Time used 0.002979s\n",
      "batch 4685, train_loss 0.003868,Time used 0.002477s\n",
      "batch 4686, train_loss 0.003973,Time used 0.002976s\n",
      "batch 4687, train_loss 0.003202,Time used 0.002976s\n",
      "batch 4688, train_loss 0.003492,Time used 0.002976s\n",
      "batch 4689, train_loss 0.003675,Time used 0.003472s\n",
      "batch 4690, train_loss 0.003994,Time used 0.002976s\n",
      "batch 4691, train_loss 0.003854,Time used 0.002975s\n",
      "batch 4692, train_loss 0.003318,Time used 0.002976s\n",
      "batch 4693, train_loss 0.003665,Time used 0.002976s\n",
      "batch 4694, train_loss 0.004510,Time used 0.002985s\n",
      "batch 4695, train_loss 0.003636,Time used 0.002976s\n",
      "batch 4696, train_loss 0.003785,Time used 0.002480s\n",
      "batch 4697, train_loss 0.003830,Time used 0.002976s\n",
      "batch 4698, train_loss 0.002583,Time used 0.002480s\n",
      "batch 4699, train_loss 0.003359,Time used 0.002480s\n",
      "batch 4700, train_loss 0.004348,Time used 0.002976s\n",
      "***************************test_batch 4700, test_rmse_loss 0.061812,test_mae_loss 0.044481,test_mape_loss 13.656571,Time used 0.012897s\n",
      "batch 4701, train_loss 0.004028,Time used 0.002979s\n",
      "batch 4702, train_loss 0.003559,Time used 0.003472s\n",
      "batch 4703, train_loss 0.004665,Time used 0.003472s\n",
      "batch 4704, train_loss 0.003324,Time used 0.002976s\n",
      "batch 4705, train_loss 0.003566,Time used 0.002976s\n",
      "batch 4706, train_loss 0.003369,Time used 0.003471s\n",
      "batch 4707, train_loss 0.002879,Time used 0.002976s\n",
      "batch 4708, train_loss 0.003416,Time used 0.002976s\n",
      "batch 4709, train_loss 0.004163,Time used 0.002976s\n",
      "batch 4710, train_loss 0.004615,Time used 0.002976s\n",
      "batch 4711, train_loss 0.003007,Time used 0.003477s\n",
      "batch 4712, train_loss 0.003710,Time used 0.003472s\n",
      "batch 4713, train_loss 0.004114,Time used 0.002976s\n",
      "batch 4714, train_loss 0.003239,Time used 0.003472s\n",
      "batch 4715, train_loss 0.003466,Time used 0.002976s\n",
      "batch 4716, train_loss 0.004814,Time used 0.003473s\n",
      "batch 4717, train_loss 0.004022,Time used 0.003472s\n",
      "batch 4718, train_loss 0.003682,Time used 0.002976s\n",
      "batch 4719, train_loss 0.004080,Time used 0.002976s\n",
      "batch 4720, train_loss 0.004800,Time used 0.002981s\n",
      "batch 4721, train_loss 0.003791,Time used 0.002972s\n",
      "batch 4722, train_loss 0.003372,Time used 0.003472s\n",
      "batch 4723, train_loss 0.004252,Time used 0.002977s\n",
      "batch 4724, train_loss 0.003670,Time used 0.002976s\n",
      "batch 4725, train_loss 0.003402,Time used 0.002972s\n",
      "batch 4726, train_loss 0.003820,Time used 0.002976s\n",
      "batch 4727, train_loss 0.003618,Time used 0.002976s\n",
      "batch 4728, train_loss 0.003605,Time used 0.002990s\n",
      "batch 4729, train_loss 0.003792,Time used 0.002976s\n",
      "batch 4730, train_loss 0.003466,Time used 0.002976s\n",
      "batch 4731, train_loss 0.002790,Time used 0.002977s\n",
      "batch 4732, train_loss 0.004276,Time used 0.002977s\n",
      "batch 4733, train_loss 0.003617,Time used 0.002976s\n",
      "batch 4734, train_loss 0.003632,Time used 0.002486s\n",
      "batch 4735, train_loss 0.004810,Time used 0.002976s\n",
      "batch 4736, train_loss 0.002801,Time used 0.002480s\n",
      "batch 4737, train_loss 0.004108,Time used 0.002975s\n",
      "batch 4738, train_loss 0.005795,Time used 0.002481s\n",
      "batch 4739, train_loss 0.003497,Time used 0.002976s\n",
      "batch 4740, train_loss 0.003708,Time used 0.002480s\n",
      "batch 4741, train_loss 0.004138,Time used 0.003472s\n",
      "batch 4742, train_loss 0.003514,Time used 0.002976s\n",
      "batch 4743, train_loss 0.004255,Time used 0.003472s\n",
      "batch 4744, train_loss 0.003588,Time used 0.002976s\n",
      "batch 4745, train_loss 0.003838,Time used 0.003969s\n",
      "batch 4746, train_loss 0.002762,Time used 0.003471s\n",
      "batch 4747, train_loss 0.002915,Time used 0.002976s\n",
      "batch 4748, train_loss 0.003851,Time used 0.002489s\n",
      "batch 4749, train_loss 0.003211,Time used 0.003472s\n",
      "batch 4750, train_loss 0.004974,Time used 0.003472s\n",
      "batch 4751, train_loss 0.003366,Time used 0.003470s\n",
      "batch 4752, train_loss 0.003650,Time used 0.013888s\n",
      "batch 4753, train_loss 0.003112,Time used 0.002977s\n",
      "batch 4754, train_loss 0.004435,Time used 0.003472s\n",
      "batch 4755, train_loss 0.003482,Time used 0.003471s\n",
      "batch 4756, train_loss 0.003204,Time used 0.003472s\n",
      "batch 4757, train_loss 0.003197,Time used 0.002976s\n",
      "batch 4758, train_loss 0.004091,Time used 0.002976s\n",
      "batch 4759, train_loss 0.003675,Time used 0.002976s\n",
      "batch 4760, train_loss 0.004242,Time used 0.002976s\n",
      "batch 4761, train_loss 0.004058,Time used 0.002976s\n",
      "batch 4762, train_loss 0.003521,Time used 0.002976s\n",
      "batch 4763, train_loss 0.003514,Time used 0.002975s\n",
      "batch 4764, train_loss 0.003842,Time used 0.004464s\n",
      "batch 4765, train_loss 0.004754,Time used 0.003472s\n",
      "batch 4766, train_loss 0.004257,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4767, train_loss 0.004119,Time used 0.002976s\n",
      "batch 4768, train_loss 0.005372,Time used 0.004464s\n",
      "batch 4769, train_loss 0.003608,Time used 0.003472s\n",
      "batch 4770, train_loss 0.003872,Time used 0.003472s\n",
      "batch 4771, train_loss 0.003260,Time used 0.003471s\n",
      "batch 4772, train_loss 0.004316,Time used 0.002976s\n",
      "batch 4773, train_loss 0.003870,Time used 0.003472s\n",
      "batch 4774, train_loss 0.003581,Time used 0.002976s\n",
      "batch 4775, train_loss 0.003736,Time used 0.003472s\n",
      "batch 4776, train_loss 0.004826,Time used 0.003472s\n",
      "batch 4777, train_loss 0.003776,Time used 0.003472s\n",
      "batch 4778, train_loss 0.005249,Time used 0.003472s\n",
      "batch 4779, train_loss 0.003703,Time used 0.003472s\n",
      "batch 4780, train_loss 0.003507,Time used 0.002977s\n",
      "batch 4781, train_loss 0.003163,Time used 0.003472s\n",
      "batch 4782, train_loss 0.004498,Time used 0.003472s\n",
      "batch 4783, train_loss 0.003028,Time used 0.003472s\n",
      "batch 4784, train_loss 0.003403,Time used 0.002976s\n",
      "batch 4785, train_loss 0.003013,Time used 0.002976s\n",
      "batch 4786, train_loss 0.003629,Time used 0.002976s\n",
      "batch 4787, train_loss 0.004014,Time used 0.002976s\n",
      "batch 4788, train_loss 0.003632,Time used 0.003472s\n",
      "batch 4789, train_loss 0.003999,Time used 0.002977s\n",
      "batch 4790, train_loss 0.003094,Time used 0.003968s\n",
      "batch 4791, train_loss 0.003682,Time used 0.003472s\n",
      "batch 4792, train_loss 0.003379,Time used 0.003475s\n",
      "batch 4793, train_loss 0.003501,Time used 0.002975s\n",
      "batch 4794, train_loss 0.003743,Time used 0.002977s\n",
      "batch 4795, train_loss 0.003233,Time used 0.002967s\n",
      "batch 4796, train_loss 0.004041,Time used 0.002975s\n",
      "batch 4797, train_loss 0.003275,Time used 0.002977s\n",
      "batch 4798, train_loss 0.003383,Time used 0.003968s\n",
      "batch 4799, train_loss 0.003249,Time used 0.003470s\n",
      "batch 4800, train_loss 0.004063,Time used 0.003967s\n",
      "***************************test_batch 4800, test_rmse_loss 0.061647,test_mae_loss 0.044267,test_mape_loss 13.336578,Time used 0.011408s\n",
      "batch 4801, train_loss 0.003945,Time used 0.003472s\n",
      "batch 4802, train_loss 0.003257,Time used 0.003472s\n",
      "batch 4803, train_loss 0.003719,Time used 0.003466s\n",
      "batch 4804, train_loss 0.004955,Time used 0.003473s\n",
      "batch 4805, train_loss 0.002911,Time used 0.002976s\n",
      "batch 4806, train_loss 0.003489,Time used 0.002976s\n",
      "batch 4807, train_loss 0.003347,Time used 0.002480s\n",
      "batch 4808, train_loss 0.003532,Time used 0.002975s\n",
      "batch 4809, train_loss 0.004353,Time used 0.003472s\n",
      "batch 4810, train_loss 0.004056,Time used 0.003471s\n",
      "batch 4811, train_loss 0.003907,Time used 0.003472s\n",
      "batch 4812, train_loss 0.003393,Time used 0.002976s\n",
      "batch 4813, train_loss 0.003610,Time used 0.002976s\n",
      "batch 4814, train_loss 0.004287,Time used 0.003472s\n",
      "batch 4815, train_loss 0.003889,Time used 0.003472s\n",
      "batch 4816, train_loss 0.004157,Time used 0.003472s\n",
      "batch 4817, train_loss 0.003012,Time used 0.002976s\n",
      "batch 4818, train_loss 0.003342,Time used 0.002975s\n",
      "batch 4819, train_loss 0.004298,Time used 0.002977s\n",
      "batch 4820, train_loss 0.003818,Time used 0.003472s\n",
      "batch 4821, train_loss 0.003826,Time used 0.002973s\n",
      "batch 4822, train_loss 0.003818,Time used 0.002968s\n",
      "batch 4823, train_loss 0.003094,Time used 0.002976s\n",
      "batch 4824, train_loss 0.003228,Time used 0.002974s\n",
      "batch 4825, train_loss 0.003067,Time used 0.003471s\n",
      "batch 4826, train_loss 0.003460,Time used 0.002975s\n",
      "batch 4827, train_loss 0.003525,Time used 0.002976s\n",
      "batch 4828, train_loss 0.003854,Time used 0.003477s\n",
      "batch 4829, train_loss 0.003828,Time used 0.002981s\n",
      "batch 4830, train_loss 0.003716,Time used 0.003471s\n",
      "batch 4831, train_loss 0.003744,Time used 0.002976s\n",
      "batch 4832, train_loss 0.003751,Time used 0.003968s\n",
      "batch 4833, train_loss 0.002923,Time used 0.012399s\n",
      "batch 4834, train_loss 0.004626,Time used 0.003472s\n",
      "batch 4835, train_loss 0.004386,Time used 0.002975s\n",
      "batch 4836, train_loss 0.004231,Time used 0.002480s\n",
      "batch 4837, train_loss 0.004100,Time used 0.003471s\n",
      "batch 4838, train_loss 0.003775,Time used 0.002976s\n",
      "batch 4839, train_loss 0.003657,Time used 0.002480s\n",
      "batch 4840, train_loss 0.004205,Time used 0.002976s\n",
      "batch 4841, train_loss 0.003339,Time used 0.003472s\n",
      "batch 4842, train_loss 0.003351,Time used 0.002976s\n",
      "batch 4843, train_loss 0.002886,Time used 0.002977s\n",
      "batch 4844, train_loss 0.003196,Time used 0.003473s\n",
      "batch 4845, train_loss 0.003853,Time used 0.002472s\n",
      "batch 4846, train_loss 0.004373,Time used 0.002976s\n",
      "batch 4847, train_loss 0.004030,Time used 0.002480s\n",
      "batch 4848, train_loss 0.004009,Time used 0.002480s\n",
      "batch 4849, train_loss 0.004505,Time used 0.002976s\n",
      "batch 4850, train_loss 0.003427,Time used 0.002968s\n",
      "batch 4851, train_loss 0.003941,Time used 0.002976s\n",
      "batch 4852, train_loss 0.003268,Time used 0.002974s\n",
      "batch 4853, train_loss 0.004527,Time used 0.002980s\n",
      "batch 4854, train_loss 0.004383,Time used 0.002479s\n",
      "batch 4855, train_loss 0.004190,Time used 0.002976s\n",
      "batch 4856, train_loss 0.004438,Time used 0.003474s\n",
      "batch 4857, train_loss 0.003383,Time used 0.002975s\n",
      "batch 4858, train_loss 0.003366,Time used 0.002976s\n",
      "batch 4859, train_loss 0.003216,Time used 0.002976s\n",
      "batch 4860, train_loss 0.010824,Time used 0.002982s\n",
      "batch 4861, train_loss 0.004535,Time used 0.002976s\n",
      "batch 4862, train_loss 0.003047,Time used 0.002975s\n",
      "batch 4863, train_loss 0.004131,Time used 0.002976s\n",
      "batch 4864, train_loss 0.005142,Time used 0.002975s\n",
      "batch 4865, train_loss 0.004278,Time used 0.002976s\n",
      "batch 4866, train_loss 0.004883,Time used 0.002976s\n",
      "batch 4867, train_loss 0.004470,Time used 0.003467s\n",
      "batch 4868, train_loss 0.004369,Time used 0.002976s\n",
      "batch 4869, train_loss 0.003973,Time used 0.003477s\n",
      "batch 4870, train_loss 0.003559,Time used 0.003471s\n",
      "batch 4871, train_loss 0.003376,Time used 0.002976s\n",
      "batch 4872, train_loss 0.003388,Time used 0.002976s\n",
      "batch 4873, train_loss 0.002914,Time used 0.013392s\n",
      "batch 4874, train_loss 0.003181,Time used 0.003472s\n",
      "batch 4875, train_loss 0.004032,Time used 0.003472s\n",
      "batch 4876, train_loss 0.003782,Time used 0.003472s\n",
      "batch 4877, train_loss 0.003150,Time used 0.002976s\n",
      "batch 4878, train_loss 0.003300,Time used 0.002973s\n",
      "batch 4879, train_loss 0.002841,Time used 0.003472s\n",
      "batch 4880, train_loss 0.003477,Time used 0.002976s\n",
      "batch 4881, train_loss 0.003136,Time used 0.003472s\n",
      "batch 4882, train_loss 0.004148,Time used 0.002976s\n",
      "batch 4883, train_loss 0.003762,Time used 0.002976s\n",
      "batch 4884, train_loss 0.004086,Time used 0.002976s\n",
      "batch 4885, train_loss 0.003286,Time used 0.003968s\n",
      "batch 4886, train_loss 0.004406,Time used 0.002970s\n",
      "batch 4887, train_loss 0.003440,Time used 0.003472s\n",
      "batch 4888, train_loss 0.004166,Time used 0.003467s\n",
      "batch 4889, train_loss 0.003904,Time used 0.003472s\n",
      "batch 4890, train_loss 0.004934,Time used 0.002975s\n",
      "batch 4891, train_loss 0.004604,Time used 0.003473s\n",
      "batch 4892, train_loss 0.003263,Time used 0.002975s\n",
      "batch 4893, train_loss 0.003443,Time used 0.003473s\n",
      "batch 4894, train_loss 0.003195,Time used 0.003475s\n",
      "batch 4895, train_loss 0.003719,Time used 0.003473s\n",
      "batch 4896, train_loss 0.004045,Time used 0.002975s\n",
      "batch 4897, train_loss 0.004080,Time used 0.003968s\n",
      "batch 4898, train_loss 0.004153,Time used 0.003472s\n",
      "batch 4899, train_loss 0.003292,Time used 0.003472s\n",
      "batch 4900, train_loss 0.003313,Time used 0.002480s\n",
      "***************************test_batch 4900, test_rmse_loss 0.061580,test_mae_loss 0.044240,test_mape_loss 13.387536,Time used 0.013393s\n",
      "batch 4901, train_loss 0.003633,Time used 0.003967s\n",
      "batch 4902, train_loss 0.003248,Time used 0.003473s\n",
      "batch 4903, train_loss 0.003916,Time used 0.002976s\n",
      "batch 4904, train_loss 0.003136,Time used 0.002976s\n",
      "batch 4905, train_loss 0.004194,Time used 0.002971s\n",
      "batch 4906, train_loss 0.003151,Time used 0.003472s\n",
      "batch 4907, train_loss 0.004733,Time used 0.003472s\n",
      "batch 4908, train_loss 0.003441,Time used 0.002976s\n",
      "batch 4909, train_loss 0.003553,Time used 0.002975s\n",
      "batch 4910, train_loss 0.003544,Time used 0.003969s\n",
      "batch 4911, train_loss 0.003208,Time used 0.003472s\n",
      "batch 4912, train_loss 0.004161,Time used 0.002975s\n",
      "batch 4913, train_loss 0.004152,Time used 0.013392s\n",
      "batch 4914, train_loss 0.001489,Time used 0.002975s\n",
      "batch 4915, train_loss 0.004365,Time used 0.003472s\n",
      "batch 4916, train_loss 0.004366,Time used 0.002976s\n",
      "batch 4917, train_loss 0.003638,Time used 0.003472s\n",
      "batch 4918, train_loss 0.003553,Time used 0.003472s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4919, train_loss 0.003872,Time used 0.003465s\n",
      "batch 4920, train_loss 0.003363,Time used 0.002976s\n",
      "batch 4921, train_loss 0.003119,Time used 0.002975s\n",
      "batch 4922, train_loss 0.003743,Time used 0.003472s\n",
      "batch 4923, train_loss 0.003879,Time used 0.003472s\n",
      "batch 4924, train_loss 0.004789,Time used 0.002976s\n",
      "batch 4925, train_loss 0.003781,Time used 0.002976s\n",
      "batch 4926, train_loss 0.003399,Time used 0.003472s\n",
      "batch 4927, train_loss 0.003279,Time used 0.003473s\n",
      "batch 4928, train_loss 0.003315,Time used 0.002976s\n",
      "batch 4929, train_loss 0.004106,Time used 0.002480s\n",
      "batch 4930, train_loss 0.002718,Time used 0.002986s\n",
      "batch 4931, train_loss 0.004458,Time used 0.003472s\n",
      "batch 4932, train_loss 0.003794,Time used 0.002976s\n",
      "batch 4933, train_loss 0.003216,Time used 0.003476s\n",
      "batch 4934, train_loss 0.002926,Time used 0.002976s\n",
      "batch 4935, train_loss 0.003108,Time used 0.003475s\n",
      "batch 4936, train_loss 0.003917,Time used 0.002481s\n",
      "batch 4937, train_loss 0.003626,Time used 0.003472s\n",
      "batch 4938, train_loss 0.003607,Time used 0.003472s\n",
      "batch 4939, train_loss 0.003466,Time used 0.002976s\n",
      "batch 4940, train_loss 0.003613,Time used 0.002976s\n",
      "batch 4941, train_loss 0.003826,Time used 0.002976s\n",
      "batch 4942, train_loss 0.003869,Time used 0.002976s\n",
      "batch 4943, train_loss 0.004564,Time used 0.002976s\n",
      "batch 4944, train_loss 0.003996,Time used 0.003472s\n",
      "batch 4945, train_loss 0.004453,Time used 0.002963s\n",
      "batch 4946, train_loss 0.004168,Time used 0.002976s\n",
      "batch 4947, train_loss 0.004276,Time used 0.002976s\n",
      "batch 4948, train_loss 0.003958,Time used 0.002976s\n",
      "batch 4949, train_loss 0.003943,Time used 0.003472s\n",
      "batch 4950, train_loss 0.004628,Time used 0.002976s\n",
      "batch 4951, train_loss 0.003432,Time used 0.002982s\n",
      "batch 4952, train_loss 0.003330,Time used 0.002975s\n",
      "batch 4953, train_loss 0.003376,Time used 0.003472s\n",
      "batch 4954, train_loss 0.003412,Time used 0.002977s\n",
      "batch 4955, train_loss 0.003130,Time used 0.002976s\n",
      "batch 4956, train_loss 0.002515,Time used 0.003466s\n",
      "batch 4957, train_loss 0.003593,Time used 0.002970s\n",
      "batch 4958, train_loss 0.004053,Time used 0.002476s\n",
      "batch 4959, train_loss 0.003791,Time used 0.002976s\n",
      "batch 4960, train_loss 0.004013,Time used 0.002977s\n",
      "batch 4961, train_loss 0.003067,Time used 0.002980s\n",
      "batch 4962, train_loss 0.003549,Time used 0.002975s\n",
      "batch 4963, train_loss 0.004359,Time used 0.002967s\n",
      "batch 4964, train_loss 0.004131,Time used 0.002971s\n",
      "batch 4965, train_loss 0.004514,Time used 0.002969s\n",
      "batch 4966, train_loss 0.004663,Time used 0.003472s\n",
      "batch 4967, train_loss 0.003698,Time used 0.003471s\n",
      "batch 4968, train_loss 0.005099,Time used 0.002976s\n",
      "batch 4969, train_loss 0.002756,Time used 0.002480s\n",
      "batch 4970, train_loss 0.003532,Time used 0.003472s\n",
      "batch 4971, train_loss 0.004376,Time used 0.002990s\n",
      "batch 4972, train_loss 0.004320,Time used 0.002976s\n",
      "batch 4973, train_loss 0.003121,Time used 0.003469s\n",
      "batch 4974, train_loss 0.003552,Time used 0.003472s\n",
      "batch 4975, train_loss 0.004105,Time used 0.002976s\n",
      "batch 4976, train_loss 0.003565,Time used 0.003472s\n",
      "batch 4977, train_loss 0.004253,Time used 0.002973s\n",
      "batch 4978, train_loss 0.003211,Time used 0.002976s\n",
      "batch 4979, train_loss 0.003534,Time used 0.003472s\n",
      "batch 4980, train_loss 0.004378,Time used 0.002976s\n",
      "batch 4981, train_loss 0.004205,Time used 0.002976s\n",
      "batch 4982, train_loss 0.003986,Time used 0.002976s\n",
      "batch 4983, train_loss 0.003912,Time used 0.002489s\n",
      "batch 4984, train_loss 0.004128,Time used 0.002976s\n",
      "batch 4985, train_loss 0.003495,Time used 0.002976s\n",
      "batch 4986, train_loss 0.004424,Time used 0.002976s\n",
      "batch 4987, train_loss 0.003836,Time used 0.002976s\n",
      "batch 4988, train_loss 0.004224,Time used 0.002976s\n",
      "batch 4989, train_loss 0.003094,Time used 0.002480s\n",
      "batch 4990, train_loss 0.003454,Time used 0.002975s\n",
      "batch 4991, train_loss 0.004549,Time used 0.002976s\n",
      "batch 4992, train_loss 0.003585,Time used 0.002976s\n",
      "batch 4993, train_loss 0.003739,Time used 0.002977s\n",
      "batch 4994, train_loss 0.003903,Time used 0.002973s\n",
      "batch 4995, train_loss 0.003581,Time used 0.003468s\n",
      "batch 4996, train_loss 0.004000,Time used 0.002976s\n",
      "batch 4997, train_loss 0.004049,Time used 0.003471s\n",
      "batch 4998, train_loss 0.003265,Time used 0.002977s\n",
      "batch 4999, train_loss 0.005948,Time used 0.002976s\n",
      "batch 5000, train_loss 0.003242,Time used 0.002975s\n",
      "***************************test_batch 5000, test_rmse_loss 0.061533,test_mae_loss 0.044175,test_mape_loss 13.306663,Time used 0.013392s\n",
      "batch 5001, train_loss 0.003575,Time used 0.011904s\n",
      "batch 5002, train_loss 0.004014,Time used 0.002976s\n",
      "batch 5003, train_loss 0.003760,Time used 0.002976s\n",
      "batch 5004, train_loss 0.004316,Time used 0.002975s\n",
      "batch 5005, train_loss 0.003550,Time used 0.002971s\n",
      "batch 5006, train_loss 0.003569,Time used 0.003472s\n",
      "batch 5007, train_loss 0.003663,Time used 0.002480s\n",
      "batch 5008, train_loss 0.003472,Time used 0.002976s\n",
      "batch 5009, train_loss 0.003125,Time used 0.003472s\n",
      "batch 5010, train_loss 0.005318,Time used 0.002968s\n",
      "batch 5011, train_loss 0.003689,Time used 0.003472s\n",
      "batch 5012, train_loss 0.003707,Time used 0.003472s\n",
      "batch 5013, train_loss 0.004018,Time used 0.002971s\n",
      "batch 5014, train_loss 0.003210,Time used 0.002480s\n",
      "batch 5015, train_loss 0.003072,Time used 0.002975s\n",
      "batch 5016, train_loss 0.003007,Time used 0.002480s\n",
      "batch 5017, train_loss 0.003578,Time used 0.002468s\n",
      "batch 5018, train_loss 0.003550,Time used 0.003472s\n",
      "batch 5019, train_loss 0.003567,Time used 0.002480s\n",
      "batch 5020, train_loss 0.002855,Time used 0.002985s\n",
      "batch 5021, train_loss 0.003778,Time used 0.002976s\n",
      "batch 5022, train_loss 0.008629,Time used 0.002480s\n",
      "batch 5023, train_loss 0.002829,Time used 0.002480s\n",
      "batch 5024, train_loss 0.003463,Time used 0.002976s\n",
      "batch 5025, train_loss 0.004177,Time used 0.003469s\n",
      "batch 5026, train_loss 0.003960,Time used 0.002976s\n",
      "batch 5027, train_loss 0.003771,Time used 0.002479s\n",
      "batch 5028, train_loss 0.003839,Time used 0.002976s\n",
      "batch 5029, train_loss 0.003865,Time used 0.002976s\n",
      "batch 5030, train_loss 0.003820,Time used 0.002976s\n",
      "batch 5031, train_loss 0.002693,Time used 0.002976s\n",
      "batch 5032, train_loss 0.003218,Time used 0.002976s\n",
      "batch 5033, train_loss 0.003834,Time used 0.002976s\n",
      "batch 5034, train_loss 0.004478,Time used 0.003968s\n",
      "batch 5035, train_loss 0.003758,Time used 0.002977s\n",
      "batch 5036, train_loss 0.003794,Time used 0.003472s\n",
      "batch 5037, train_loss 0.004503,Time used 0.003968s\n",
      "batch 5038, train_loss 0.003443,Time used 0.003972s\n",
      "batch 5039, train_loss 0.003294,Time used 0.003472s\n",
      "batch 5040, train_loss 0.003075,Time used 0.003968s\n",
      "batch 5041, train_loss 0.003720,Time used 0.002975s\n",
      "batch 5042, train_loss 0.003699,Time used 0.002975s\n",
      "batch 5043, train_loss 0.002790,Time used 0.002977s\n",
      "batch 5044, train_loss 0.003287,Time used 0.002480s\n",
      "batch 5045, train_loss 0.003718,Time used 0.002480s\n",
      "batch 5046, train_loss 0.003495,Time used 0.002976s\n",
      "batch 5047, train_loss 0.005053,Time used 0.002975s\n",
      "batch 5048, train_loss 0.004588,Time used 0.002480s\n",
      "batch 5049, train_loss 0.003472,Time used 0.002976s\n",
      "batch 5050, train_loss 0.004469,Time used 0.002966s\n",
      "batch 5051, train_loss 0.004207,Time used 0.002480s\n",
      "batch 5052, train_loss 0.003731,Time used 0.003471s\n",
      "batch 5053, train_loss 0.003584,Time used 0.002480s\n",
      "batch 5054, train_loss 0.004238,Time used 0.003473s\n",
      "batch 5055, train_loss 0.002690,Time used 0.003472s\n",
      "batch 5056, train_loss 0.004410,Time used 0.002985s\n",
      "batch 5057, train_loss 0.004556,Time used 0.012400s\n",
      "batch 5058, train_loss 0.003663,Time used 0.003464s\n",
      "batch 5059, train_loss 0.003555,Time used 0.002975s\n",
      "batch 5060, train_loss 0.003377,Time used 0.003472s\n",
      "batch 5061, train_loss 0.003025,Time used 0.002976s\n",
      "batch 5062, train_loss 0.004882,Time used 0.002983s\n",
      "batch 5063, train_loss 0.003841,Time used 0.003472s\n",
      "batch 5064, train_loss 0.003204,Time used 0.003472s\n",
      "batch 5065, train_loss 0.002803,Time used 0.002479s\n",
      "batch 5066, train_loss 0.004532,Time used 0.002977s\n",
      "batch 5067, train_loss 0.003637,Time used 0.002976s\n",
      "batch 5068, train_loss 0.003747,Time used 0.002977s\n",
      "batch 5069, train_loss 0.003710,Time used 0.003473s\n",
      "batch 5070, train_loss 0.003703,Time used 0.003967s\n",
      "batch 5071, train_loss 0.004951,Time used 0.003472s\n",
      "batch 5072, train_loss 0.002915,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5073, train_loss 0.003524,Time used 0.003968s\n",
      "batch 5074, train_loss 0.004994,Time used 0.003473s\n",
      "batch 5075, train_loss 0.003528,Time used 0.002976s\n",
      "batch 5076, train_loss 0.001125,Time used 0.013888s\n",
      "batch 5077, train_loss 0.004987,Time used 0.002981s\n",
      "batch 5078, train_loss 0.003412,Time used 0.003472s\n",
      "batch 5079, train_loss 0.005141,Time used 0.003472s\n",
      "batch 5080, train_loss 0.003571,Time used 0.002976s\n",
      "batch 5081, train_loss 0.003195,Time used 0.002971s\n",
      "batch 5082, train_loss 0.003733,Time used 0.002480s\n",
      "batch 5083, train_loss 0.004004,Time used 0.003473s\n",
      "batch 5084, train_loss 0.003820,Time used 0.003972s\n",
      "batch 5085, train_loss 0.003398,Time used 0.003473s\n",
      "batch 5086, train_loss 0.003101,Time used 0.003473s\n",
      "batch 5087, train_loss 0.004574,Time used 0.002977s\n",
      "batch 5088, train_loss 0.003704,Time used 0.012400s\n",
      "batch 5089, train_loss 0.004695,Time used 0.003472s\n",
      "batch 5090, train_loss 0.003248,Time used 0.002976s\n",
      "batch 5091, train_loss 0.003255,Time used 0.002976s\n",
      "batch 5092, train_loss 0.003898,Time used 0.002481s\n",
      "batch 5093, train_loss 0.003745,Time used 0.002976s\n",
      "batch 5094, train_loss 0.003282,Time used 0.002480s\n",
      "batch 5095, train_loss 0.006084,Time used 0.002976s\n",
      "batch 5096, train_loss 0.004363,Time used 0.002976s\n",
      "batch 5097, train_loss 0.003038,Time used 0.002976s\n",
      "batch 5098, train_loss 0.003837,Time used 0.003472s\n",
      "batch 5099, train_loss 0.003433,Time used 0.003472s\n",
      "batch 5100, train_loss 0.004038,Time used 0.002976s\n",
      "***************************test_batch 5100, test_rmse_loss 0.061488,test_mae_loss 0.044162,test_mape_loss 13.358483,Time used 0.013887s\n",
      "batch 5101, train_loss 0.003271,Time used 0.002975s\n",
      "batch 5102, train_loss 0.003528,Time used 0.002481s\n",
      "batch 5103, train_loss 0.003887,Time used 0.002480s\n",
      "batch 5104, train_loss 0.002441,Time used 0.002976s\n",
      "batch 5105, train_loss 0.003483,Time used 0.002976s\n",
      "batch 5106, train_loss 0.003486,Time used 0.002975s\n",
      "batch 5107, train_loss 0.003282,Time used 0.002976s\n",
      "batch 5108, train_loss 0.003423,Time used 0.002975s\n",
      "batch 5109, train_loss 0.003525,Time used 0.002480s\n",
      "batch 5110, train_loss 0.003639,Time used 0.002485s\n",
      "batch 5111, train_loss 0.003432,Time used 0.003472s\n",
      "batch 5112, train_loss 0.004162,Time used 0.003472s\n",
      "batch 5113, train_loss 0.003909,Time used 0.002976s\n",
      "batch 5114, train_loss 0.003160,Time used 0.002976s\n",
      "batch 5115, train_loss 0.003856,Time used 0.002974s\n",
      "batch 5116, train_loss 0.003328,Time used 0.002976s\n",
      "batch 5117, train_loss 0.003684,Time used 0.002976s\n",
      "batch 5118, train_loss 0.003608,Time used 0.002976s\n",
      "batch 5119, train_loss 0.003234,Time used 0.002975s\n",
      "batch 5120, train_loss 0.003628,Time used 0.002975s\n",
      "batch 5121, train_loss 0.003201,Time used 0.002480s\n",
      "batch 5122, train_loss 0.003792,Time used 0.003472s\n",
      "batch 5123, train_loss 0.005102,Time used 0.002977s\n",
      "batch 5124, train_loss 0.003631,Time used 0.002976s\n",
      "batch 5125, train_loss 0.004091,Time used 0.002976s\n",
      "batch 5126, train_loss 0.003529,Time used 0.002976s\n",
      "batch 5127, train_loss 0.003282,Time used 0.002481s\n",
      "batch 5128, train_loss 0.004064,Time used 0.002481s\n",
      "batch 5129, train_loss 0.003906,Time used 0.002976s\n",
      "batch 5130, train_loss 0.005293,Time used 0.003472s\n",
      "batch 5131, train_loss 0.003815,Time used 0.002479s\n",
      "batch 5132, train_loss 0.003426,Time used 0.002975s\n",
      "batch 5133, train_loss 0.003457,Time used 0.002976s\n",
      "batch 5134, train_loss 0.003523,Time used 0.002976s\n",
      "batch 5135, train_loss 0.003751,Time used 0.002977s\n",
      "batch 5136, train_loss 0.004747,Time used 0.002987s\n",
      "batch 5137, train_loss 0.003328,Time used 0.002976s\n",
      "batch 5138, train_loss 0.003690,Time used 0.002480s\n",
      "batch 5139, train_loss 0.003556,Time used 0.002976s\n",
      "batch 5140, train_loss 0.004681,Time used 0.002977s\n",
      "batch 5141, train_loss 0.003244,Time used 0.002976s\n",
      "batch 5142, train_loss 0.003734,Time used 0.002976s\n",
      "batch 5143, train_loss 0.004849,Time used 0.002972s\n",
      "batch 5144, train_loss 0.004033,Time used 0.002977s\n",
      "batch 5145, train_loss 0.003600,Time used 0.002975s\n",
      "batch 5146, train_loss 0.003965,Time used 0.002976s\n",
      "batch 5147, train_loss 0.003617,Time used 0.003472s\n",
      "batch 5148, train_loss 0.004399,Time used 0.002976s\n",
      "batch 5149, train_loss 0.003360,Time used 0.002482s\n",
      "batch 5150, train_loss 0.004584,Time used 0.002480s\n",
      "batch 5151, train_loss 0.003737,Time used 0.002976s\n",
      "batch 5152, train_loss 0.003601,Time used 0.003968s\n",
      "batch 5153, train_loss 0.002855,Time used 0.003472s\n",
      "batch 5154, train_loss 0.003108,Time used 0.002976s\n",
      "batch 5155, train_loss 0.003048,Time used 0.002976s\n",
      "batch 5156, train_loss 0.004152,Time used 0.002976s\n",
      "batch 5157, train_loss 0.003382,Time used 0.003472s\n",
      "batch 5158, train_loss 0.003597,Time used 0.003472s\n",
      "batch 5159, train_loss 0.004023,Time used 0.002976s\n",
      "batch 5160, train_loss 0.004350,Time used 0.002976s\n",
      "batch 5161, train_loss 0.003665,Time used 0.002480s\n",
      "batch 5162, train_loss 0.003303,Time used 0.002976s\n",
      "batch 5163, train_loss 0.003077,Time used 0.002480s\n",
      "batch 5164, train_loss 0.003498,Time used 0.002976s\n",
      "batch 5165, train_loss 0.003656,Time used 0.003472s\n",
      "batch 5166, train_loss 0.004647,Time used 0.002976s\n",
      "batch 5167, train_loss 0.003286,Time used 0.002978s\n",
      "batch 5168, train_loss 0.003925,Time used 0.002480s\n",
      "batch 5169, train_loss 0.003245,Time used 0.002976s\n",
      "batch 5170, train_loss 0.002946,Time used 0.002480s\n",
      "batch 5171, train_loss 0.003762,Time used 0.003472s\n",
      "batch 5172, train_loss 0.004041,Time used 0.002984s\n",
      "batch 5173, train_loss 0.003632,Time used 0.003969s\n",
      "batch 5174, train_loss 0.005098,Time used 0.003473s\n",
      "batch 5175, train_loss 0.003725,Time used 0.002968s\n",
      "batch 5176, train_loss 0.002469,Time used 0.013887s\n",
      "batch 5177, train_loss 0.004412,Time used 0.003472s\n",
      "batch 5178, train_loss 0.003782,Time used 0.003472s\n",
      "batch 5179, train_loss 0.004605,Time used 0.002990s\n",
      "batch 5180, train_loss 0.003499,Time used 0.002975s\n",
      "batch 5181, train_loss 0.004542,Time used 0.002976s\n",
      "batch 5182, train_loss 0.002730,Time used 0.002976s\n",
      "batch 5183, train_loss 0.003480,Time used 0.003469s\n",
      "batch 5184, train_loss 0.002904,Time used 0.003472s\n",
      "batch 5185, train_loss 0.003929,Time used 0.002976s\n",
      "batch 5186, train_loss 0.004111,Time used 0.002976s\n",
      "batch 5187, train_loss 0.002807,Time used 0.003472s\n",
      "batch 5188, train_loss 0.003422,Time used 0.002976s\n",
      "batch 5189, train_loss 0.003300,Time used 0.003472s\n",
      "batch 5190, train_loss 0.003783,Time used 0.003472s\n",
      "batch 5191, train_loss 0.004660,Time used 0.002973s\n",
      "batch 5192, train_loss 0.003667,Time used 0.003472s\n",
      "batch 5193, train_loss 0.003803,Time used 0.002976s\n",
      "batch 5194, train_loss 0.004415,Time used 0.002984s\n",
      "batch 5195, train_loss 0.003778,Time used 0.002976s\n",
      "batch 5196, train_loss 0.003455,Time used 0.002976s\n",
      "batch 5197, train_loss 0.003249,Time used 0.002976s\n",
      "batch 5198, train_loss 0.002946,Time used 0.002976s\n",
      "batch 5199, train_loss 0.004106,Time used 0.003472s\n",
      "batch 5200, train_loss 0.003780,Time used 0.002976s\n",
      "***************************test_batch 5200, test_rmse_loss 0.061994,test_mae_loss 0.044395,test_mape_loss 13.020655,Time used 0.010417s\n",
      "batch 5201, train_loss 0.004215,Time used 0.003472s\n",
      "batch 5202, train_loss 0.003941,Time used 0.002976s\n",
      "batch 5203, train_loss 0.003400,Time used 0.003473s\n",
      "batch 5204, train_loss 0.003658,Time used 0.002976s\n",
      "batch 5205, train_loss 0.003987,Time used 0.002975s\n",
      "batch 5206, train_loss 0.004384,Time used 0.002480s\n",
      "batch 5207, train_loss 0.003825,Time used 0.002976s\n",
      "batch 5208, train_loss 0.002567,Time used 0.002981s\n",
      "batch 5209, train_loss 0.003306,Time used 0.002976s\n",
      "batch 5210, train_loss 0.003592,Time used 0.003464s\n",
      "batch 5211, train_loss 0.003573,Time used 0.002971s\n",
      "batch 5212, train_loss 0.003658,Time used 0.002975s\n",
      "batch 5213, train_loss 0.002941,Time used 0.002976s\n",
      "batch 5214, train_loss 0.002763,Time used 0.002976s\n",
      "batch 5215, train_loss 0.003332,Time used 0.002982s\n",
      "batch 5216, train_loss 0.003702,Time used 0.002971s\n",
      "batch 5217, train_loss 0.002870,Time used 0.002975s\n",
      "batch 5218, train_loss 0.003378,Time used 0.002976s\n",
      "batch 5219, train_loss 0.003871,Time used 0.002976s\n",
      "batch 5220, train_loss 0.003965,Time used 0.002970s\n",
      "batch 5221, train_loss 0.004362,Time used 0.002977s\n",
      "batch 5222, train_loss 0.003218,Time used 0.002976s\n",
      "batch 5223, train_loss 0.004480,Time used 0.002976s\n",
      "batch 5224, train_loss 0.003225,Time used 0.002976s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5225, train_loss 0.004417,Time used 0.003472s\n",
      "batch 5226, train_loss 0.003962,Time used 0.002976s\n",
      "batch 5227, train_loss 0.004932,Time used 0.002976s\n",
      "batch 5228, train_loss 0.003727,Time used 0.002976s\n",
      "batch 5229, train_loss 0.003435,Time used 0.002976s\n",
      "batch 5230, train_loss 0.004930,Time used 0.003473s\n",
      "batch 5231, train_loss 0.004186,Time used 0.003472s\n",
      "batch 5232, train_loss 0.003803,Time used 0.002976s\n",
      "batch 5233, train_loss 0.003920,Time used 0.003464s\n",
      "batch 5234, train_loss 0.003504,Time used 0.003472s\n",
      "batch 5235, train_loss 0.003531,Time used 0.003964s\n",
      "batch 5236, train_loss 0.004071,Time used 0.002976s\n",
      "batch 5237, train_loss 0.004713,Time used 0.003472s\n",
      "batch 5238, train_loss 0.003697,Time used 0.003473s\n",
      "batch 5239, train_loss 0.003446,Time used 0.002480s\n",
      "batch 5240, train_loss 0.004064,Time used 0.002977s\n",
      "batch 5241, train_loss 0.003352,Time used 0.002976s\n",
      "batch 5242, train_loss 0.003267,Time used 0.003472s\n",
      "batch 5243, train_loss 0.004037,Time used 0.002976s\n",
      "batch 5244, train_loss 0.003763,Time used 0.002967s\n",
      "batch 5245, train_loss 0.003572,Time used 0.004464s\n",
      "batch 5246, train_loss 0.003900,Time used 0.002976s\n",
      "batch 5247, train_loss 0.003748,Time used 0.003472s\n",
      "batch 5248, train_loss 0.003170,Time used 0.003472s\n",
      "batch 5249, train_loss 0.003009,Time used 0.003472s\n",
      "batch 5250, train_loss 0.003359,Time used 0.002977s\n",
      "batch 5251, train_loss 0.004043,Time used 0.003968s\n",
      "batch 5252, train_loss 0.003854,Time used 0.003472s\n",
      "batch 5253, train_loss 0.003274,Time used 0.002973s\n",
      "batch 5254, train_loss 0.004697,Time used 0.003472s\n",
      "batch 5255, train_loss 0.003943,Time used 0.003463s\n",
      "batch 5256, train_loss 0.003432,Time used 0.003472s\n",
      "batch 5257, train_loss 0.004492,Time used 0.003469s\n",
      "batch 5258, train_loss 0.004427,Time used 0.003472s\n",
      "batch 5259, train_loss 0.003219,Time used 0.003480s\n",
      "batch 5260, train_loss 0.003336,Time used 0.003472s\n",
      "batch 5261, train_loss 0.003018,Time used 0.002976s\n",
      "batch 5262, train_loss 0.003688,Time used 0.004464s\n",
      "batch 5263, train_loss 0.004350,Time used 0.003968s\n",
      "batch 5264, train_loss 0.003405,Time used 0.004465s\n",
      "batch 5265, train_loss 0.004960,Time used 0.002976s\n",
      "batch 5266, train_loss 0.003480,Time used 0.004464s\n",
      "batch 5267, train_loss 0.004496,Time used 0.011408s\n",
      "batch 5268, train_loss 0.003983,Time used 0.002975s\n",
      "batch 5269, train_loss 0.004038,Time used 0.003472s\n",
      "batch 5270, train_loss 0.002751,Time used 0.002976s\n",
      "batch 5271, train_loss 0.004452,Time used 0.002967s\n",
      "batch 5272, train_loss 0.003847,Time used 0.002976s\n",
      "batch 5273, train_loss 0.003713,Time used 0.002481s\n",
      "batch 5274, train_loss 0.003596,Time used 0.002975s\n",
      "batch 5275, train_loss 0.003303,Time used 0.002977s\n",
      "batch 5276, train_loss 0.003900,Time used 0.002980s\n",
      "batch 5277, train_loss 0.003793,Time used 0.003472s\n",
      "batch 5278, train_loss 0.003738,Time used 0.002976s\n",
      "batch 5279, train_loss 0.003223,Time used 0.002976s\n",
      "batch 5280, train_loss 0.003982,Time used 0.002976s\n",
      "batch 5281, train_loss 0.003577,Time used 0.002976s\n",
      "batch 5282, train_loss 0.003411,Time used 0.002976s\n",
      "batch 5283, train_loss 0.003103,Time used 0.003968s\n",
      "batch 5284, train_loss 0.003860,Time used 0.002975s\n",
      "batch 5285, train_loss 0.004200,Time used 0.002976s\n",
      "batch 5286, train_loss 0.003832,Time used 0.003471s\n",
      "batch 5287, train_loss 0.004505,Time used 0.002976s\n",
      "batch 5288, train_loss 0.003376,Time used 0.014383s\n",
      "batch 5289, train_loss 0.003225,Time used 0.002976s\n",
      "batch 5290, train_loss 0.004140,Time used 0.003472s\n",
      "batch 5291, train_loss 0.003473,Time used 0.003477s\n",
      "batch 5292, train_loss 0.001783,Time used 0.003472s\n",
      "batch 5293, train_loss 0.003121,Time used 0.002976s\n",
      "batch 5294, train_loss 0.004442,Time used 0.003968s\n",
      "batch 5295, train_loss 0.005254,Time used 0.002976s\n",
      "batch 5296, train_loss 0.003850,Time used 0.002976s\n",
      "batch 5297, train_loss 0.003028,Time used 0.002976s\n",
      "batch 5298, train_loss 0.004393,Time used 0.003472s\n",
      "batch 5299, train_loss 0.004206,Time used 0.002976s\n",
      "batch 5300, train_loss 0.002706,Time used 0.002976s\n",
      "***************************test_batch 5300, test_rmse_loss 0.061423,test_mae_loss 0.044117,test_mape_loss 13.379809,Time used 0.012888s\n",
      "batch 5301, train_loss 0.003359,Time used 0.002976s\n",
      "batch 5302, train_loss 0.003956,Time used 0.002963s\n",
      "batch 5303, train_loss 0.003330,Time used 0.002976s\n",
      "batch 5304, train_loss 0.003539,Time used 0.002976s\n",
      "batch 5305, train_loss 0.003933,Time used 0.002976s\n",
      "batch 5306, train_loss 0.002645,Time used 0.003472s\n",
      "batch 5307, train_loss 0.002942,Time used 0.002976s\n",
      "batch 5308, train_loss 0.003673,Time used 0.002480s\n",
      "batch 5309, train_loss 0.003895,Time used 0.002981s\n",
      "batch 5310, train_loss 0.003759,Time used 0.003473s\n",
      "batch 5311, train_loss 0.003704,Time used 0.002976s\n",
      "batch 5312, train_loss 0.003191,Time used 0.003472s\n",
      "batch 5313, train_loss 0.003624,Time used 0.002976s\n",
      "batch 5314, train_loss 0.003146,Time used 0.002976s\n",
      "batch 5315, train_loss 0.003176,Time used 0.002975s\n",
      "batch 5316, train_loss 0.004064,Time used 0.002976s\n",
      "batch 5317, train_loss 0.003690,Time used 0.003471s\n",
      "batch 5318, train_loss 0.003129,Time used 0.002976s\n",
      "batch 5319, train_loss 0.003974,Time used 0.002975s\n",
      "batch 5320, train_loss 0.003547,Time used 0.003472s\n",
      "batch 5321, train_loss 0.003019,Time used 0.002480s\n",
      "batch 5322, train_loss 0.004087,Time used 0.002976s\n",
      "batch 5323, train_loss 0.003732,Time used 0.002968s\n",
      "batch 5324, train_loss 0.003615,Time used 0.002481s\n",
      "batch 5325, train_loss 0.005536,Time used 0.003472s\n",
      "batch 5326, train_loss 0.003499,Time used 0.002491s\n",
      "batch 5327, train_loss 0.003462,Time used 0.003473s\n",
      "batch 5328, train_loss 0.003707,Time used 0.003476s\n",
      "batch 5329, train_loss 0.004389,Time used 0.002976s\n",
      "batch 5330, train_loss 0.003835,Time used 0.002481s\n",
      "batch 5331, train_loss 0.004077,Time used 0.002983s\n",
      "batch 5332, train_loss 0.004143,Time used 0.002975s\n",
      "batch 5333, train_loss 0.004385,Time used 0.002976s\n",
      "batch 5334, train_loss 0.003772,Time used 0.002470s\n",
      "batch 5335, train_loss 0.003551,Time used 0.002976s\n",
      "batch 5336, train_loss 0.003437,Time used 0.002977s\n",
      "batch 5337, train_loss 0.003110,Time used 0.002976s\n",
      "batch 5338, train_loss 0.004615,Time used 0.002480s\n",
      "batch 5339, train_loss 0.004022,Time used 0.002976s\n",
      "batch 5340, train_loss 0.003645,Time used 0.003968s\n",
      "batch 5341, train_loss 0.003682,Time used 0.003472s\n",
      "batch 5342, train_loss 0.004412,Time used 0.002976s\n",
      "batch 5343, train_loss 0.003734,Time used 0.012904s\n",
      "batch 5344, train_loss 0.003227,Time used 0.003473s\n",
      "batch 5345, train_loss 0.004025,Time used 0.003472s\n",
      "batch 5346, train_loss 0.005277,Time used 0.003472s\n",
      "batch 5347, train_loss 0.003613,Time used 0.002479s\n",
      "batch 5348, train_loss 0.003383,Time used 0.002975s\n",
      "batch 5349, train_loss 0.002835,Time used 0.003472s\n",
      "batch 5350, train_loss 0.003964,Time used 0.002975s\n",
      "batch 5351, train_loss 0.003158,Time used 0.003472s\n",
      "batch 5352, train_loss 0.003776,Time used 0.002976s\n",
      "batch 5353, train_loss 0.003750,Time used 0.002971s\n",
      "batch 5354, train_loss 0.004311,Time used 0.002976s\n",
      "batch 5355, train_loss 0.003987,Time used 0.003472s\n",
      "batch 5356, train_loss 0.003102,Time used 0.003472s\n",
      "batch 5357, train_loss 0.003178,Time used 0.002480s\n",
      "batch 5358, train_loss 0.003011,Time used 0.002972s\n",
      "batch 5359, train_loss 0.004138,Time used 0.002976s\n",
      "batch 5360, train_loss 0.004003,Time used 0.003472s\n",
      "batch 5361, train_loss 0.004477,Time used 0.002977s\n",
      "batch 5362, train_loss 0.003902,Time used 0.002977s\n",
      "batch 5363, train_loss 0.002770,Time used 0.002975s\n",
      "batch 5364, train_loss 0.002999,Time used 0.002975s\n",
      "batch 5365, train_loss 0.003927,Time used 0.002976s\n",
      "batch 5366, train_loss 0.003989,Time used 0.002976s\n",
      "batch 5367, train_loss 0.003627,Time used 0.002976s\n",
      "batch 5368, train_loss 0.003937,Time used 0.002976s\n",
      "batch 5369, train_loss 0.004059,Time used 0.002976s\n",
      "batch 5370, train_loss 0.003710,Time used 0.002976s\n",
      "batch 5371, train_loss 0.003307,Time used 0.002976s\n",
      "batch 5372, train_loss 0.004083,Time used 0.002976s\n",
      "batch 5373, train_loss 0.004150,Time used 0.003472s\n",
      "batch 5374, train_loss 0.003133,Time used 0.003472s\n",
      "batch 5375, train_loss 0.003930,Time used 0.002977s\n",
      "batch 5376, train_loss 0.003132,Time used 0.002975s\n",
      "batch 5377, train_loss 0.004726,Time used 0.002480s\n",
      "batch 5378, train_loss 0.002864,Time used 0.002480s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5379, train_loss 0.005577,Time used 0.002976s\n",
      "batch 5380, train_loss 0.004128,Time used 0.002976s\n",
      "batch 5381, train_loss 0.003812,Time used 0.003471s\n",
      "batch 5382, train_loss 0.003628,Time used 0.002977s\n",
      "batch 5383, train_loss 0.003950,Time used 0.002976s\n",
      "batch 5384, train_loss 0.003515,Time used 0.002976s\n",
      "batch 5385, train_loss 0.003549,Time used 0.002977s\n",
      "batch 5386, train_loss 0.003927,Time used 0.002480s\n",
      "batch 5387, train_loss 0.002833,Time used 0.002985s\n",
      "batch 5388, train_loss 0.003879,Time used 0.002976s\n",
      "batch 5389, train_loss 0.003458,Time used 0.002977s\n",
      "batch 5390, train_loss 0.004304,Time used 0.002976s\n",
      "batch 5391, train_loss 0.003815,Time used 0.002479s\n",
      "batch 5392, train_loss 0.003239,Time used 0.003471s\n",
      "batch 5393, train_loss 0.004181,Time used 0.002975s\n",
      "batch 5394, train_loss 0.003585,Time used 0.002976s\n",
      "batch 5395, train_loss 0.003615,Time used 0.002985s\n",
      "batch 5396, train_loss 0.004381,Time used 0.002972s\n",
      "batch 5397, train_loss 0.003894,Time used 0.002480s\n",
      "batch 5398, train_loss 0.002964,Time used 0.002976s\n",
      "batch 5399, train_loss 0.005551,Time used 0.002976s\n",
      "batch 5400, train_loss 0.002053,Time used 0.002480s\n",
      "***************************test_batch 5400, test_rmse_loss 0.061375,test_mae_loss 0.044007,test_mape_loss 13.101159,Time used 0.011903s\n",
      "The total time is 26.308846s\n"
     ]
    }
   ],
   "source": [
    "train_log = []\n",
    "test_log = []\n",
    "#开始时间\n",
    "timestart = time.time()\n",
    "trained_batches = 0 #记录多少个batch \n",
    "for epoch in range(100):\n",
    "   \n",
    "    total_1oss = 0 #记录Loss\n",
    "    for batch in next_batch(shuffle(train_set), batch_size=256):\n",
    "        #每一个batch的开始时间\n",
    "        batchstart = time.time()\n",
    "        \n",
    "        batch = torch.from_numpy(batch).float().to(device)  # (batch, seq_len)\n",
    "        # 使用短序列的前12个值作为历史，最后一个值作为预测值。\n",
    "        x, label = batch[:, :12], batch[:, -1]\n",
    "        out, hidden = model(x.unsqueeze(-1))  # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out_linear(out[:, -1, :])\n",
    "        prediction = out.squeeze(-1)  # (batch)\n",
    "        \n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #correct += (prediction == label).sum().item()\n",
    "        #累加loss\n",
    "        total_1oss += loss.item( )\n",
    "        trained_batches += 1\n",
    "         #计算平均oss与准确率\n",
    "        #train_loss = total_1oss / train_batch_num\n",
    "        #train_log.append(train_loss)   \n",
    "        # 每训练一定数量的batch，就在测试集上测试模型效果。\n",
    "        #if trained_batches % 100 == 0:\n",
    "        train_log.append(loss.detach().cpu().numpy().tolist());\n",
    "        train_batch_time = (time.time() - batchstart)\n",
    "        print('batch %d, train_loss %.6f,Time used %.6fs'%(trained_batches, loss,train_batch_time))\n",
    "       \n",
    "    \n",
    "        \n",
    "        # 每训练一定数量的batch，就在测试集上测试模型效果。\n",
    "        if trained_batches % 100 == 0:\n",
    "            #每一个batch的开始时间\n",
    "            batch_test_start = time.time()\n",
    "            #在每个epoch上测试\n",
    "            all_prediction = []\n",
    "            for batch in next_batch(test_set, batch_size=256):\n",
    "                batch = torch.from_numpy(batch).float().to(device)  # (batch, seq_len)\n",
    "                x, label = batch[:, :12], batch[:, -1]\n",
    "                out, hidden = model(x.unsqueeze(-1))  # out: (batch_size, seq_len, hidden_size)\n",
    "                out = out_linear(out[:, -1, :])\n",
    "                prediction = out.squeeze(-1)  # (batch)\n",
    "                all_prediction.append(prediction.detach().cpu().numpy())\n",
    "\n",
    "            all_prediction = np.concatenate(all_prediction)\n",
    "            all_label = test_set[:, -1]\n",
    "            # 没有进行反归一化操作。\n",
    "            #all_prediction = denormalize(all_prediction)\n",
    "            #all_label = denormalize(all_label)\n",
    "            # 计算测试指标。\n",
    "            rmse_score = math.sqrt(mse(all_label, all_prediction))\n",
    "            mae_score = mae(all_label, all_prediction)\n",
    "            mape_score = mape(all_label, all_prediction)\n",
    "            test_log.append([rmse_score, mae_score, mape_score])\n",
    "            test_batch_time = (time.time() - batch_test_start)\n",
    "            print('***************************test_batch %d, test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(trained_batches, rmse_score,mae_score,mape_score,test_batch_time))\n",
    "\n",
    "        #每一个epoch的结束时间\n",
    "        #elapsed = (time.time() - epochstart)\n",
    "    #print('epoch %d, train_loss %.6f,test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(epoch+1, train_loss,rmse_score,mae_score,mape_score,elapsed))\n",
    "    #print('epoch %d, train_loss %.6f,test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(epoch+1, train_loss,rmse_score,mae_score,mape_score,elapsed),file=f)\n",
    "    \n",
    "#计算总时间\n",
    "timesum = (time.time() - timestart)\n",
    "print('The total time is %fs'%(timesum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_loss的曲线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_loss曲线  \n",
    "x = np.linspace(0,len(train_log),len(train_log))  \n",
    "plt.plot(x,train_log,label=\"train_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('train_loss.jpg')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rmse、mae、mape的曲线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test_loss曲线  \n",
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,0],label=\"test_rmse_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_rmse_loss.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,1],label=\"test_mae_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_mae_loss.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,2],label=\"test_mape_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_mape_loss.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
