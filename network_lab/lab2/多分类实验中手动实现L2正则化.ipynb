{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad79d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"  #防止jupyter崩溃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1368a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#下载MNIST手写数据集\n",
    "mnist_train = torchvision.datasets.MNIST(root='./Datasets/MNIST', train=True,\n",
    "download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root='./Datasets/MNIST', train=False,\n",
    "download=True, transform=transforms.ToTensor())\n",
    "#读取数据\n",
    "batch_size = 32\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True,\n",
    "num_workers=0)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False,\n",
    "num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caefc606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化参数\n",
    "# num_inputs,num_hiddens,num_outputs = 784,256,10\n",
    "num_inputs,num_hiddens,num_outputs = 784,512,10\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens,num_inputs)), dtype=torch.float32)\n",
    "b1 = torch.zeros(1, dtype=torch.float32)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_outputs,num_hiddens)), dtype=torch.float32)\n",
    "b2 = torch.zeros(1, dtype=torch.float32)\n",
    "params =[W1,b1,W2,b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78821060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_penalty(w):\n",
    "    return (w**2).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0bd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义随机梯度下降法\n",
    "def SGD(paras,lr):\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce61883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义交叉熵损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049a5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "def net(X):\n",
    "    X = X.view((-1,num_inputs))\n",
    "    H=torch.sigmoid(torch.matmul(X,W1.t())+b1)\n",
    "    return torch.matmul(H,W2.t())+b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b1aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集loss\n",
    "def evaluate_loss(data_iter,net):\n",
    "    acc_sum,loss_sum,n = 0.0,0.0,0\n",
    "    for X,y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        acc_sum += (y_hat.argmax(dim=1)==y).sum().item()\n",
    "        l = loss(y_hat,y) # l是有关小批量X和y的损失\n",
    "        loss_sum += l.sum().item()*y.shape[0]\n",
    "        n+=y.shape[0]\n",
    "    return acc_sum/n,loss_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f1b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型训练函数\n",
    "def train(net,train_iter,test_iter,loss,num_epochs,batch_size,lambd,params=None,lr=None,optimizer=None):\n",
    "    train_ls = []\n",
    "    test_ls = []\n",
    "    for epoch in range(num_epochs): # 训练模型一共需要num_epochs个迭代周期\n",
    "        train_l_sum, train_acc_num,n = 0.0,0.0,0\n",
    "        # 在每一个迭代周期中，会使用训练数据集中所有样本一次\n",
    "        for X, y in train_iter: # x和y分别是小批量样本的特征和标签\n",
    "            y_hat = net(X)\n",
    "            #添加惩罚项，用lambd控制惩罚权重，0的话为不使用\n",
    "            l = loss(y_hat, y)+lambd*(l2_penalty(W1)+l2_penalty(W2)) # l是有关小批量X和y的损失\n",
    "            l=l.sum()\n",
    "            #梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward() # 小批量的损失对模型参数求梯度\n",
    "            if optimizer is None:\n",
    "                SGD(params,lr)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            #计算每个epoch的loss\n",
    "            train_l_sum += l.item()*y.shape[0]\n",
    "            #计算训练样本的准确率\n",
    "            train_acc_num += (y_hat.argmax(dim=1)==y).sum().item()\n",
    "            #每一个epoch的所有样本数\n",
    "            n+= y.shape[0]\n",
    "        train_ls.append(train_l_sum/n)\n",
    "        test_acc,test_l = evaluate_loss(test_iter,net)\n",
    "        test_ls.append(test_l)\n",
    "        print('epoch %d, train_loss %.6f,test_loss %f,train_acc %.6f,test_acc %.6f'%(epoch+1, train_ls[epoch],test_ls[epoch],train_acc_num/n,test_acc))\n",
    "    return train_ls,test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfef1141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 7.388573,test_loss 2.303842,train_acc 0.195500,test_acc 0.103200\n",
      "epoch 2, train_loss 2.308028,test_loss 2.309228,train_acc 0.104183,test_acc 0.095800\n",
      "epoch 3, train_loss 2.307144,test_loss 2.304580,train_acc 0.103667,test_acc 0.113500\n",
      "epoch 4, train_loss 2.306595,test_loss 2.310961,train_acc 0.103650,test_acc 0.102800\n",
      "epoch 5, train_loss 2.305888,test_loss 2.302462,train_acc 0.104100,test_acc 0.113500\n",
      "epoch 6, train_loss 2.305518,test_loss 2.304229,train_acc 0.105567,test_acc 0.097400\n",
      "epoch 7, train_loss 2.305011,test_loss 2.303120,train_acc 0.106883,test_acc 0.102800\n",
      "epoch 8, train_loss 2.304636,test_loss 2.304774,train_acc 0.105133,test_acc 0.102800\n",
      "epoch 9, train_loss 2.304257,test_loss 2.303165,train_acc 0.108167,test_acc 0.102800\n",
      "epoch 10, train_loss 2.304026,test_loss 2.302712,train_acc 0.105617,test_acc 0.098000\n",
      "epoch 11, train_loss 2.303824,test_loss 2.303895,train_acc 0.106217,test_acc 0.113500\n",
      "epoch 12, train_loss 2.304008,test_loss 2.302258,train_acc 0.106167,test_acc 0.103200\n",
      "epoch 13, train_loss 2.303758,test_loss 2.302800,train_acc 0.106583,test_acc 0.102800\n",
      "epoch 14, train_loss 2.303740,test_loss 2.301514,train_acc 0.105883,test_acc 0.113500\n",
      "epoch 15, train_loss 2.303474,test_loss 2.301721,train_acc 0.107300,test_acc 0.113500\n",
      "epoch 16, train_loss 2.303321,test_loss 2.301822,train_acc 0.107567,test_acc 0.113500\n",
      "epoch 17, train_loss 2.303377,test_loss 2.301817,train_acc 0.106933,test_acc 0.113500\n",
      "epoch 18, train_loss 2.303352,test_loss 2.301523,train_acc 0.107533,test_acc 0.113500\n",
      "epoch 19, train_loss 2.303432,test_loss 2.302115,train_acc 0.106000,test_acc 0.098200\n",
      "epoch 20, train_loss 2.303229,test_loss 2.301555,train_acc 0.108383,test_acc 0.113500\n",
      "epoch 21, train_loss 2.303159,test_loss 2.303839,train_acc 0.108483,test_acc 0.101000\n",
      "epoch 22, train_loss 2.303080,test_loss 2.302744,train_acc 0.107250,test_acc 0.102800\n",
      "epoch 23, train_loss 2.303184,test_loss 2.302307,train_acc 0.107350,test_acc 0.103200\n",
      "epoch 24, train_loss 2.302844,test_loss 2.302337,train_acc 0.106483,test_acc 0.113500\n",
      "epoch 25, train_loss 2.303078,test_loss 2.302494,train_acc 0.108050,test_acc 0.102800\n",
      "epoch 26, train_loss 2.302970,test_loss 2.301530,train_acc 0.108900,test_acc 0.113500\n",
      "epoch 27, train_loss 2.302997,test_loss 2.301759,train_acc 0.108050,test_acc 0.102800\n",
      "epoch 28, train_loss 2.302902,test_loss 2.301584,train_acc 0.108950,test_acc 0.113500\n",
      "epoch 29, train_loss 2.302946,test_loss 2.302447,train_acc 0.109917,test_acc 0.101000\n",
      "epoch 30, train_loss 2.302881,test_loss 2.302303,train_acc 0.108000,test_acc 0.097400\n",
      "epoch 31, train_loss 2.302957,test_loss 2.301497,train_acc 0.108033,test_acc 0.113500\n",
      "epoch 32, train_loss 2.302877,test_loss 2.301234,train_acc 0.109067,test_acc 0.113500\n",
      "epoch 33, train_loss 2.302937,test_loss 2.301833,train_acc 0.107650,test_acc 0.102800\n",
      "epoch 34, train_loss 2.302871,test_loss 2.301961,train_acc 0.107333,test_acc 0.100900\n",
      "epoch 35, train_loss 2.302864,test_loss 2.302627,train_acc 0.106833,test_acc 0.098200\n",
      "epoch 36, train_loss 2.302774,test_loss 2.301861,train_acc 0.107933,test_acc 0.101000\n",
      "epoch 37, train_loss 2.302869,test_loss 2.302128,train_acc 0.108267,test_acc 0.113500\n",
      "epoch 38, train_loss 2.302830,test_loss 2.302316,train_acc 0.109467,test_acc 0.102800\n",
      "epoch 39, train_loss 2.302679,test_loss 2.302115,train_acc 0.107483,test_acc 0.102800\n",
      "epoch 40, train_loss 2.302773,test_loss 2.301301,train_acc 0.108367,test_acc 0.113500\n",
      "epoch 41, train_loss 2.302610,test_loss 2.301732,train_acc 0.108233,test_acc 0.113500\n",
      "epoch 42, train_loss 2.302673,test_loss 2.301798,train_acc 0.108650,test_acc 0.102800\n",
      "epoch 43, train_loss 2.302843,test_loss 2.301798,train_acc 0.108833,test_acc 0.113500\n",
      "epoch 44, train_loss 2.302729,test_loss 2.301607,train_acc 0.107733,test_acc 0.113500\n",
      "epoch 45, train_loss 2.302778,test_loss 2.301355,train_acc 0.107067,test_acc 0.113500\n",
      "epoch 46, train_loss 2.302683,test_loss 2.302544,train_acc 0.109017,test_acc 0.098000\n",
      "epoch 47, train_loss 2.302744,test_loss 2.301838,train_acc 0.107717,test_acc 0.102800\n",
      "epoch 48, train_loss 2.302802,test_loss 2.301663,train_acc 0.107317,test_acc 0.113500\n",
      "epoch 49, train_loss 2.302706,test_loss 2.301663,train_acc 0.109333,test_acc 0.113500\n",
      "epoch 50, train_loss 2.302803,test_loss 2.301544,train_acc 0.109317,test_acc 0.113500\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 50\n",
    "train_loss,test_loss = train(net,train_iter,test_iter,loss,num_epochs,batch_size,1,params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a89990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf1b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
