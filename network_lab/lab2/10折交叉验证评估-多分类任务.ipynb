{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"  #防止jupyter崩溃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#下载MNIST手写数据集\n",
    "mnist_train = torchvision.datasets.MNIST(root='./Datasets/MNIST', train=True,\n",
    "download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root='./Datasets/MNIST', train=False,\n",
    "download=True, transform=transforms.ToTensor())\n",
    "#读取数据\n",
    "batch_size = 32\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True,\n",
    "num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、构建模型\n",
    "num_inputs = 784\n",
    "num_outputs = 10  # 共10类\n",
    "num_hiddens = 256\n",
    "\n",
    "\n",
    "class FlattenLayer(torch.nn.Module):  # Flatten层\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "class SoftmaxLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxLayer, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_exp = X.exp()  # 对每个元素做指数运算\n",
    "        partition = X_exp.sum(dim=1, keepdim=True)  # 求列和，即对同行元素求和 n*1\n",
    "        return X_exp / partition  # broadcast\n",
    "\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    FlattenLayer(),\n",
    "    torch.nn.Linear(num_inputs, num_hiddens),\n",
    "    # 下面是三种可选用的激活函数\n",
    "    torch.nn.ReLU(),  # Relu激活函数\n",
    "    # torch.nn.Softplus(),  # Softplus激活函数\n",
    "    # torch.nn.Tanh(),  # Tanh激活函数\n",
    "    torch.nn.Linear(num_hiddens, num_outputs),\n",
    "    SoftmaxLayer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、初始化模型参数\n",
    "for params in net.parameters():  # 对网络中的每个参数\n",
    "    torch.nn.init.normal_(params, mean=0, std=0.01)  # 初始化为服从均值0标准差0.01正态分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5、损失函数与优化器\n",
    "num_epochs = 10  # 训练轮次\n",
    "lr = 0.1\n",
    "loss = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def evaluate(data_iter, net):\n",
    "    right_sum, n, loss_sum = 0.0, 0, 0.0\n",
    "    for x, y in data_iter:\n",
    "        y_ = net(x)\n",
    "        l = loss(y_, y).sum()\n",
    "        right_sum += (y_.argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "        loss_sum += l.item()\n",
    "    return right_sum / n, loss_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_data(k, i, data):  # 获取第i+1（i=0~k-1）折的训练集和验证集\n",
    "    # train_features = mnist_train.data  # 训练集特征数据\n",
    "    # train_labels = mnist_train.targets  # 训练集标签数据\n",
    "    fold_size = data.targets.shape[0] // k  # 每份数据个数\n",
    "    valid_data = deepcopy(data)\n",
    "    train_data = deepcopy(data)\n",
    "    start_ = i*fold_size\n",
    "    if i != k-1:\n",
    "        end_ = (i+1)*fold_size\n",
    "        valid_data.data = valid_data.data[start_:end_]  # 验证集\n",
    "        valid_data.targets = valid_data.targets[start_:end_]  # 验证集\n",
    "        train_data.data = torch.cat((train_data.data[0:start_], train_data.data[end_:]), dim=0)  # cat拼接\n",
    "        train_data.targets = torch.cat((train_data.targets[0:start_], train_data.targets[end_:]), dim=0)  # cat拼接\n",
    "    else:  # 是最后一折\n",
    "        valid_data.data, valid_data.targets = valid_data.data[start_:], valid_data.targets[start_:]  #\n",
    "        train_data.data, train_data.targets = train_data.data[0:start_], train_data.targets[0:start_]\n",
    "    return train_data, valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 折验证\n",
      "train loss 0.0499, val loss 0.0488, train acc 0.882, val acc 0.911\n",
      "第 2 折验证\n",
      "train loss 0.0499, val loss 0.0492, train acc 0.880, val acc 0.898\n",
      "第 3 折验证\n",
      "train loss 0.0493, val loss 0.0489, train acc 0.898, val acc 0.908\n",
      "第 4 折验证\n",
      "train loss 0.0499, val loss 0.0491, train acc 0.878, val acc 0.903\n",
      "第 5 折验证\n",
      "train loss 0.0495, val loss 0.0490, train acc 0.892, val acc 0.907\n",
      "第 6 折验证\n",
      "train loss 0.0494, val loss 0.0488, train acc 0.895, val acc 0.911\n",
      "第 7 折验证\n",
      "train loss 0.0497, val loss 0.0491, train acc 0.888, val acc 0.905\n",
      "第 8 折验证\n",
      "train loss 0.0494, val loss 0.0489, train acc 0.894, val acc 0.911\n",
      "第 9 折验证\n",
      "train loss 0.0496, val loss 0.0491, train acc 0.891, val acc 0.903\n",
      "第 10 折验证\n",
      "train loss 0.0494, val loss 0.0481, train acc 0.895, val acc 0.936\n",
      "\n",
      "最终k折交叉验证结果：\n",
      "ave train loss: 0.0496, ave train acc: 0.889\n",
      "ave valid loss: 0.0489, ave valid acc: 0.909\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def k_train(net, train_data, valid_data):\n",
    "    train_iter = Data.DataLoader(\n",
    "        dataset=train_data,  # torch TensorDataset format\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,  # 是否打乱数据\n",
    "        num_workers=0,  # 多线程来读数据，在Win下需要设置为0\n",
    "    )\n",
    "    valid_iter = Data.DataLoader(\n",
    "        dataset=valid_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    train_acc, train_l = 0.0, 0.0\n",
    "    valid_acc, valid_l = 0.0, 0.0\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_r_num, train_l_, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            # optimizer.step()\n",
    "            optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            train_r_num += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            train_l_ += l.item()\n",
    "            n += y.shape[0]\n",
    "        v_acc, v_l = evaluate(valid_iter, net)\n",
    "        valid_acc += v_acc\n",
    "        valid_l += v_l\n",
    "        train_acc += train_r_num / n\n",
    "        train_l += train_l_ / n\n",
    "    return train_l/num_epochs, valid_l/num_epochs, train_acc/num_epochs, valid_acc/num_epochs\n",
    "\n",
    "\n",
    "def kfold_train(k):\n",
    "    train_loss_sum, valid_loss_sum = 0, 0\n",
    "    train_acc_sum, valid_acc_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        print('第', i+1, '折验证')\n",
    "        train_data, valid_data = get_kfold_data(k, i, mnist_train)\n",
    "        net_ = torch.nn.Sequential(\n",
    "            FlattenLayer(),\n",
    "            torch.nn.Linear(num_inputs, num_hiddens),\n",
    "            torch.nn.ReLU(),  # Relu激活函数\n",
    "            torch.nn.Linear(num_hiddens, num_outputs),\n",
    "            SoftmaxLayer(),\n",
    "        )\n",
    "        for params in net_.parameters():  # 对网络中的每个参数\n",
    "            torch.nn.init.normal_(params, mean=0, std=0.01)  # 初始化为服从均值0标准差0.01正态分布\n",
    "            \n",
    "        train_loss, val_loss, train_acc, val_acc = k_train(net_, train_data, valid_data)\n",
    "        print('train loss %.4f, val loss %.4f, train acc %.3f, val acc %.3f' % (train_loss, val_loss, train_acc, val_acc))\n",
    "\n",
    "        train_loss_sum += train_loss\n",
    "        valid_loss_sum += val_loss\n",
    "        train_acc_sum += train_acc\n",
    "        valid_acc_sum += val_acc\n",
    "    print('\\n最终k折交叉验证结果：')\n",
    "    print('ave train loss: %.4f, ave train acc: %.3f' % (train_loss_sum/k, train_acc_sum/k))\n",
    "    print('ave valid loss: %.4f, ave valid acc: %.3f' % (valid_loss_sum/k, valid_acc_sum/k))\n",
    "\n",
    "kfold_train(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
