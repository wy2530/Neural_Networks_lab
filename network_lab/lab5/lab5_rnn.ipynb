{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 1.9.0+cu102\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: d:\\download\\anaconda\\envs\\pytorch\\lib\\site-packages\n",
      "Requires: typing-extensions\n",
      "Required-by: torchvision, torchaudio\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import pandas\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 10, 20, 2)\n",
      "(960, 10, 20, 2)\n",
      "volume_train: [[[64.]]]\n",
      "(1920, 1, 1, 1)\n",
      "(960, 1, 1, 1)\n",
      "(1913, 7) (953, 7)\n"
     ]
    }
   ],
   "source": [
    "## 划分数据集\n",
    "def sliding_window(seq,window_size):\n",
    "    result=[]\n",
    "    for i in range(len(seq)-window_size):\n",
    "        result.append(seq[i:i+window_size])\n",
    "    return result\n",
    "\n",
    "\n",
    "train_path=\"./volume_train.npz\"\n",
    "test_path=\"./volume_test.npz\"\n",
    "\n",
    "volume_train=np.load(open(train_path,'rb'))[\"volume\"]\n",
    "volume_test=np.load(open(test_path,'rb'))[\"volume\"]\n",
    "\n",
    "print(volume_train.shape)  # 1920个时间段，10*20个区域，2个特征(入流量+出流量)\n",
    "print(volume_test.shape)\n",
    "\"\"\"\n",
    ": 取1920个时间段数据\n",
    "0:1  取的10*20的网格的第一行数据\n",
    "0:1  取的是20的路口的第一个路口数据 \n",
    "0:1  取得是每个路口的入流量\n",
    "\"\"\"\n",
    "volume_train=volume_train[:,0:1,0:1,0:1]\n",
    "print(\"volume_train:\",volume_train[0])\n",
    "# print(volume_train[:,:,:,:])\n",
    "volume_test=volume_test[:,0:1,0:1,0:1]\n",
    "print(volume_train.shape)  # \n",
    "print(volume_test.shape)\n",
    "## 归一化\n",
    "dmin,dmax=volume_train.min(),volume_train.max()\n",
    "volume_train=(volume_train-dmin)/(dmax-dmin)\n",
    "\n",
    "dmin,dmax=volume_test.min(),volume_test.max()\n",
    "volume_test=(volume_test-dmin)/(dmax-dmin)\n",
    "\n",
    "\n",
    "## 滑动窗口划分\n",
    "train_set,test_set = [],[]  \n",
    "for  i in range(1) :  \n",
    "    train_seq = volume_train[:,:,:,:]  \n",
    "    test_seq = volume_test[:,:,:,:]  \n",
    "    ## 将长序列划分为若干个长度为6的短序列\n",
    "    train_set += sliding_window(train_seq,window_size=7)  \n",
    "    test_set += sliding_window(test_seq,window_size=7)  \n",
    "train_set,test_set= np.array(train_set).squeeze(), np.array(test_set).squeeze()  \n",
    "print(train_set.shape,test_set.shape)    \n",
    "print(train_set,test_set)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化：\n",
    "1. min-max标准化  \n",
    "\n",
    "$ x'=x-min/max-min $ \n",
    "\n",
    "这种方法有个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。\n",
    "\n",
    "2. Z-score 标准化方法\n",
    "\n",
    "$ x'=x-u/q $\n",
    "\n",
    "https://blog.csdn.net/haoji007/article/details/81157224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'  \n",
    "model = nn.RNN(input_size=1, hidden_size=32, num_layers=1, batch_first=True).to(device)\n",
    "out_linear = nn.Sequential(nn.Linear(32, 1), nn.LeakyReLU()).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(out_linear.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):  \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)  \n",
    "    non_zero_index = (y_true > 0)  \n",
    "    y_true = y_true[non_zero_index]  \n",
    "    y_pred = y_pred[non_zero_index]  \n",
    "  \n",
    "    mape = np.abs((y_true - y_pred) / y_true)  \n",
    "    mape[np.isinf(mape)] = 0  \n",
    "    return np.mean(mape) * 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next_batch函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(data, batch_size):  \n",
    "    data_length = len(data)  \n",
    "    num_batches = math.ceil(data_length / batch_size)  \n",
    "    for batch_index in range(num_batches):  \n",
    "        start_index = batch_index * batch_size  \n",
    "        end_index = min((batch_index + 1) * batch_size, data_length)  \n",
    "        yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle \n",
    "import math \n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image \n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, train_loss 0.075954,Time used 0.583888s\n",
      "batch 2, train_loss 0.073781,Time used 0.002992s\n",
      "batch 3, train_loss 0.078928,Time used 0.002992s\n",
      "batch 4, train_loss 0.078714,Time used 0.002992s\n",
      "batch 5, train_loss 0.068961,Time used 0.002992s\n",
      "batch 6, train_loss 0.064772,Time used 0.001995s\n",
      "batch 7, train_loss 0.073495,Time used 0.002992s\n",
      "batch 8, train_loss 0.061305,Time used 0.002992s\n",
      "batch 9, train_loss 0.080388,Time used 0.002992s\n",
      "batch 10, train_loss 0.067506,Time used 0.001994s\n",
      "batch 11, train_loss 0.074622,Time used 0.002993s\n",
      "batch 12, train_loss 0.059457,Time used 0.002992s\n",
      "batch 13, train_loss 0.052891,Time used 0.001994s\n",
      "batch 14, train_loss 0.061801,Time used 0.001994s\n",
      "batch 15, train_loss 0.061977,Time used 0.002992s\n",
      "batch 16, train_loss 0.050851,Time used 0.002992s\n",
      "batch 17, train_loss 0.074433,Time used 0.002992s\n",
      "batch 18, train_loss 0.059687,Time used 0.001994s\n",
      "batch 19, train_loss 0.065221,Time used 0.001995s\n",
      "batch 20, train_loss 0.050544,Time used 0.002992s\n",
      "batch 21, train_loss 0.044987,Time used 0.001994s\n",
      "batch 22, train_loss 0.061603,Time used 0.002993s\n",
      "batch 23, train_loss 0.058329,Time used 0.002991s\n",
      "batch 24, train_loss 0.055708,Time used 0.001995s\n",
      "batch 25, train_loss 0.060488,Time used 0.001994s\n",
      "batch 26, train_loss 0.061041,Time used 0.001994s\n",
      "batch 27, train_loss 0.059877,Time used 0.002992s\n",
      "batch 28, train_loss 0.052970,Time used 0.001995s\n",
      "batch 29, train_loss 0.052794,Time used 0.002992s\n",
      "batch 30, train_loss 0.051703,Time used 0.002992s\n",
      "batch 31, train_loss 0.052428,Time used 0.001994s\n",
      "batch 32, train_loss 0.052712,Time used 0.001995s\n",
      "batch 33, train_loss 0.048596,Time used 0.002992s\n",
      "batch 34, train_loss 0.059739,Time used 0.001995s\n",
      "batch 35, train_loss 0.043949,Time used 0.001994s\n",
      "batch 36, train_loss 0.038951,Time used 0.001996s\n",
      "batch 37, train_loss 0.058421,Time used 0.001994s\n",
      "batch 38, train_loss 0.055775,Time used 0.002993s\n",
      "batch 39, train_loss 0.049221,Time used 0.002991s\n",
      "batch 40, train_loss 0.050912,Time used 0.001994s\n",
      "batch 41, train_loss 0.047423,Time used 0.001995s\n",
      "batch 42, train_loss 0.053822,Time used 0.002992s\n",
      "batch 43, train_loss 0.047530,Time used 0.001993s\n",
      "batch 44, train_loss 0.050864,Time used 0.002993s\n",
      "batch 45, train_loss 0.045476,Time used 0.001993s\n",
      "batch 46, train_loss 0.044518,Time used 0.001994s\n",
      "batch 47, train_loss 0.046276,Time used 0.002992s\n",
      "batch 48, train_loss 0.041515,Time used 0.002992s\n",
      "batch 49, train_loss 0.053115,Time used 0.001995s\n",
      "batch 50, train_loss 0.042575,Time used 0.002992s\n",
      "batch 51, train_loss 0.053733,Time used 0.001994s\n",
      "batch 52, train_loss 0.049106,Time used 0.002992s\n",
      "batch 53, train_loss 0.038351,Time used 0.001995s\n",
      "batch 54, train_loss 0.044643,Time used 0.002992s\n",
      "batch 55, train_loss 0.047674,Time used 0.001995s\n",
      "batch 56, train_loss 0.050260,Time used 0.002992s\n",
      "batch 57, train_loss 0.052848,Time used 0.001995s\n",
      "batch 58, train_loss 0.041307,Time used 0.002992s\n",
      "batch 59, train_loss 0.050014,Time used 0.001994s\n",
      "batch 60, train_loss 0.046961,Time used 0.002992s\n",
      "batch 61, train_loss 0.045436,Time used 0.002992s\n",
      "batch 62, train_loss 0.043200,Time used 0.001994s\n",
      "batch 63, train_loss 0.045217,Time used 0.002992s\n",
      "batch 64, train_loss 0.041225,Time used 0.001994s\n",
      "batch 65, train_loss 0.048413,Time used 0.002992s\n",
      "batch 66, train_loss 0.039843,Time used 0.001994s\n",
      "batch 67, train_loss 0.047005,Time used 0.002992s\n",
      "batch 68, train_loss 0.054679,Time used 0.001994s\n",
      "batch 69, train_loss 0.048367,Time used 0.002993s\n",
      "batch 70, train_loss 0.042397,Time used 0.002990s\n",
      "batch 71, train_loss 0.044516,Time used 0.001994s\n",
      "batch 72, train_loss 0.037886,Time used 0.002993s\n",
      "batch 73, train_loss 0.047279,Time used 0.003991s\n",
      "batch 74, train_loss 0.046659,Time used 0.002989s\n",
      "batch 75, train_loss 0.047675,Time used 0.002992s\n",
      "batch 76, train_loss 0.048409,Time used 0.002991s\n",
      "batch 77, train_loss 0.041666,Time used 0.002430s\n",
      "batch 78, train_loss 0.042277,Time used 0.002992s\n",
      "batch 79, train_loss 0.044465,Time used 0.002992s\n",
      "batch 80, train_loss 0.041598,Time used 0.002992s\n",
      "batch 81, train_loss 0.039788,Time used 0.001994s\n",
      "batch 82, train_loss 0.044300,Time used 0.002993s\n",
      "batch 83, train_loss 0.045331,Time used 0.002992s\n",
      "batch 84, train_loss 0.047509,Time used 0.002992s\n",
      "batch 85, train_loss 0.043667,Time used 0.001995s\n",
      "batch 86, train_loss 0.041726,Time used 0.002993s\n",
      "batch 87, train_loss 0.041879,Time used 0.002991s\n",
      "batch 88, train_loss 0.047791,Time used 0.002992s\n",
      "batch 89, train_loss 0.045583,Time used 0.001994s\n",
      "batch 90, train_loss 0.049345,Time used 0.001995s\n",
      "batch 91, train_loss 0.047361,Time used 0.002991s\n",
      "batch 92, train_loss 0.044863,Time used 0.001994s\n",
      "batch 93, train_loss 0.041411,Time used 0.003038s\n",
      "batch 94, train_loss 0.040837,Time used 0.002947s\n",
      "batch 95, train_loss 0.047986,Time used 0.001993s\n",
      "batch 96, train_loss 0.045098,Time used 0.001995s\n",
      "batch 97, train_loss 0.041795,Time used 0.002992s\n",
      "batch 98, train_loss 0.042291,Time used 0.002992s\n",
      "batch 99, train_loss 0.047110,Time used 0.001994s\n",
      "batch 100, train_loss 0.039377,Time used 0.002002s\n",
      "test_batch 100, test_rmse_loss 0.213271,test_mae_loss 0.186480,test_mape_loss 178.535178,Time used 0.008967s\n",
      "batch 101, train_loss 0.046560,Time used 0.002993s\n",
      "batch 102, train_loss 0.043081,Time used 0.001994s\n",
      "batch 103, train_loss 0.037510,Time used 0.002992s\n",
      "batch 104, train_loss 0.048923,Time used 0.001994s\n",
      "batch 105, train_loss 0.035013,Time used 0.002993s\n",
      "batch 106, train_loss 0.044986,Time used 0.002992s\n",
      "batch 107, train_loss 0.043883,Time used 0.001994s\n",
      "batch 108, train_loss 0.042744,Time used 0.002993s\n",
      "batch 109, train_loss 0.045326,Time used 0.001994s\n",
      "batch 110, train_loss 0.038668,Time used 0.001994s\n",
      "batch 111, train_loss 0.043795,Time used 0.002992s\n",
      "batch 112, train_loss 0.033659,Time used 0.001995s\n",
      "batch 113, train_loss 0.047120,Time used 0.002991s\n",
      "batch 114, train_loss 0.043870,Time used 0.002993s\n",
      "batch 115, train_loss 0.039786,Time used 0.002992s\n",
      "batch 116, train_loss 0.042070,Time used 0.001994s\n",
      "batch 117, train_loss 0.046987,Time used 0.002992s\n",
      "batch 118, train_loss 0.042186,Time used 0.001994s\n",
      "batch 119, train_loss 0.036615,Time used 0.002993s\n",
      "batch 120, train_loss 0.041202,Time used 0.001993s\n",
      "batch 121, train_loss 0.041902,Time used 0.001994s\n",
      "batch 122, train_loss 0.042079,Time used 0.002993s\n",
      "batch 123, train_loss 0.043391,Time used 0.001994s\n",
      "batch 124, train_loss 0.043328,Time used 0.001994s\n",
      "batch 125, train_loss 0.041575,Time used 0.002992s\n",
      "batch 126, train_loss 0.034836,Time used 0.001994s\n",
      "batch 127, train_loss 0.044709,Time used 0.002992s\n",
      "batch 128, train_loss 0.042065,Time used 0.001994s\n",
      "batch 129, train_loss 0.039327,Time used 0.002992s\n",
      "batch 130, train_loss 0.042065,Time used 0.001995s\n",
      "batch 131, train_loss 0.035575,Time used 0.002992s\n",
      "batch 132, train_loss 0.037726,Time used 0.001994s\n",
      "batch 133, train_loss 0.044489,Time used 0.002992s\n",
      "batch 134, train_loss 0.042407,Time used 0.002993s\n",
      "batch 135, train_loss 0.040146,Time used 0.002992s\n",
      "batch 136, train_loss 0.044494,Time used 0.001995s\n",
      "batch 137, train_loss 0.042601,Time used 0.002992s\n",
      "batch 138, train_loss 0.035995,Time used 0.002992s\n",
      "batch 139, train_loss 0.040546,Time used 0.002992s\n",
      "batch 140, train_loss 0.038037,Time used 0.003990s\n",
      "batch 141, train_loss 0.041744,Time used 0.002991s\n",
      "batch 142, train_loss 0.045748,Time used 0.002999s\n",
      "batch 143, train_loss 0.041419,Time used 0.002985s\n",
      "batch 144, train_loss 0.045416,Time used 0.002993s\n",
      "batch 145, train_loss 0.041640,Time used 0.002991s\n",
      "batch 146, train_loss 0.037358,Time used 0.002992s\n",
      "batch 147, train_loss 0.038387,Time used 0.002990s\n",
      "batch 148, train_loss 0.039728,Time used 0.002987s\n",
      "batch 149, train_loss 0.032407,Time used 0.001993s\n",
      "batch 150, train_loss 0.031868,Time used 0.001994s\n",
      "batch 151, train_loss 0.040610,Time used 0.002022s\n",
      "batch 152, train_loss 0.037836,Time used 0.002964s\n",
      "batch 153, train_loss 0.043117,Time used 0.003990s\n",
      "batch 154, train_loss 0.036022,Time used 0.002992s\n",
      "batch 155, train_loss 0.041753,Time used 0.002991s\n",
      "batch 156, train_loss 0.039445,Time used 0.002993s\n",
      "batch 157, train_loss 0.037024,Time used 0.002992s\n",
      "batch 158, train_loss 0.041286,Time used 0.001994s\n",
      "batch 159, train_loss 0.039173,Time used 0.002991s\n",
      "batch 160, train_loss 0.031364,Time used 0.003989s\n",
      "batch 161, train_loss 0.036862,Time used 0.002991s\n",
      "batch 162, train_loss 0.037933,Time used 0.002993s\n",
      "batch 163, train_loss 0.037278,Time used 0.003988s\n",
      "batch 164, train_loss 0.039952,Time used 0.002993s\n",
      "batch 165, train_loss 0.039056,Time used 0.002993s\n",
      "batch 166, train_loss 0.039948,Time used 0.001992s\n",
      "batch 167, train_loss 0.041047,Time used 0.001995s\n",
      "batch 168, train_loss 0.035465,Time used 0.002992s\n",
      "batch 169, train_loss 0.036357,Time used 0.002993s\n",
      "batch 170, train_loss 0.041006,Time used 0.001994s\n",
      "batch 171, train_loss 0.035378,Time used 0.002993s\n",
      "batch 172, train_loss 0.043436,Time used 0.001994s\n",
      "batch 173, train_loss 0.031216,Time used 0.002993s\n",
      "batch 174, train_loss 0.035126,Time used 0.001994s\n",
      "batch 175, train_loss 0.033725,Time used 0.001995s\n",
      "batch 176, train_loss 0.036433,Time used 0.002992s\n",
      "batch 177, train_loss 0.035359,Time used 0.001995s\n",
      "batch 178, train_loss 0.039447,Time used 0.002992s\n",
      "batch 179, train_loss 0.042203,Time used 0.001995s\n",
      "batch 180, train_loss 0.032317,Time used 0.002993s\n",
      "batch 181, train_loss 0.040754,Time used 0.002992s\n",
      "batch 182, train_loss 0.040583,Time used 0.001994s\n",
      "batch 183, train_loss 0.030096,Time used 0.002992s\n",
      "batch 184, train_loss 0.039049,Time used 0.001993s\n",
      "batch 185, train_loss 0.031309,Time used 0.002993s\n",
      "batch 186, train_loss 0.038886,Time used 0.001994s\n",
      "batch 187, train_loss 0.039337,Time used 0.002994s\n",
      "batch 188, train_loss 0.039211,Time used 0.001993s\n",
      "batch 189, train_loss 0.031487,Time used 0.001995s\n",
      "batch 190, train_loss 0.036849,Time used 0.002992s\n",
      "batch 191, train_loss 0.032235,Time used 0.001994s\n",
      "batch 192, train_loss 0.035975,Time used 0.002992s\n",
      "batch 193, train_loss 0.036398,Time used 0.001994s\n",
      "batch 194, train_loss 0.032092,Time used 0.002993s\n",
      "batch 195, train_loss 0.033314,Time used 0.001994s\n",
      "batch 196, train_loss 0.035343,Time used 0.001994s\n",
      "batch 197, train_loss 0.029778,Time used 0.002992s\n",
      "batch 198, train_loss 0.039679,Time used 0.001995s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 199, train_loss 0.029854,Time used 0.002993s\n",
      "batch 200, train_loss 0.030477,Time used 0.002991s\n",
      "test_batch 200, test_rmse_loss 0.192065,test_mae_loss 0.165361,test_mape_loss 155.828662,Time used 0.005984s\n",
      "batch 201, train_loss 0.036995,Time used 0.001994s\n",
      "batch 202, train_loss 0.034630,Time used 0.001994s\n",
      "batch 203, train_loss 0.033481,Time used 0.001994s\n",
      "batch 204, train_loss 0.035987,Time used 0.002992s\n",
      "batch 205, train_loss 0.036723,Time used 0.001995s\n",
      "batch 206, train_loss 0.032260,Time used 0.002992s\n",
      "batch 207, train_loss 0.032080,Time used 0.001994s\n",
      "batch 208, train_loss 0.038406,Time used 0.002993s\n",
      "batch 209, train_loss 0.033647,Time used 0.001994s\n",
      "batch 210, train_loss 0.036475,Time used 0.002994s\n",
      "batch 211, train_loss 0.037707,Time used 0.002992s\n",
      "batch 212, train_loss 0.032117,Time used 0.002991s\n",
      "batch 213, train_loss 0.029856,Time used 0.002992s\n",
      "batch 214, train_loss 0.031663,Time used 0.002992s\n",
      "batch 215, train_loss 0.035184,Time used 0.002991s\n",
      "batch 216, train_loss 0.034465,Time used 0.001994s\n",
      "batch 217, train_loss 0.032293,Time used 0.002992s\n",
      "batch 218, train_loss 0.035594,Time used 0.001995s\n",
      "batch 219, train_loss 0.032091,Time used 0.002992s\n",
      "batch 220, train_loss 0.030451,Time used 0.001995s\n",
      "batch 221, train_loss 0.030284,Time used 0.002993s\n",
      "batch 222, train_loss 0.029989,Time used 0.001994s\n",
      "batch 223, train_loss 0.030999,Time used 0.002992s\n",
      "batch 224, train_loss 0.035553,Time used 0.001994s\n",
      "batch 225, train_loss 0.034931,Time used 0.002993s\n",
      "batch 226, train_loss 0.029827,Time used 0.002993s\n",
      "batch 227, train_loss 0.034842,Time used 0.003990s\n",
      "batch 228, train_loss 0.031986,Time used 0.002990s\n",
      "batch 229, train_loss 0.028520,Time used 0.002990s\n",
      "batch 230, train_loss 0.032892,Time used 0.001995s\n",
      "batch 231, train_loss 0.036532,Time used 0.002991s\n",
      "batch 232, train_loss 0.031044,Time used 0.002992s\n",
      "batch 233, train_loss 0.031220,Time used 0.002992s\n",
      "batch 234, train_loss 0.030088,Time used 0.002993s\n",
      "batch 235, train_loss 0.031730,Time used 0.001994s\n",
      "batch 236, train_loss 0.025455,Time used 0.001994s\n",
      "batch 237, train_loss 0.031797,Time used 0.001993s\n",
      "batch 238, train_loss 0.031527,Time used 0.001994s\n",
      "batch 239, train_loss 0.032225,Time used 0.002992s\n",
      "batch 240, train_loss 0.029790,Time used 0.001994s\n",
      "batch 241, train_loss 0.032110,Time used 0.001994s\n",
      "batch 242, train_loss 0.034711,Time used 0.002993s\n",
      "batch 243, train_loss 0.033308,Time used 0.002993s\n",
      "batch 244, train_loss 0.026670,Time used 0.002991s\n",
      "batch 245, train_loss 0.031590,Time used 0.002993s\n",
      "batch 246, train_loss 0.033428,Time used 0.001994s\n",
      "batch 247, train_loss 0.025958,Time used 0.001995s\n",
      "batch 248, train_loss 0.026382,Time used 0.001993s\n",
      "batch 249, train_loss 0.031033,Time used 0.001994s\n",
      "batch 250, train_loss 0.027200,Time used 0.002992s\n",
      "batch 251, train_loss 0.027674,Time used 0.001995s\n",
      "batch 252, train_loss 0.031160,Time used 0.002994s\n",
      "batch 253, train_loss 0.028232,Time used 0.002991s\n",
      "batch 254, train_loss 0.029041,Time used 0.002992s\n",
      "batch 255, train_loss 0.027354,Time used 0.002991s\n",
      "batch 256, train_loss 0.028259,Time used 0.002992s\n",
      "batch 257, train_loss 0.028183,Time used 0.001994s\n",
      "batch 258, train_loss 0.030100,Time used 0.002991s\n",
      "batch 259, train_loss 0.029837,Time used 0.002993s\n",
      "batch 260, train_loss 0.035309,Time used 0.002991s\n",
      "batch 261, train_loss 0.026672,Time used 0.002992s\n",
      "batch 262, train_loss 0.031813,Time used 0.001995s\n",
      "batch 263, train_loss 0.025789,Time used 0.002991s\n",
      "batch 264, train_loss 0.027000,Time used 0.002992s\n",
      "batch 265, train_loss 0.024820,Time used 0.002992s\n",
      "batch 266, train_loss 0.028100,Time used 0.002991s\n",
      "batch 267, train_loss 0.025418,Time used 0.002992s\n",
      "batch 268, train_loss 0.027126,Time used 0.001994s\n",
      "batch 269, train_loss 0.025310,Time used 0.002993s\n",
      "batch 270, train_loss 0.029138,Time used 0.001994s\n",
      "batch 271, train_loss 0.027375,Time used 0.002990s\n",
      "batch 272, train_loss 0.026276,Time used 0.001994s\n",
      "batch 273, train_loss 0.025904,Time used 0.002992s\n",
      "batch 274, train_loss 0.030984,Time used 0.001994s\n",
      "batch 275, train_loss 0.022996,Time used 0.002993s\n",
      "batch 276, train_loss 0.032812,Time used 0.002991s\n",
      "batch 277, train_loss 0.023364,Time used 0.002991s\n",
      "batch 278, train_loss 0.022888,Time used 0.001994s\n",
      "batch 279, train_loss 0.028670,Time used 0.001993s\n",
      "batch 280, train_loss 0.026340,Time used 0.002993s\n",
      "batch 281, train_loss 0.028111,Time used 0.003990s\n",
      "batch 282, train_loss 0.029175,Time used 0.002990s\n",
      "batch 283, train_loss 0.026116,Time used 0.002992s\n",
      "batch 284, train_loss 0.024187,Time used 0.002992s\n",
      "batch 285, train_loss 0.025279,Time used 0.001994s\n",
      "batch 286, train_loss 0.025563,Time used 0.002992s\n",
      "batch 287, train_loss 0.023702,Time used 0.002992s\n",
      "batch 288, train_loss 0.022231,Time used 0.001994s\n",
      "batch 289, train_loss 0.025897,Time used 0.002992s\n",
      "batch 290, train_loss 0.023913,Time used 0.001994s\n",
      "batch 291, train_loss 0.027857,Time used 0.001995s\n",
      "batch 292, train_loss 0.024039,Time used 0.002989s\n",
      "batch 293, train_loss 0.029125,Time used 0.002994s\n",
      "batch 294, train_loss 0.027354,Time used 0.001993s\n",
      "batch 295, train_loss 0.023777,Time used 0.001996s\n",
      "batch 296, train_loss 0.025286,Time used 0.002991s\n",
      "batch 297, train_loss 0.021382,Time used 0.002991s\n",
      "batch 298, train_loss 0.028224,Time used 0.002994s\n",
      "batch 299, train_loss 0.024627,Time used 0.002990s\n",
      "batch 300, train_loss 0.027566,Time used 0.002992s\n",
      "test_batch 300, test_rmse_loss 0.164023,test_mae_loss 0.134520,test_mape_loss 119.510380,Time used 0.005985s\n",
      "batch 301, train_loss 0.031388,Time used 0.002988s\n",
      "batch 302, train_loss 0.023565,Time used 0.001994s\n",
      "batch 303, train_loss 0.026867,Time used 0.001994s\n",
      "batch 304, train_loss 0.029522,Time used 0.001994s\n",
      "batch 305, train_loss 0.030788,Time used 0.002992s\n",
      "batch 306, train_loss 0.019214,Time used 0.001994s\n",
      "batch 307, train_loss 0.020880,Time used 0.002992s\n",
      "batch 308, train_loss 0.021454,Time used 0.002992s\n",
      "batch 309, train_loss 0.027126,Time used 0.001995s\n",
      "batch 310, train_loss 0.022024,Time used 0.002992s\n",
      "batch 311, train_loss 0.020702,Time used 0.001995s\n",
      "batch 312, train_loss 0.022075,Time used 0.002993s\n",
      "batch 313, train_loss 0.023790,Time used 0.001993s\n",
      "batch 314, train_loss 0.022754,Time used 0.001994s\n",
      "batch 315, train_loss 0.019880,Time used 0.002992s\n",
      "batch 316, train_loss 0.023666,Time used 0.002992s\n",
      "batch 317, train_loss 0.025569,Time used 0.001994s\n",
      "batch 318, train_loss 0.022549,Time used 0.002993s\n",
      "batch 319, train_loss 0.018850,Time used 0.002996s\n",
      "batch 320, train_loss 0.029558,Time used 0.002988s\n",
      "batch 321, train_loss 0.024759,Time used 0.001995s\n",
      "batch 322, train_loss 0.024127,Time used 0.002992s\n",
      "batch 323, train_loss 0.021170,Time used 0.001995s\n",
      "batch 324, train_loss 0.021891,Time used 0.002992s\n",
      "batch 325, train_loss 0.018295,Time used 0.001994s\n",
      "batch 326, train_loss 0.023830,Time used 0.001995s\n",
      "batch 327, train_loss 0.022429,Time used 0.002993s\n",
      "batch 328, train_loss 0.027864,Time used 0.002993s\n",
      "batch 329, train_loss 0.020640,Time used 0.003227s\n",
      "batch 330, train_loss 0.023502,Time used 0.002756s\n",
      "batch 331, train_loss 0.020498,Time used 0.002993s\n",
      "batch 332, train_loss 0.019683,Time used 0.001994s\n",
      "batch 333, train_loss 0.025698,Time used 0.002993s\n",
      "batch 334, train_loss 0.025150,Time used 0.001994s\n",
      "batch 335, train_loss 0.019610,Time used 0.001994s\n",
      "batch 336, train_loss 0.025062,Time used 0.002992s\n",
      "batch 337, train_loss 0.024202,Time used 0.002992s\n",
      "batch 338, train_loss 0.018749,Time used 0.001994s\n",
      "batch 339, train_loss 0.021992,Time used 0.002992s\n",
      "batch 340, train_loss 0.022998,Time used 0.001995s\n",
      "batch 341, train_loss 0.022593,Time used 0.002992s\n",
      "batch 342, train_loss 0.019373,Time used 0.001994s\n",
      "batch 343, train_loss 0.025561,Time used 0.002993s\n",
      "batch 344, train_loss 0.024851,Time used 0.001993s\n",
      "batch 345, train_loss 0.021201,Time used 0.001995s\n",
      "batch 346, train_loss 0.024842,Time used 0.002992s\n",
      "batch 347, train_loss 0.020654,Time used 0.001994s\n",
      "batch 348, train_loss 0.020663,Time used 0.002993s\n",
      "batch 349, train_loss 0.023832,Time used 0.001994s\n",
      "batch 350, train_loss 0.020750,Time used 0.002993s\n",
      "batch 351, train_loss 0.023361,Time used 0.001994s\n",
      "batch 352, train_loss 0.020735,Time used 0.001995s\n",
      "batch 353, train_loss 0.022470,Time used 0.002992s\n",
      "batch 354, train_loss 0.022256,Time used 0.002994s\n",
      "batch 355, train_loss 0.023518,Time used 0.002991s\n",
      "batch 356, train_loss 0.020719,Time used 0.002991s\n",
      "batch 357, train_loss 0.020807,Time used 0.002991s\n",
      "batch 358, train_loss 0.022517,Time used 0.001993s\n",
      "batch 359, train_loss 0.018119,Time used 0.001994s\n",
      "batch 360, train_loss 0.024062,Time used 0.001994s\n",
      "batch 361, train_loss 0.019911,Time used 0.002992s\n",
      "batch 362, train_loss 0.030128,Time used 0.002994s\n",
      "batch 363, train_loss 0.019353,Time used 0.002989s\n",
      "batch 364, train_loss 0.019626,Time used 0.001994s\n",
      "batch 365, train_loss 0.023388,Time used 0.001994s\n",
      "batch 366, train_loss 0.020683,Time used 0.002992s\n",
      "batch 367, train_loss 0.019170,Time used 0.002993s\n",
      "batch 368, train_loss 0.023353,Time used 0.002993s\n",
      "batch 369, train_loss 0.020465,Time used 0.001993s\n",
      "batch 370, train_loss 0.019513,Time used 0.001994s\n",
      "batch 371, train_loss 0.018259,Time used 0.002992s\n",
      "batch 372, train_loss 0.019109,Time used 0.001994s\n",
      "batch 373, train_loss 0.019663,Time used 0.002992s\n",
      "batch 374, train_loss 0.026486,Time used 0.002992s\n",
      "batch 375, train_loss 0.023858,Time used 0.002992s\n",
      "batch 376, train_loss 0.021190,Time used 0.001994s\n",
      "batch 377, train_loss 0.020592,Time used 0.003008s\n",
      "batch 378, train_loss 0.023394,Time used 0.003974s\n",
      "batch 379, train_loss 0.022644,Time used 0.002992s\n",
      "batch 380, train_loss 0.023761,Time used 0.001993s\n",
      "batch 381, train_loss 0.022406,Time used 0.002993s\n",
      "batch 382, train_loss 0.017854,Time used 0.002992s\n",
      "batch 383, train_loss 0.017596,Time used 0.003001s\n",
      "batch 384, train_loss 0.023265,Time used 0.002984s\n",
      "batch 385, train_loss 0.016543,Time used 0.002992s\n",
      "batch 386, train_loss 0.022847,Time used 0.003992s\n",
      "batch 387, train_loss 0.019519,Time used 0.002989s\n",
      "batch 388, train_loss 0.023206,Time used 0.001994s\n",
      "batch 389, train_loss 0.020670,Time used 0.003990s\n",
      "batch 390, train_loss 0.022545,Time used 0.002992s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 391, train_loss 0.023689,Time used 0.002990s\n",
      "batch 392, train_loss 0.022065,Time used 0.002991s\n",
      "batch 393, train_loss 0.021224,Time used 0.002992s\n",
      "batch 394, train_loss 0.021971,Time used 0.002993s\n",
      "batch 395, train_loss 0.021055,Time used 0.002993s\n",
      "batch 396, train_loss 0.016921,Time used 0.002991s\n",
      "batch 397, train_loss 0.022571,Time used 0.002990s\n",
      "batch 398, train_loss 0.018537,Time used 0.002992s\n",
      "batch 399, train_loss 0.022380,Time used 0.003081s\n",
      "batch 400, train_loss 0.020231,Time used 0.002903s\n",
      "test_batch 400, test_rmse_loss 0.152909,test_mae_loss 0.119357,test_mape_loss 89.122900,Time used 0.004983s\n",
      "batch 401, train_loss 0.025879,Time used 0.001994s\n",
      "batch 402, train_loss 0.023795,Time used 0.002992s\n",
      "batch 403, train_loss 0.016459,Time used 0.002992s\n",
      "batch 404, train_loss 0.020630,Time used 0.002991s\n",
      "batch 405, train_loss 0.015817,Time used 0.002992s\n",
      "batch 406, train_loss 0.023942,Time used 0.002992s\n",
      "batch 407, train_loss 0.020620,Time used 0.001994s\n",
      "batch 408, train_loss 0.020610,Time used 0.003012s\n",
      "batch 409, train_loss 0.018703,Time used 0.002999s\n",
      "batch 410, train_loss 0.024832,Time used 0.001987s\n",
      "batch 411, train_loss 0.017883,Time used 0.001994s\n",
      "batch 412, train_loss 0.022179,Time used 0.002992s\n",
      "batch 413, train_loss 0.017444,Time used 0.001995s\n",
      "batch 414, train_loss 0.023248,Time used 0.002992s\n",
      "batch 415, train_loss 0.017460,Time used 0.003250s\n",
      "batch 416, train_loss 0.018214,Time used 0.001736s\n",
      "batch 417, train_loss 0.019907,Time used 0.002992s\n",
      "batch 418, train_loss 0.024536,Time used 0.001994s\n",
      "batch 419, train_loss 0.022311,Time used 0.001994s\n",
      "batch 420, train_loss 0.016966,Time used 0.002992s\n",
      "batch 421, train_loss 0.018175,Time used 0.002992s\n",
      "batch 422, train_loss 0.019543,Time used 0.001994s\n",
      "batch 423, train_loss 0.021647,Time used 0.002992s\n",
      "batch 424, train_loss 0.020488,Time used 0.001994s\n",
      "batch 425, train_loss 0.020314,Time used 0.002993s\n",
      "batch 426, train_loss 0.021225,Time used 0.001994s\n",
      "batch 427, train_loss 0.022935,Time used 0.001993s\n",
      "batch 428, train_loss 0.018735,Time used 0.002992s\n",
      "batch 429, train_loss 0.023387,Time used 0.001995s\n",
      "batch 430, train_loss 0.018872,Time used 0.002992s\n",
      "batch 431, train_loss 0.020505,Time used 0.001994s\n",
      "batch 432, train_loss 0.022779,Time used 0.002992s\n",
      "batch 433, train_loss 0.021166,Time used 0.001995s\n",
      "batch 434, train_loss 0.018435,Time used 0.002992s\n",
      "batch 435, train_loss 0.016747,Time used 0.001995s\n",
      "batch 436, train_loss 0.018161,Time used 0.001994s\n",
      "batch 437, train_loss 0.016541,Time used 0.002992s\n",
      "batch 438, train_loss 0.019999,Time used 0.001995s\n",
      "batch 439, train_loss 0.017972,Time used 0.002992s\n",
      "batch 440, train_loss 0.019400,Time used 0.001995s\n",
      "batch 441, train_loss 0.021518,Time used 0.002992s\n",
      "batch 442, train_loss 0.018822,Time used 0.001995s\n",
      "batch 443, train_loss 0.020037,Time used 0.002992s\n",
      "batch 444, train_loss 0.015869,Time used 0.001994s\n",
      "batch 445, train_loss 0.023281,Time used 0.002993s\n",
      "batch 446, train_loss 0.021010,Time used 0.001993s\n",
      "batch 447, train_loss 0.022141,Time used 0.001995s\n",
      "batch 448, train_loss 0.021040,Time used 0.001993s\n",
      "batch 449, train_loss 0.023240,Time used 0.002993s\n",
      "batch 450, train_loss 0.022236,Time used 0.001994s\n",
      "batch 451, train_loss 0.017925,Time used 0.001994s\n",
      "batch 452, train_loss 0.018664,Time used 0.002992s\n",
      "batch 453, train_loss 0.017987,Time used 0.001994s\n",
      "batch 454, train_loss 0.023579,Time used 0.002993s\n",
      "batch 455, train_loss 0.019777,Time used 0.001994s\n",
      "batch 456, train_loss 0.020174,Time used 0.001994s\n",
      "batch 457, train_loss 0.016411,Time used 0.002994s\n",
      "batch 458, train_loss 0.020498,Time used 0.002991s\n",
      "batch 459, train_loss 0.018611,Time used 0.002992s\n",
      "batch 460, train_loss 0.017268,Time used 0.002992s\n",
      "batch 461, train_loss 0.024201,Time used 0.002991s\n",
      "batch 462, train_loss 0.019127,Time used 0.002994s\n",
      "batch 463, train_loss 0.018786,Time used 0.001994s\n",
      "batch 464, train_loss 0.022721,Time used 0.002992s\n",
      "batch 465, train_loss 0.021576,Time used 0.002992s\n",
      "batch 466, train_loss 0.022399,Time used 0.002991s\n",
      "batch 467, train_loss 0.018522,Time used 0.001995s\n",
      "batch 468, train_loss 0.017494,Time used 0.002992s\n",
      "batch 469, train_loss 0.018200,Time used 0.001995s\n",
      "batch 470, train_loss 0.022224,Time used 0.002992s\n",
      "batch 471, train_loss 0.020898,Time used 0.001994s\n",
      "batch 472, train_loss 0.021723,Time used 0.002992s\n",
      "batch 473, train_loss 0.022295,Time used 0.001994s\n",
      "batch 474, train_loss 0.019823,Time used 0.002993s\n",
      "batch 475, train_loss 0.017656,Time used 0.001993s\n",
      "batch 476, train_loss 0.018137,Time used 0.002993s\n",
      "batch 477, train_loss 0.021619,Time used 0.001994s\n",
      "batch 478, train_loss 0.019753,Time used 0.001995s\n",
      "batch 479, train_loss 0.016784,Time used 0.001994s\n",
      "batch 480, train_loss 0.015392,Time used 0.001995s\n",
      "batch 481, train_loss 0.017775,Time used 0.002993s\n",
      "batch 482, train_loss 0.018741,Time used 0.001994s\n",
      "batch 483, train_loss 0.015692,Time used 0.002992s\n",
      "batch 484, train_loss 0.017174,Time used 0.001994s\n",
      "batch 485, train_loss 0.021909,Time used 0.001995s\n",
      "batch 486, train_loss 0.016155,Time used 0.002992s\n",
      "batch 487, train_loss 0.019269,Time used 0.001995s\n",
      "batch 488, train_loss 0.022278,Time used 0.002992s\n",
      "batch 489, train_loss 0.016940,Time used 0.001995s\n",
      "batch 490, train_loss 0.026434,Time used 0.002992s\n",
      "batch 491, train_loss 0.018401,Time used 0.001995s\n",
      "batch 492, train_loss 0.019009,Time used 0.002992s\n",
      "batch 493, train_loss 0.017142,Time used 0.001995s\n",
      "batch 494, train_loss 0.021004,Time used 0.003032s\n",
      "batch 495, train_loss 0.021540,Time used 0.001954s\n",
      "batch 496, train_loss 0.023012,Time used 0.001994s\n",
      "batch 497, train_loss 0.020631,Time used 0.002992s\n",
      "batch 498, train_loss 0.015221,Time used 0.001995s\n",
      "batch 499, train_loss 0.023111,Time used 0.002992s\n",
      "batch 500, train_loss 0.020358,Time used 0.001994s\n",
      "test_batch 500, test_rmse_loss 0.145791,test_mae_loss 0.113570,test_mape_loss 84.313605,Time used 0.005984s\n",
      "batch 501, train_loss 0.018716,Time used 0.001994s\n",
      "batch 502, train_loss 0.015267,Time used 0.002993s\n",
      "batch 503, train_loss 0.018787,Time used 0.002992s\n",
      "batch 504, train_loss 0.016504,Time used 0.001995s\n",
      "batch 505, train_loss 0.019786,Time used 0.002992s\n",
      "batch 506, train_loss 0.023058,Time used 0.001994s\n",
      "batch 507, train_loss 0.018682,Time used 0.002992s\n",
      "batch 508, train_loss 0.016163,Time used 0.001994s\n",
      "batch 509, train_loss 0.017789,Time used 0.002992s\n",
      "batch 510, train_loss 0.018749,Time used 0.001994s\n",
      "batch 511, train_loss 0.017833,Time used 0.001996s\n",
      "batch 512, train_loss 0.017419,Time used 0.001993s\n",
      "batch 513, train_loss 0.018562,Time used 0.002992s\n",
      "batch 514, train_loss 0.021869,Time used 0.002992s\n",
      "batch 515, train_loss 0.018591,Time used 0.001995s\n",
      "batch 516, train_loss 0.016769,Time used 0.002992s\n",
      "batch 517, train_loss 0.018059,Time used 0.001995s\n",
      "batch 518, train_loss 0.023453,Time used 0.002992s\n",
      "batch 519, train_loss 0.023097,Time used 0.001995s\n",
      "batch 520, train_loss 0.017795,Time used 0.002992s\n",
      "batch 521, train_loss 0.019805,Time used 0.001994s\n",
      "batch 522, train_loss 0.015999,Time used 0.001995s\n",
      "batch 523, train_loss 0.015160,Time used 0.002992s\n",
      "batch 524, train_loss 0.020253,Time used 0.001995s\n",
      "batch 525, train_loss 0.017208,Time used 0.001994s\n",
      "batch 526, train_loss 0.017319,Time used 0.001994s\n",
      "batch 527, train_loss 0.020521,Time used 0.002993s\n",
      "batch 528, train_loss 0.016819,Time used 0.001993s\n",
      "batch 529, train_loss 0.015969,Time used 0.001995s\n",
      "batch 530, train_loss 0.017775,Time used 0.002991s\n",
      "batch 531, train_loss 0.013576,Time used 0.001995s\n",
      "batch 532, train_loss 0.019236,Time used 0.002993s\n",
      "batch 533, train_loss 0.017119,Time used 0.002995s\n",
      "batch 534, train_loss 0.019362,Time used 0.002989s\n",
      "batch 535, train_loss 0.019387,Time used 0.001995s\n",
      "batch 536, train_loss 0.020689,Time used 0.002992s\n",
      "batch 537, train_loss 0.018255,Time used 0.001995s\n",
      "batch 538, train_loss 0.017089,Time used 0.002992s\n",
      "batch 539, train_loss 0.024166,Time used 0.001994s\n",
      "batch 540, train_loss 0.021449,Time used 0.002992s\n",
      "batch 541, train_loss 0.018251,Time used 0.002992s\n",
      "batch 542, train_loss 0.015191,Time used 0.002992s\n",
      "batch 543, train_loss 0.019476,Time used 0.001995s\n",
      "batch 544, train_loss 0.018542,Time used 0.002992s\n",
      "batch 545, train_loss 0.018671,Time used 0.001995s\n",
      "batch 546, train_loss 0.021637,Time used 0.002992s\n",
      "batch 547, train_loss 0.016257,Time used 0.001995s\n",
      "batch 548, train_loss 0.020527,Time used 0.002992s\n",
      "batch 549, train_loss 0.023702,Time used 0.001994s\n",
      "batch 550, train_loss 0.016272,Time used 0.002993s\n",
      "batch 551, train_loss 0.019177,Time used 0.001994s\n",
      "batch 552, train_loss 0.013644,Time used 0.002993s\n",
      "batch 553, train_loss 0.014480,Time used 0.001993s\n",
      "batch 554, train_loss 0.019330,Time used 0.001994s\n",
      "batch 555, train_loss 0.019668,Time used 0.001993s\n",
      "batch 556, train_loss 0.017113,Time used 0.001994s\n",
      "batch 557, train_loss 0.017056,Time used 0.002993s\n",
      "batch 558, train_loss 0.018677,Time used 0.001994s\n",
      "batch 559, train_loss 0.019938,Time used 0.002994s\n",
      "batch 560, train_loss 0.014858,Time used 0.002991s\n",
      "batch 561, train_loss 0.015373,Time used 0.001993s\n",
      "batch 562, train_loss 0.016803,Time used 0.002993s\n",
      "batch 563, train_loss 0.016558,Time used 0.002992s\n",
      "batch 564, train_loss 0.018979,Time used 0.001995s\n",
      "batch 565, train_loss 0.020730,Time used 0.002992s\n",
      "batch 566, train_loss 0.020407,Time used 0.001995s\n",
      "batch 567, train_loss 0.016981,Time used 0.001993s\n",
      "batch 568, train_loss 0.015277,Time used 0.001994s\n",
      "batch 569, train_loss 0.020075,Time used 0.002992s\n",
      "batch 570, train_loss 0.021866,Time used 0.001994s\n",
      "batch 571, train_loss 0.016126,Time used 0.001995s\n",
      "batch 572, train_loss 0.018818,Time used 0.002993s\n",
      "batch 573, train_loss 0.020205,Time used 0.002991s\n",
      "batch 574, train_loss 0.015740,Time used 0.002992s\n",
      "batch 575, train_loss 0.020601,Time used 0.001995s\n",
      "batch 576, train_loss 0.016432,Time used 0.002992s\n",
      "batch 577, train_loss 0.021410,Time used 0.001994s\n",
      "batch 578, train_loss 0.015352,Time used 0.002993s\n",
      "batch 579, train_loss 0.018283,Time used 0.002995s\n",
      "batch 580, train_loss 0.018675,Time used 0.002989s\n",
      "batch 581, train_loss 0.016840,Time used 0.001994s\n",
      "batch 582, train_loss 0.018395,Time used 0.003991s\n",
      "batch 583, train_loss 0.019198,Time used 0.002991s\n",
      "batch 584, train_loss 0.013757,Time used 0.002995s\n",
      "batch 585, train_loss 0.016794,Time used 0.002990s\n",
      "batch 586, train_loss 0.019164,Time used 0.001993s\n",
      "batch 587, train_loss 0.019945,Time used 0.002992s\n",
      "batch 588, train_loss 0.016555,Time used 0.001994s\n",
      "batch 589, train_loss 0.019512,Time used 0.002993s\n",
      "batch 590, train_loss 0.014490,Time used 0.002992s\n",
      "batch 591, train_loss 0.017188,Time used 0.002992s\n",
      "batch 592, train_loss 0.018893,Time used 0.001994s\n",
      "batch 593, train_loss 0.017499,Time used 0.001995s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 594, train_loss 0.020962,Time used 0.003995s\n",
      "batch 595, train_loss 0.017866,Time used 0.002988s\n",
      "batch 596, train_loss 0.015164,Time used 0.003988s\n",
      "batch 597, train_loss 0.014367,Time used 0.002992s\n",
      "batch 598, train_loss 0.014678,Time used 0.001994s\n",
      "batch 599, train_loss 0.019067,Time used 0.002992s\n",
      "batch 600, train_loss 0.017676,Time used 0.002993s\n",
      "test_batch 600, test_rmse_loss 0.139438,test_mae_loss 0.107980,test_mape_loss 76.630548,Time used 0.006981s\n",
      "batch 601, train_loss 0.021587,Time used 0.001995s\n",
      "batch 602, train_loss 0.013083,Time used 0.002992s\n",
      "batch 603, train_loss 0.016401,Time used 0.003991s\n",
      "batch 604, train_loss 0.013844,Time used 0.002992s\n",
      "batch 605, train_loss 0.014573,Time used 0.002991s\n",
      "batch 606, train_loss 0.020604,Time used 0.001994s\n",
      "batch 607, train_loss 0.018291,Time used 0.002992s\n",
      "batch 608, train_loss 0.020638,Time used 0.001994s\n",
      "batch 609, train_loss 0.017353,Time used 0.002991s\n",
      "batch 610, train_loss 0.014816,Time used 0.002993s\n",
      "batch 611, train_loss 0.014842,Time used 0.001994s\n",
      "batch 612, train_loss 0.017799,Time used 0.002992s\n",
      "batch 613, train_loss 0.020784,Time used 0.001994s\n",
      "batch 614, train_loss 0.017428,Time used 0.002994s\n",
      "batch 615, train_loss 0.017962,Time used 0.002988s\n",
      "batch 616, train_loss 0.016255,Time used 0.002991s\n",
      "batch 617, train_loss 0.017481,Time used 0.002991s\n",
      "batch 618, train_loss 0.013990,Time used 0.001995s\n",
      "batch 619, train_loss 0.021588,Time used 0.002992s\n",
      "batch 620, train_loss 0.016461,Time used 0.002992s\n",
      "batch 621, train_loss 0.015306,Time used 0.002994s\n",
      "batch 622, train_loss 0.018410,Time used 0.002989s\n",
      "batch 623, train_loss 0.017534,Time used 0.002992s\n",
      "batch 624, train_loss 0.019050,Time used 0.001995s\n",
      "batch 625, train_loss 0.015274,Time used 0.002992s\n",
      "batch 626, train_loss 0.018717,Time used 0.002992s\n",
      "batch 627, train_loss 0.018231,Time used 0.002992s\n",
      "batch 628, train_loss 0.019512,Time used 0.002991s\n",
      "batch 629, train_loss 0.014668,Time used 0.001994s\n",
      "batch 630, train_loss 0.013168,Time used 0.002993s\n",
      "batch 631, train_loss 0.014081,Time used 0.003991s\n",
      "batch 632, train_loss 0.016441,Time used 0.002991s\n",
      "batch 633, train_loss 0.013107,Time used 0.002991s\n",
      "batch 634, train_loss 0.018475,Time used 0.002992s\n",
      "batch 635, train_loss 0.015854,Time used 0.001994s\n",
      "batch 636, train_loss 0.018184,Time used 0.001995s\n",
      "batch 637, train_loss 0.014330,Time used 0.002991s\n",
      "batch 638, train_loss 0.020318,Time used 0.002992s\n",
      "batch 639, train_loss 0.016621,Time used 0.001994s\n",
      "batch 640, train_loss 0.017694,Time used 0.002993s\n",
      "batch 641, train_loss 0.016487,Time used 0.001994s\n",
      "batch 642, train_loss 0.018580,Time used 0.002993s\n",
      "batch 643, train_loss 0.017197,Time used 0.001994s\n",
      "batch 644, train_loss 0.015803,Time used 0.002991s\n",
      "batch 645, train_loss 0.019910,Time used 0.001994s\n",
      "batch 646, train_loss 0.018208,Time used 0.001994s\n",
      "batch 647, train_loss 0.016817,Time used 0.002992s\n",
      "batch 648, train_loss 0.015803,Time used 0.001994s\n",
      "batch 649, train_loss 0.017047,Time used 0.002993s\n",
      "batch 650, train_loss 0.018824,Time used 0.001994s\n",
      "batch 651, train_loss 0.013743,Time used 0.001994s\n",
      "batch 652, train_loss 0.017494,Time used 0.001994s\n",
      "batch 653, train_loss 0.012739,Time used 0.001995s\n",
      "batch 654, train_loss 0.017480,Time used 0.001994s\n",
      "batch 655, train_loss 0.020288,Time used 0.001995s\n",
      "batch 656, train_loss 0.016594,Time used 0.001994s\n",
      "batch 657, train_loss 0.016070,Time used 0.001994s\n",
      "batch 658, train_loss 0.015249,Time used 0.001994s\n",
      "batch 659, train_loss 0.016718,Time used 0.002992s\n",
      "batch 660, train_loss 0.016592,Time used 0.002993s\n",
      "batch 661, train_loss 0.016969,Time used 0.002992s\n",
      "batch 662, train_loss 0.018903,Time used 0.002990s\n",
      "batch 663, train_loss 0.016192,Time used 0.002990s\n",
      "batch 664, train_loss 0.015811,Time used 0.001994s\n",
      "batch 665, train_loss 0.017253,Time used 0.002005s\n",
      "batch 666, train_loss 0.015213,Time used 0.002981s\n",
      "batch 667, train_loss 0.014399,Time used 0.002993s\n",
      "batch 668, train_loss 0.015656,Time used 0.002992s\n",
      "batch 669, train_loss 0.016447,Time used 0.002992s\n",
      "batch 670, train_loss 0.019047,Time used 0.001994s\n",
      "batch 671, train_loss 0.014076,Time used 0.002992s\n",
      "batch 672, train_loss 0.016415,Time used 0.001994s\n",
      "batch 673, train_loss 0.015971,Time used 0.002992s\n",
      "batch 674, train_loss 0.015594,Time used 0.002995s\n",
      "batch 675, train_loss 0.017575,Time used 0.002989s\n",
      "batch 676, train_loss 0.021797,Time used 0.001994s\n",
      "batch 677, train_loss 0.015505,Time used 0.002992s\n",
      "batch 678, train_loss 0.013598,Time used 0.001994s\n",
      "batch 679, train_loss 0.016964,Time used 0.002992s\n",
      "batch 680, train_loss 0.012009,Time used 0.002992s\n",
      "batch 681, train_loss 0.013414,Time used 0.002992s\n",
      "batch 682, train_loss 0.017334,Time used 0.001994s\n",
      "batch 683, train_loss 0.014991,Time used 0.001995s\n",
      "batch 684, train_loss 0.018543,Time used 0.001993s\n",
      "batch 685, train_loss 0.014292,Time used 0.002991s\n",
      "batch 686, train_loss 0.015955,Time used 0.002991s\n",
      "batch 687, train_loss 0.014841,Time used 0.002992s\n",
      "batch 688, train_loss 0.018224,Time used 0.002992s\n",
      "batch 689, train_loss 0.017645,Time used 0.001994s\n",
      "batch 690, train_loss 0.016855,Time used 0.001995s\n",
      "batch 691, train_loss 0.014618,Time used 0.002992s\n",
      "batch 692, train_loss 0.015652,Time used 0.002992s\n",
      "batch 693, train_loss 0.016023,Time used 0.002991s\n",
      "batch 694, train_loss 0.017936,Time used 0.001994s\n",
      "batch 695, train_loss 0.014097,Time used 0.002993s\n",
      "batch 696, train_loss 0.017117,Time used 0.001994s\n",
      "batch 697, train_loss 0.016768,Time used 0.001995s\n",
      "batch 698, train_loss 0.017355,Time used 0.002993s\n",
      "batch 699, train_loss 0.010914,Time used 0.002990s\n",
      "batch 700, train_loss 0.018539,Time used 0.002992s\n",
      "test_batch 700, test_rmse_loss 0.132969,test_mae_loss 0.102740,test_mape_loss 70.609880,Time used 0.004986s\n",
      "batch 701, train_loss 0.011964,Time used 0.002992s\n",
      "batch 702, train_loss 0.018079,Time used 0.002993s\n",
      "batch 703, train_loss 0.016904,Time used 0.001993s\n",
      "batch 704, train_loss 0.016221,Time used 0.001995s\n",
      "batch 705, train_loss 0.016369,Time used 0.001994s\n",
      "batch 706, train_loss 0.016887,Time used 0.001994s\n",
      "batch 707, train_loss 0.014765,Time used 0.002993s\n",
      "batch 708, train_loss 0.017365,Time used 0.002994s\n",
      "batch 709, train_loss 0.014555,Time used 0.002990s\n",
      "batch 710, train_loss 0.014468,Time used 0.001994s\n",
      "batch 711, train_loss 0.014715,Time used 0.001993s\n",
      "batch 712, train_loss 0.011757,Time used 0.002992s\n",
      "batch 713, train_loss 0.018538,Time used 0.002993s\n",
      "batch 714, train_loss 0.015674,Time used 0.002993s\n",
      "batch 715, train_loss 0.018285,Time used 0.002990s\n",
      "batch 716, train_loss 0.015910,Time used 0.001994s\n",
      "batch 717, train_loss 0.021345,Time used 0.002993s\n",
      "batch 718, train_loss 0.012075,Time used 0.001994s\n",
      "batch 719, train_loss 0.015091,Time used 0.002993s\n",
      "batch 720, train_loss 0.013532,Time used 0.002993s\n",
      "batch 721, train_loss 0.012895,Time used 0.002991s\n",
      "batch 722, train_loss 0.018174,Time used 0.002992s\n",
      "batch 723, train_loss 0.018436,Time used 0.001995s\n",
      "batch 724, train_loss 0.014735,Time used 0.002993s\n",
      "batch 725, train_loss 0.015808,Time used 0.002990s\n",
      "batch 726, train_loss 0.016965,Time used 0.002993s\n",
      "batch 727, train_loss 0.020321,Time used 0.002992s\n",
      "batch 728, train_loss 0.012998,Time used 0.002992s\n",
      "batch 729, train_loss 0.014525,Time used 0.002992s\n",
      "batch 730, train_loss 0.014104,Time used 0.002992s\n",
      "batch 731, train_loss 0.012491,Time used 0.002993s\n",
      "batch 732, train_loss 0.016049,Time used 0.002991s\n",
      "batch 733, train_loss 0.016168,Time used 0.001994s\n",
      "batch 734, train_loss 0.012202,Time used 0.001995s\n",
      "batch 735, train_loss 0.016735,Time used 0.002991s\n",
      "batch 736, train_loss 0.016189,Time used 0.002992s\n",
      "batch 737, train_loss 0.018808,Time used 0.001996s\n",
      "batch 738, train_loss 0.011422,Time used 0.002991s\n",
      "batch 739, train_loss 0.015946,Time used 0.001995s\n",
      "batch 740, train_loss 0.014621,Time used 0.002992s\n",
      "batch 741, train_loss 0.015952,Time used 0.001994s\n",
      "batch 742, train_loss 0.017095,Time used 0.001995s\n",
      "batch 743, train_loss 0.011630,Time used 0.002992s\n",
      "batch 744, train_loss 0.014097,Time used 0.002993s\n",
      "batch 745, train_loss 0.018115,Time used 0.001993s\n",
      "batch 746, train_loss 0.010844,Time used 0.002993s\n",
      "batch 747, train_loss 0.017000,Time used 0.002992s\n",
      "batch 748, train_loss 0.015862,Time used 0.001994s\n",
      "batch 749, train_loss 0.017056,Time used 0.002992s\n",
      "batch 750, train_loss 0.014834,Time used 0.001995s\n",
      "batch 751, train_loss 0.018231,Time used 0.002989s\n",
      "batch 752, train_loss 0.018770,Time used 0.002992s\n",
      "batch 753, train_loss 0.014192,Time used 0.001995s\n",
      "batch 754, train_loss 0.014265,Time used 0.002992s\n",
      "batch 755, train_loss 0.013481,Time used 0.002992s\n",
      "batch 756, train_loss 0.016509,Time used 0.001994s\n",
      "batch 757, train_loss 0.015618,Time used 0.001992s\n",
      "batch 758, train_loss 0.012168,Time used 0.002992s\n",
      "batch 759, train_loss 0.017783,Time used 0.002992s\n",
      "batch 760, train_loss 0.015149,Time used 0.001995s\n",
      "batch 761, train_loss 0.013343,Time used 0.001994s\n",
      "batch 762, train_loss 0.012382,Time used 0.001994s\n",
      "batch 763, train_loss 0.013840,Time used 0.002992s\n",
      "batch 764, train_loss 0.015192,Time used 0.001993s\n",
      "batch 765, train_loss 0.014806,Time used 0.002992s\n",
      "batch 766, train_loss 0.017661,Time used 0.002992s\n",
      "batch 767, train_loss 0.011129,Time used 0.001994s\n",
      "batch 768, train_loss 0.015557,Time used 0.001994s\n",
      "batch 769, train_loss 0.014205,Time used 0.002992s\n",
      "batch 770, train_loss 0.015310,Time used 0.001993s\n",
      "batch 771, train_loss 0.011557,Time used 0.002993s\n",
      "batch 772, train_loss 0.012118,Time used 0.001994s\n",
      "batch 773, train_loss 0.018081,Time used 0.001994s\n",
      "batch 774, train_loss 0.017400,Time used 0.001994s\n",
      "batch 775, train_loss 0.015168,Time used 0.002991s\n",
      "batch 776, train_loss 0.017720,Time used 0.002991s\n",
      "batch 777, train_loss 0.015226,Time used 0.001994s\n",
      "batch 778, train_loss 0.012761,Time used 0.002992s\n",
      "batch 779, train_loss 0.015123,Time used 0.002992s\n",
      "batch 780, train_loss 0.013329,Time used 0.001994s\n",
      "batch 781, train_loss 0.016063,Time used 0.002994s\n",
      "batch 782, train_loss 0.015405,Time used 0.002991s\n",
      "batch 783, train_loss 0.016637,Time used 0.002991s\n",
      "batch 784, train_loss 0.015456,Time used 0.002992s\n",
      "batch 785, train_loss 0.014703,Time used 0.001994s\n",
      "batch 786, train_loss 0.015839,Time used 0.003000s\n",
      "batch 787, train_loss 0.014115,Time used 0.002987s\n",
      "batch 788, train_loss 0.015519,Time used 0.002989s\n",
      "batch 789, train_loss 0.014941,Time used 0.002992s\n",
      "batch 790, train_loss 0.013774,Time used 0.001994s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 791, train_loss 0.013425,Time used 0.002993s\n",
      "batch 792, train_loss 0.014305,Time used 0.002990s\n",
      "batch 793, train_loss 0.012465,Time used 0.002992s\n",
      "batch 794, train_loss 0.012519,Time used 0.003989s\n",
      "batch 795, train_loss 0.014257,Time used 0.001994s\n",
      "batch 796, train_loss 0.017996,Time used 0.001995s\n",
      "batch 797, train_loss 0.014371,Time used 0.002992s\n",
      "batch 798, train_loss 0.015656,Time used 0.002993s\n",
      "batch 799, train_loss 0.013327,Time used 0.002991s\n",
      "batch 800, train_loss 0.012177,Time used 0.001994s\n",
      "test_batch 800, test_rmse_loss 0.126315,test_mae_loss 0.097638,test_mape_loss 65.849413,Time used 0.005985s\n",
      "batch 801, train_loss 0.012368,Time used 0.003011s\n",
      "batch 802, train_loss 0.015613,Time used 0.002973s\n",
      "batch 803, train_loss 0.012642,Time used 0.002991s\n",
      "batch 804, train_loss 0.017343,Time used 0.002992s\n",
      "batch 805, train_loss 0.013628,Time used 0.001994s\n",
      "batch 806, train_loss 0.011531,Time used 0.002992s\n",
      "batch 807, train_loss 0.014994,Time used 0.002992s\n",
      "batch 808, train_loss 0.014896,Time used 0.003990s\n",
      "batch 809, train_loss 0.014912,Time used 0.002992s\n",
      "batch 810, train_loss 0.014451,Time used 0.001994s\n",
      "batch 811, train_loss 0.014516,Time used 0.002075s\n",
      "batch 812, train_loss 0.011674,Time used 0.003270s\n",
      "batch 813, train_loss 0.012766,Time used 0.002990s\n",
      "batch 814, train_loss 0.014232,Time used 0.002993s\n",
      "batch 815, train_loss 0.013461,Time used 0.001994s\n",
      "batch 816, train_loss 0.015737,Time used 0.002993s\n",
      "batch 817, train_loss 0.014043,Time used 0.001993s\n",
      "batch 818, train_loss 0.016872,Time used 0.002991s\n",
      "batch 819, train_loss 0.015396,Time used 0.002995s\n",
      "batch 820, train_loss 0.012927,Time used 0.002990s\n",
      "batch 821, train_loss 0.013287,Time used 0.002992s\n",
      "batch 822, train_loss 0.016327,Time used 0.001995s\n",
      "batch 823, train_loss 0.013145,Time used 0.002992s\n",
      "batch 824, train_loss 0.014098,Time used 0.002992s\n",
      "batch 825, train_loss 0.014466,Time used 0.002992s\n",
      "batch 826, train_loss 0.013488,Time used 0.002992s\n",
      "batch 827, train_loss 0.014064,Time used 0.001993s\n",
      "batch 828, train_loss 0.016127,Time used 0.002993s\n",
      "batch 829, train_loss 0.014010,Time used 0.001994s\n",
      "batch 830, train_loss 0.013555,Time used 0.003991s\n",
      "batch 831, train_loss 0.011731,Time used 0.002992s\n",
      "batch 832, train_loss 0.013638,Time used 0.002991s\n",
      "batch 833, train_loss 0.016694,Time used 0.002992s\n",
      "batch 834, train_loss 0.013259,Time used 0.001994s\n",
      "batch 835, train_loss 0.013952,Time used 0.003996s\n",
      "batch 836, train_loss 0.014339,Time used 0.002986s\n",
      "batch 837, train_loss 0.011648,Time used 0.002992s\n",
      "batch 838, train_loss 0.013272,Time used 0.002992s\n",
      "batch 839, train_loss 0.015062,Time used 0.001995s\n",
      "batch 840, train_loss 0.015129,Time used 0.002992s\n",
      "batch 841, train_loss 0.014864,Time used 0.002992s\n",
      "batch 842, train_loss 0.014158,Time used 0.002992s\n",
      "batch 843, train_loss 0.012002,Time used 0.001994s\n",
      "batch 844, train_loss 0.014425,Time used 0.001995s\n",
      "batch 845, train_loss 0.012402,Time used 0.002992s\n",
      "batch 846, train_loss 0.012276,Time used 0.001994s\n",
      "batch 847, train_loss 0.015703,Time used 0.002992s\n",
      "batch 848, train_loss 0.011634,Time used 0.001994s\n",
      "batch 849, train_loss 0.017202,Time used 0.002992s\n",
      "batch 850, train_loss 0.009840,Time used 0.001994s\n",
      "batch 851, train_loss 0.011286,Time used 0.001995s\n",
      "batch 852, train_loss 0.013266,Time used 0.001993s\n",
      "batch 853, train_loss 0.014724,Time used 0.001995s\n",
      "batch 854, train_loss 0.017402,Time used 0.002994s\n",
      "batch 855, train_loss 0.015823,Time used 0.002043s\n",
      "batch 856, train_loss 0.013592,Time used 0.001968s\n",
      "batch 857, train_loss 0.014098,Time used 0.002992s\n",
      "batch 858, train_loss 0.015620,Time used 0.002991s\n",
      "batch 859, train_loss 0.012505,Time used 0.002992s\n",
      "batch 860, train_loss 0.013691,Time used 0.002992s\n",
      "batch 861, train_loss 0.010336,Time used 0.002992s\n",
      "batch 862, train_loss 0.014593,Time used 0.002991s\n",
      "batch 863, train_loss 0.011890,Time used 0.002992s\n",
      "batch 864, train_loss 0.013074,Time used 0.001994s\n",
      "batch 865, train_loss 0.015603,Time used 0.002992s\n",
      "batch 866, train_loss 0.013973,Time used 0.002992s\n",
      "batch 867, train_loss 0.011482,Time used 0.001994s\n",
      "batch 868, train_loss 0.018122,Time used 0.002992s\n",
      "batch 869, train_loss 0.012483,Time used 0.001994s\n",
      "batch 870, train_loss 0.013428,Time used 0.002993s\n",
      "batch 871, train_loss 0.017233,Time used 0.002992s\n",
      "batch 872, train_loss 0.012566,Time used 0.002992s\n",
      "batch 873, train_loss 0.010687,Time used 0.001994s\n",
      "batch 874, train_loss 0.013279,Time used 0.002992s\n",
      "batch 875, train_loss 0.012157,Time used 0.001994s\n",
      "batch 876, train_loss 0.011769,Time used 0.002993s\n",
      "batch 877, train_loss 0.013060,Time used 0.001994s\n",
      "batch 878, train_loss 0.013998,Time used 0.002993s\n",
      "batch 879, train_loss 0.013956,Time used 0.001994s\n",
      "batch 880, train_loss 0.013772,Time used 0.001994s\n",
      "batch 881, train_loss 0.015918,Time used 0.002993s\n",
      "batch 882, train_loss 0.011267,Time used 0.001994s\n",
      "batch 883, train_loss 0.014614,Time used 0.002992s\n",
      "batch 884, train_loss 0.014604,Time used 0.001994s\n",
      "batch 885, train_loss 0.012748,Time used 0.002993s\n",
      "batch 886, train_loss 0.016148,Time used 0.002992s\n",
      "batch 887, train_loss 0.011232,Time used 0.001994s\n",
      "batch 888, train_loss 0.017963,Time used 0.002992s\n",
      "batch 889, train_loss 0.009995,Time used 0.001995s\n",
      "batch 890, train_loss 0.012964,Time used 0.002992s\n",
      "batch 891, train_loss 0.013489,Time used 0.001994s\n",
      "batch 892, train_loss 0.014093,Time used 0.002993s\n",
      "batch 893, train_loss 0.013465,Time used 0.001993s\n",
      "batch 894, train_loss 0.012440,Time used 0.001968s\n",
      "batch 895, train_loss 0.013737,Time used 0.001994s\n",
      "batch 896, train_loss 0.012368,Time used 0.001995s\n",
      "batch 897, train_loss 0.013446,Time used 0.002991s\n",
      "batch 898, train_loss 0.013166,Time used 0.001996s\n",
      "batch 899, train_loss 0.011758,Time used 0.002992s\n",
      "batch 900, train_loss 0.012459,Time used 0.001994s\n",
      "test_batch 900, test_rmse_loss 0.120094,test_mae_loss 0.092826,test_mape_loss 60.916766,Time used 0.005985s\n",
      "batch 901, train_loss 0.016123,Time used 0.002991s\n",
      "batch 902, train_loss 0.012130,Time used 0.002993s\n",
      "batch 903, train_loss 0.013299,Time used 0.002991s\n",
      "batch 904, train_loss 0.014441,Time used 0.001994s\n",
      "batch 905, train_loss 0.012909,Time used 0.003019s\n",
      "batch 906, train_loss 0.013446,Time used 0.002965s\n",
      "batch 907, train_loss 0.014185,Time used 0.002995s\n",
      "batch 908, train_loss 0.012842,Time used 0.003988s\n",
      "batch 909, train_loss 0.011142,Time used 0.002991s\n",
      "batch 910, train_loss 0.011945,Time used 0.001994s\n",
      "batch 911, train_loss 0.015001,Time used 0.001994s\n",
      "batch 912, train_loss 0.013348,Time used 0.002992s\n",
      "batch 913, train_loss 0.013108,Time used 0.002993s\n",
      "batch 914, train_loss 0.012470,Time used 0.003990s\n",
      "batch 915, train_loss 0.009637,Time used 0.002991s\n",
      "batch 916, train_loss 0.011347,Time used 0.003003s\n",
      "batch 917, train_loss 0.010991,Time used 0.002980s\n",
      "batch 918, train_loss 0.013990,Time used 0.002998s\n",
      "batch 919, train_loss 0.012461,Time used 0.002987s\n",
      "batch 920, train_loss 0.012261,Time used 0.002990s\n",
      "batch 921, train_loss 0.014318,Time used 0.002991s\n",
      "batch 922, train_loss 0.015420,Time used 0.001994s\n",
      "batch 923, train_loss 0.012619,Time used 0.002992s\n",
      "batch 924, train_loss 0.010148,Time used 0.002992s\n",
      "batch 925, train_loss 0.014118,Time used 0.002992s\n",
      "batch 926, train_loss 0.013716,Time used 0.002990s\n",
      "batch 927, train_loss 0.013855,Time used 0.002993s\n",
      "batch 928, train_loss 0.014997,Time used 0.001994s\n",
      "batch 929, train_loss 0.012100,Time used 0.002994s\n",
      "batch 930, train_loss 0.011157,Time used 0.002992s\n",
      "batch 931, train_loss 0.012195,Time used 0.002992s\n",
      "batch 932, train_loss 0.015169,Time used 0.002990s\n",
      "batch 933, train_loss 0.013485,Time used 0.002993s\n",
      "batch 934, train_loss 0.013792,Time used 0.001994s\n",
      "batch 935, train_loss 0.017206,Time used 0.003991s\n",
      "batch 936, train_loss 0.013156,Time used 0.002990s\n",
      "batch 937, train_loss 0.011428,Time used 0.002992s\n",
      "batch 938, train_loss 0.011873,Time used 0.001994s\n",
      "batch 939, train_loss 0.009393,Time used 0.002992s\n",
      "batch 940, train_loss 0.012862,Time used 0.002991s\n",
      "batch 941, train_loss 0.010985,Time used 0.002994s\n",
      "batch 942, train_loss 0.013760,Time used 0.002991s\n",
      "batch 943, train_loss 0.012070,Time used 0.002995s\n",
      "batch 944, train_loss 0.010050,Time used 0.002989s\n",
      "batch 945, train_loss 0.013845,Time used 0.001995s\n",
      "batch 946, train_loss 0.011405,Time used 0.003003s\n",
      "batch 947, train_loss 0.014533,Time used 0.002983s\n",
      "batch 948, train_loss 0.012989,Time used 0.002990s\n",
      "batch 949, train_loss 0.011197,Time used 0.002992s\n",
      "batch 950, train_loss 0.010832,Time used 0.001993s\n",
      "batch 951, train_loss 0.014154,Time used 0.001995s\n",
      "batch 952, train_loss 0.013522,Time used 0.002992s\n",
      "batch 953, train_loss 0.015857,Time used 0.002993s\n",
      "batch 954, train_loss 0.014723,Time used 0.001994s\n",
      "batch 955, train_loss 0.012266,Time used 0.002992s\n",
      "batch 956, train_loss 0.009940,Time used 0.001994s\n",
      "batch 957, train_loss 0.012108,Time used 0.002992s\n",
      "batch 958, train_loss 0.012466,Time used 0.002993s\n",
      "batch 959, train_loss 0.010785,Time used 0.001993s\n",
      "batch 960, train_loss 0.012130,Time used 0.001995s\n",
      "batch 961, train_loss 0.010563,Time used 0.002992s\n",
      "batch 962, train_loss 0.011449,Time used 0.001994s\n",
      "batch 963, train_loss 0.014379,Time used 0.002993s\n",
      "batch 964, train_loss 0.014741,Time used 0.002993s\n",
      "batch 965, train_loss 0.012010,Time used 0.001994s\n",
      "batch 966, train_loss 0.011742,Time used 0.002992s\n",
      "batch 967, train_loss 0.010229,Time used 0.001995s\n",
      "batch 968, train_loss 0.015633,Time used 0.002992s\n",
      "batch 969, train_loss 0.011825,Time used 0.001995s\n",
      "batch 970, train_loss 0.014842,Time used 0.002992s\n",
      "batch 971, train_loss 0.012401,Time used 0.001994s\n",
      "batch 972, train_loss 0.011148,Time used 0.002993s\n",
      "batch 973, train_loss 0.009514,Time used 0.001994s\n",
      "batch 974, train_loss 0.011209,Time used 0.002993s\n",
      "batch 975, train_loss 0.015017,Time used 0.001994s\n",
      "batch 976, train_loss 0.015289,Time used 0.001994s\n",
      "batch 977, train_loss 0.013515,Time used 0.001995s\n",
      "batch 978, train_loss 0.009724,Time used 0.001994s\n",
      "batch 979, train_loss 0.010063,Time used 0.001995s\n",
      "batch 980, train_loss 0.013990,Time used 0.002992s\n",
      "batch 981, train_loss 0.013325,Time used 0.001995s\n",
      "batch 982, train_loss 0.012734,Time used 0.002992s\n",
      "batch 983, train_loss 0.011464,Time used 0.001993s\n",
      "batch 984, train_loss 0.013484,Time used 0.001992s\n",
      "batch 985, train_loss 0.012252,Time used 0.001995s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 986, train_loss 0.011873,Time used 0.001994s\n",
      "batch 987, train_loss 0.013205,Time used 0.002994s\n",
      "batch 988, train_loss 0.010574,Time used 0.002990s\n",
      "batch 989, train_loss 0.011205,Time used 0.002992s\n",
      "batch 990, train_loss 0.011637,Time used 0.002992s\n",
      "batch 991, train_loss 0.014481,Time used 0.002993s\n",
      "batch 992, train_loss 0.010187,Time used 0.002991s\n",
      "batch 993, train_loss 0.011974,Time used 0.002991s\n",
      "batch 994, train_loss 0.012892,Time used 0.002994s\n",
      "batch 995, train_loss 0.012567,Time used 0.002991s\n",
      "batch 996, train_loss 0.009682,Time used 0.002987s\n",
      "batch 997, train_loss 0.011776,Time used 0.002992s\n",
      "batch 998, train_loss 0.014570,Time used 0.001993s\n",
      "batch 999, train_loss 0.010918,Time used 0.002993s\n",
      "batch 1000, train_loss 0.013882,Time used 0.002992s\n",
      "test_batch 1000, test_rmse_loss 0.115120,test_mae_loss 0.088506,test_mape_loss 54.867626,Time used 0.005984s\n",
      "batch 1001, train_loss 0.011198,Time used 0.002993s\n",
      "batch 1002, train_loss 0.014634,Time used 0.001994s\n",
      "batch 1003, train_loss 0.009685,Time used 0.003019s\n",
      "batch 1004, train_loss 0.011481,Time used 0.002965s\n",
      "batch 1005, train_loss 0.012595,Time used 0.002994s\n",
      "batch 1006, train_loss 0.011947,Time used 0.002990s\n",
      "batch 1007, train_loss 0.012953,Time used 0.002992s\n",
      "batch 1008, train_loss 0.010000,Time used 0.001994s\n",
      "batch 1009, train_loss 0.012637,Time used 0.002992s\n",
      "batch 1010, train_loss 0.012548,Time used 0.002992s\n",
      "batch 1011, train_loss 0.008647,Time used 0.002992s\n",
      "batch 1012, train_loss 0.014378,Time used 0.002991s\n",
      "batch 1013, train_loss 0.011390,Time used 0.001994s\n",
      "batch 1014, train_loss 0.013175,Time used 0.001995s\n",
      "batch 1015, train_loss 0.009674,Time used 0.001994s\n",
      "batch 1016, train_loss 0.012121,Time used 0.002992s\n",
      "batch 1017, train_loss 0.012731,Time used 0.002992s\n",
      "batch 1018, train_loss 0.009698,Time used 0.002992s\n",
      "batch 1019, train_loss 0.017055,Time used 0.001994s\n",
      "batch 1020, train_loss 0.011383,Time used 0.001994s\n",
      "batch 1021, train_loss 0.012862,Time used 0.002992s\n",
      "batch 1022, train_loss 0.011440,Time used 0.002992s\n",
      "batch 1023, train_loss 0.011047,Time used 0.001993s\n",
      "batch 1024, train_loss 0.012275,Time used 0.001994s\n",
      "batch 1025, train_loss 0.014469,Time used 0.002993s\n",
      "batch 1026, train_loss 0.010199,Time used 0.001995s\n",
      "batch 1027, train_loss 0.010308,Time used 0.002992s\n",
      "batch 1028, train_loss 0.012105,Time used 0.002991s\n",
      "batch 1029, train_loss 0.012270,Time used 0.002993s\n",
      "batch 1030, train_loss 0.009417,Time used 0.002992s\n",
      "batch 1031, train_loss 0.009106,Time used 0.001995s\n",
      "batch 1032, train_loss 0.010378,Time used 0.002992s\n",
      "batch 1033, train_loss 0.012933,Time used 0.001995s\n",
      "batch 1034, train_loss 0.013843,Time used 0.002990s\n",
      "batch 1035, train_loss 0.016015,Time used 0.002994s\n",
      "batch 1036, train_loss 0.014001,Time used 0.001992s\n",
      "batch 1037, train_loss 0.012267,Time used 0.001994s\n",
      "batch 1038, train_loss 0.010155,Time used 0.001995s\n",
      "batch 1039, train_loss 0.010843,Time used 0.002993s\n",
      "batch 1040, train_loss 0.011594,Time used 0.002991s\n",
      "batch 1041, train_loss 0.010557,Time used 0.001994s\n",
      "batch 1042, train_loss 0.010465,Time used 0.001995s\n",
      "batch 1043, train_loss 0.012618,Time used 0.001994s\n",
      "batch 1044, train_loss 0.012187,Time used 0.001994s\n",
      "batch 1045, train_loss 0.012969,Time used 0.002992s\n",
      "batch 1046, train_loss 0.014296,Time used 0.001994s\n",
      "batch 1047, train_loss 0.010583,Time used 0.002992s\n",
      "batch 1048, train_loss 0.011398,Time used 0.001994s\n",
      "batch 1049, train_loss 0.011521,Time used 0.002993s\n",
      "batch 1050, train_loss 0.011530,Time used 0.002994s\n",
      "batch 1051, train_loss 0.010793,Time used 0.002990s\n",
      "batch 1052, train_loss 0.011914,Time used 0.002992s\n",
      "batch 1053, train_loss 0.012030,Time used 0.002991s\n",
      "batch 1054, train_loss 0.011404,Time used 0.002993s\n",
      "batch 1055, train_loss 0.013136,Time used 0.002992s\n",
      "batch 1056, train_loss 0.011673,Time used 0.001995s\n",
      "batch 1057, train_loss 0.013373,Time used 0.002993s\n",
      "batch 1058, train_loss 0.009269,Time used 0.001993s\n",
      "batch 1059, train_loss 0.011596,Time used 0.001994s\n",
      "batch 1060, train_loss 0.011821,Time used 0.002992s\n",
      "batch 1061, train_loss 0.012210,Time used 0.001995s\n",
      "batch 1062, train_loss 0.009635,Time used 0.002992s\n",
      "batch 1063, train_loss 0.011773,Time used 0.001994s\n",
      "batch 1064, train_loss 0.013968,Time used 0.002992s\n",
      "batch 1065, train_loss 0.010585,Time used 0.002992s\n",
      "batch 1066, train_loss 0.011171,Time used 0.001993s\n",
      "batch 1067, train_loss 0.010261,Time used 0.002993s\n",
      "batch 1068, train_loss 0.012144,Time used 0.001993s\n",
      "batch 1069, train_loss 0.012437,Time used 0.001995s\n",
      "batch 1070, train_loss 0.013063,Time used 0.002992s\n",
      "batch 1071, train_loss 0.012743,Time used 0.001994s\n",
      "batch 1072, train_loss 0.009057,Time used 0.002992s\n",
      "batch 1073, train_loss 0.009900,Time used 0.001994s\n",
      "batch 1074, train_loss 0.013611,Time used 0.002993s\n",
      "batch 1075, train_loss 0.011285,Time used 0.001994s\n",
      "batch 1076, train_loss 0.014706,Time used 0.001995s\n",
      "batch 1077, train_loss 0.012357,Time used 0.002992s\n",
      "batch 1078, train_loss 0.008941,Time used 0.001995s\n",
      "batch 1079, train_loss 0.011075,Time used 0.001994s\n",
      "batch 1080, train_loss 0.010613,Time used 0.001994s\n",
      "batch 1081, train_loss 0.007772,Time used 0.002992s\n",
      "batch 1082, train_loss 0.014639,Time used 0.001994s\n",
      "batch 1083, train_loss 0.009662,Time used 0.001995s\n",
      "batch 1084, train_loss 0.011072,Time used 0.001994s\n",
      "batch 1085, train_loss 0.012552,Time used 0.001995s\n",
      "batch 1086, train_loss 0.013223,Time used 0.002992s\n",
      "batch 1087, train_loss 0.009740,Time used 0.001994s\n",
      "batch 1088, train_loss 0.014011,Time used 0.002992s\n",
      "batch 1089, train_loss 0.012736,Time used 0.001995s\n",
      "batch 1090, train_loss 0.009855,Time used 0.002992s\n",
      "batch 1091, train_loss 0.009473,Time used 0.001994s\n",
      "batch 1092, train_loss 0.012775,Time used 0.002992s\n",
      "batch 1093, train_loss 0.012735,Time used 0.001994s\n",
      "batch 1094, train_loss 0.010540,Time used 0.002992s\n",
      "batch 1095, train_loss 0.011269,Time used 0.001994s\n",
      "batch 1096, train_loss 0.012157,Time used 0.001994s\n",
      "batch 1097, train_loss 0.014305,Time used 0.002992s\n",
      "batch 1098, train_loss 0.011866,Time used 0.001995s\n",
      "batch 1099, train_loss 0.010135,Time used 0.002992s\n",
      "batch 1100, train_loss 0.011099,Time used 0.001995s\n",
      "test_batch 1100, test_rmse_loss 0.110528,test_mae_loss 0.085231,test_mape_loss 52.416708,Time used 0.005984s\n",
      "batch 1101, train_loss 0.011457,Time used 0.001995s\n",
      "batch 1102, train_loss 0.012549,Time used 0.002993s\n",
      "batch 1103, train_loss 0.010398,Time used 0.002991s\n",
      "batch 1104, train_loss 0.010060,Time used 0.001994s\n",
      "batch 1105, train_loss 0.011609,Time used 0.002992s\n",
      "batch 1106, train_loss 0.012637,Time used 0.001995s\n",
      "batch 1107, train_loss 0.011135,Time used 0.002992s\n",
      "batch 1108, train_loss 0.008431,Time used 0.001994s\n",
      "batch 1109, train_loss 0.011790,Time used 0.002992s\n",
      "batch 1110, train_loss 0.010898,Time used 0.001994s\n",
      "batch 1111, train_loss 0.014282,Time used 0.001995s\n",
      "batch 1112, train_loss 0.009799,Time used 0.002991s\n",
      "batch 1113, train_loss 0.010006,Time used 0.002995s\n",
      "batch 1114, train_loss 0.008545,Time used 0.002988s\n",
      "batch 1115, train_loss 0.012683,Time used 0.002991s\n",
      "batch 1116, train_loss 0.011947,Time used 0.001994s\n",
      "batch 1117, train_loss 0.010289,Time used 0.001994s\n",
      "batch 1118, train_loss 0.011403,Time used 0.002993s\n",
      "batch 1119, train_loss 0.010336,Time used 0.002997s\n",
      "batch 1120, train_loss 0.010091,Time used 0.002988s\n",
      "batch 1121, train_loss 0.010195,Time used 0.002990s\n",
      "batch 1122, train_loss 0.008136,Time used 0.002992s\n",
      "batch 1123, train_loss 0.010760,Time used 0.002992s\n",
      "batch 1124, train_loss 0.015810,Time used 0.001994s\n",
      "batch 1125, train_loss 0.015164,Time used 0.003991s\n",
      "batch 1126, train_loss 0.010860,Time used 0.002992s\n",
      "batch 1127, train_loss 0.010644,Time used 0.002993s\n",
      "batch 1128, train_loss 0.010071,Time used 0.001992s\n",
      "batch 1129, train_loss 0.010250,Time used 0.001994s\n",
      "batch 1130, train_loss 0.013202,Time used 0.002995s\n",
      "batch 1131, train_loss 0.010650,Time used 0.002992s\n",
      "batch 1132, train_loss 0.013260,Time used 0.001992s\n",
      "batch 1133, train_loss 0.010692,Time used 0.001994s\n",
      "batch 1134, train_loss 0.012513,Time used 0.002992s\n",
      "batch 1135, train_loss 0.011218,Time used 0.001994s\n",
      "batch 1136, train_loss 0.010243,Time used 0.002993s\n",
      "batch 1137, train_loss 0.010438,Time used 0.002995s\n",
      "batch 1138, train_loss 0.010498,Time used 0.002988s\n",
      "batch 1139, train_loss 0.012681,Time used 0.002992s\n",
      "batch 1140, train_loss 0.010840,Time used 0.001994s\n",
      "batch 1141, train_loss 0.010890,Time used 0.001995s\n",
      "batch 1142, train_loss 0.013077,Time used 0.003990s\n",
      "batch 1143, train_loss 0.009394,Time used 0.002992s\n",
      "batch 1144, train_loss 0.009890,Time used 0.002991s\n",
      "batch 1145, train_loss 0.011970,Time used 0.001994s\n",
      "batch 1146, train_loss 0.012240,Time used 0.001994s\n",
      "batch 1147, train_loss 0.013612,Time used 0.002994s\n",
      "batch 1148, train_loss 0.010768,Time used 0.003142s\n",
      "batch 1149, train_loss 0.012274,Time used 0.002506s\n",
      "batch 1150, train_loss 0.011506,Time used 0.003501s\n",
      "batch 1151, train_loss 0.008014,Time used 0.001995s\n",
      "batch 1152, train_loss 0.010976,Time used 0.002992s\n",
      "batch 1153, train_loss 0.010933,Time used 0.002992s\n",
      "batch 1154, train_loss 0.010614,Time used 0.002994s\n",
      "batch 1155, train_loss 0.011030,Time used 0.002990s\n",
      "batch 1156, train_loss 0.013342,Time used 0.002992s\n",
      "batch 1157, train_loss 0.010422,Time used 0.001995s\n",
      "batch 1158, train_loss 0.010398,Time used 0.001994s\n",
      "batch 1159, train_loss 0.009615,Time used 0.002993s\n",
      "batch 1160, train_loss 0.010372,Time used 0.002992s\n",
      "batch 1161, train_loss 0.010936,Time used 0.002992s\n",
      "batch 1162, train_loss 0.011873,Time used 0.001994s\n",
      "batch 1163, train_loss 0.013036,Time used 0.002992s\n",
      "batch 1164, train_loss 0.011190,Time used 0.001995s\n",
      "batch 1165, train_loss 0.013008,Time used 0.002991s\n",
      "batch 1166, train_loss 0.009305,Time used 0.002991s\n",
      "batch 1167, train_loss 0.011607,Time used 0.001994s\n",
      "batch 1168, train_loss 0.008719,Time used 0.002992s\n",
      "batch 1169, train_loss 0.012564,Time used 0.001994s\n",
      "batch 1170, train_loss 0.009426,Time used 0.002991s\n",
      "batch 1171, train_loss 0.010391,Time used 0.002992s\n",
      "batch 1172, train_loss 0.011354,Time used 0.002993s\n",
      "batch 1173, train_loss 0.010709,Time used 0.001994s\n",
      "batch 1174, train_loss 0.012023,Time used 0.002992s\n",
      "batch 1175, train_loss 0.009537,Time used 0.001994s\n",
      "batch 1176, train_loss 0.011656,Time used 0.002993s\n",
      "batch 1177, train_loss 0.011670,Time used 0.001994s\n",
      "batch 1178, train_loss 0.011410,Time used 0.002993s\n",
      "batch 1179, train_loss 0.011831,Time used 0.001993s\n",
      "batch 1180, train_loss 0.013096,Time used 0.001995s\n",
      "batch 1181, train_loss 0.012468,Time used 0.001993s\n",
      "batch 1182, train_loss 0.010496,Time used 0.002993s\n",
      "batch 1183, train_loss 0.008879,Time used 0.002992s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1184, train_loss 0.009609,Time used 0.001993s\n",
      "batch 1185, train_loss 0.009714,Time used 0.001992s\n",
      "batch 1186, train_loss 0.009840,Time used 0.001994s\n",
      "batch 1187, train_loss 0.010473,Time used 0.001995s\n",
      "batch 1188, train_loss 0.009519,Time used 0.002992s\n",
      "batch 1189, train_loss 0.010922,Time used 0.002994s\n",
      "batch 1190, train_loss 0.011831,Time used 0.002992s\n",
      "batch 1191, train_loss 0.010457,Time used 0.002992s\n",
      "batch 1192, train_loss 0.009243,Time used 0.002991s\n",
      "batch 1193, train_loss 0.012146,Time used 0.002992s\n",
      "batch 1194, train_loss 0.010634,Time used 0.001995s\n",
      "batch 1195, train_loss 0.011044,Time used 0.001992s\n",
      "batch 1196, train_loss 0.009623,Time used 0.001995s\n",
      "batch 1197, train_loss 0.013239,Time used 0.002993s\n",
      "batch 1198, train_loss 0.011613,Time used 0.002991s\n",
      "batch 1199, train_loss 0.012113,Time used 0.001994s\n",
      "batch 1200, train_loss 0.011415,Time used 0.002992s\n",
      "test_batch 1200, test_rmse_loss 0.107483,test_mae_loss 0.082993,test_mape_loss 50.269482,Time used 0.006982s\n",
      "batch 1201, train_loss 0.010391,Time used 0.003011s\n",
      "batch 1202, train_loss 0.010848,Time used 0.002972s\n",
      "batch 1203, train_loss 0.013140,Time used 0.002992s\n",
      "batch 1204, train_loss 0.009040,Time used 0.002992s\n",
      "batch 1205, train_loss 0.012301,Time used 0.003001s\n",
      "batch 1206, train_loss 0.010980,Time used 0.002982s\n",
      "batch 1207, train_loss 0.011594,Time used 0.002992s\n",
      "batch 1208, train_loss 0.010766,Time used 0.001995s\n",
      "batch 1209, train_loss 0.011128,Time used 0.002993s\n",
      "batch 1210, train_loss 0.007961,Time used 0.002991s\n",
      "batch 1211, train_loss 0.011420,Time used 0.003990s\n",
      "batch 1212, train_loss 0.011115,Time used 0.001994s\n",
      "batch 1213, train_loss 0.012030,Time used 0.002993s\n",
      "batch 1214, train_loss 0.009559,Time used 0.001994s\n",
      "batch 1215, train_loss 0.011436,Time used 0.002993s\n",
      "batch 1216, train_loss 0.010754,Time used 0.002989s\n",
      "batch 1217, train_loss 0.009109,Time used 0.002992s\n",
      "batch 1218, train_loss 0.008543,Time used 0.002991s\n",
      "batch 1219, train_loss 0.012262,Time used 0.001994s\n",
      "batch 1220, train_loss 0.010214,Time used 0.002993s\n",
      "batch 1221, train_loss 0.011676,Time used 0.002991s\n",
      "batch 1222, train_loss 0.010850,Time used 0.002993s\n",
      "batch 1223, train_loss 0.009932,Time used 0.002992s\n",
      "batch 1224, train_loss 0.014280,Time used 0.003021s\n",
      "batch 1225, train_loss 0.012163,Time used 0.001965s\n",
      "batch 1226, train_loss 0.012285,Time used 0.001994s\n",
      "batch 1227, train_loss 0.008608,Time used 0.002993s\n",
      "batch 1228, train_loss 0.009179,Time used 0.002993s\n",
      "batch 1229, train_loss 0.011700,Time used 0.001994s\n",
      "batch 1230, train_loss 0.011239,Time used 0.002992s\n",
      "batch 1231, train_loss 0.009861,Time used 0.002992s\n",
      "batch 1232, train_loss 0.010413,Time used 0.001994s\n",
      "batch 1233, train_loss 0.011702,Time used 0.002992s\n",
      "batch 1234, train_loss 0.013297,Time used 0.001996s\n",
      "batch 1235, train_loss 0.009295,Time used 0.001994s\n",
      "batch 1236, train_loss 0.012050,Time used 0.002992s\n",
      "batch 1237, train_loss 0.010504,Time used 0.001994s\n",
      "batch 1238, train_loss 0.011507,Time used 0.002992s\n",
      "batch 1239, train_loss 0.008346,Time used 0.002993s\n",
      "batch 1240, train_loss 0.011558,Time used 0.002993s\n",
      "batch 1241, train_loss 0.012711,Time used 0.002990s\n",
      "batch 1242, train_loss 0.010752,Time used 0.001994s\n",
      "batch 1243, train_loss 0.009204,Time used 0.001995s\n",
      "batch 1244, train_loss 0.008564,Time used 0.001993s\n",
      "batch 1245, train_loss 0.012123,Time used 0.002991s\n",
      "batch 1246, train_loss 0.010022,Time used 0.002992s\n",
      "batch 1247, train_loss 0.013154,Time used 0.002992s\n",
      "batch 1248, train_loss 0.010896,Time used 0.002993s\n",
      "batch 1249, train_loss 0.011908,Time used 0.002991s\n",
      "batch 1250, train_loss 0.012827,Time used 0.002992s\n",
      "batch 1251, train_loss 0.009119,Time used 0.002992s\n",
      "batch 1252, train_loss 0.009977,Time used 0.002990s\n",
      "batch 1253, train_loss 0.011386,Time used 0.002992s\n",
      "batch 1254, train_loss 0.010871,Time used 0.001994s\n",
      "batch 1255, train_loss 0.011779,Time used 0.001994s\n",
      "batch 1256, train_loss 0.010994,Time used 0.001998s\n",
      "batch 1257, train_loss 0.009389,Time used 0.002990s\n",
      "batch 1258, train_loss 0.009012,Time used 0.002992s\n",
      "batch 1259, train_loss 0.009079,Time used 0.001994s\n",
      "batch 1260, train_loss 0.010869,Time used 0.002992s\n",
      "batch 1261, train_loss 0.010698,Time used 0.002992s\n",
      "batch 1262, train_loss 0.011460,Time used 0.001994s\n",
      "batch 1263, train_loss 0.010783,Time used 0.001993s\n",
      "batch 1264, train_loss 0.010378,Time used 0.001994s\n",
      "batch 1265, train_loss 0.010814,Time used 0.002993s\n",
      "batch 1266, train_loss 0.011364,Time used 0.002991s\n",
      "batch 1267, train_loss 0.014084,Time used 0.001994s\n",
      "batch 1268, train_loss 0.008788,Time used 0.002994s\n",
      "batch 1269, train_loss 0.007401,Time used 0.002991s\n",
      "batch 1270, train_loss 0.010722,Time used 0.002991s\n",
      "batch 1271, train_loss 0.009558,Time used 0.002992s\n",
      "batch 1272, train_loss 0.009834,Time used 0.002992s\n",
      "batch 1273, train_loss 0.010710,Time used 0.001993s\n",
      "batch 1274, train_loss 0.012869,Time used 0.002993s\n",
      "batch 1275, train_loss 0.010834,Time used 0.002993s\n",
      "batch 1276, train_loss 0.009783,Time used 0.001994s\n",
      "batch 1277, train_loss 0.012000,Time used 0.002992s\n",
      "batch 1278, train_loss 0.011285,Time used 0.002992s\n",
      "batch 1279, train_loss 0.008888,Time used 0.001995s\n",
      "batch 1280, train_loss 0.010391,Time used 0.002991s\n",
      "batch 1281, train_loss 0.010322,Time used 0.002991s\n",
      "batch 1282, train_loss 0.010761,Time used 0.001994s\n",
      "batch 1283, train_loss 0.012795,Time used 0.002993s\n",
      "batch 1284, train_loss 0.011343,Time used 0.001994s\n",
      "batch 1285, train_loss 0.012139,Time used 0.002993s\n",
      "batch 1286, train_loss 0.008382,Time used 0.002992s\n",
      "batch 1287, train_loss 0.010794,Time used 0.002992s\n",
      "batch 1288, train_loss 0.009018,Time used 0.002502s\n",
      "batch 1289, train_loss 0.009962,Time used 0.002503s\n",
      "batch 1290, train_loss 0.011994,Time used 0.001994s\n",
      "batch 1291, train_loss 0.010110,Time used 0.002994s\n",
      "batch 1292, train_loss 0.008371,Time used 0.002990s\n",
      "batch 1293, train_loss 0.010374,Time used 0.002992s\n",
      "batch 1294, train_loss 0.011229,Time used 0.002992s\n",
      "batch 1295, train_loss 0.009188,Time used 0.001994s\n",
      "batch 1296, train_loss 0.009638,Time used 0.002993s\n",
      "batch 1297, train_loss 0.013178,Time used 0.002993s\n",
      "batch 1298, train_loss 0.009544,Time used 0.001993s\n",
      "batch 1299, train_loss 0.012509,Time used 0.002991s\n",
      "batch 1300, train_loss 0.012188,Time used 0.001994s\n",
      "test_batch 1300, test_rmse_loss 0.106234,test_mae_loss 0.081097,test_mape_loss 45.145078,Time used 0.004987s\n",
      "batch 1301, train_loss 0.008584,Time used 0.002991s\n",
      "batch 1302, train_loss 0.009816,Time used 0.002992s\n",
      "batch 1303, train_loss 0.008478,Time used 0.001993s\n",
      "batch 1304, train_loss 0.013161,Time used 0.002993s\n",
      "batch 1305, train_loss 0.012812,Time used 0.002992s\n",
      "batch 1306, train_loss 0.009688,Time used 0.001993s\n",
      "batch 1307, train_loss 0.011481,Time used 0.002991s\n",
      "batch 1308, train_loss 0.012039,Time used 0.002992s\n",
      "batch 1309, train_loss 0.011742,Time used 0.001995s\n",
      "batch 1310, train_loss 0.009634,Time used 0.002992s\n",
      "batch 1311, train_loss 0.011792,Time used 0.002993s\n",
      "batch 1312, train_loss 0.010161,Time used 0.002991s\n",
      "batch 1313, train_loss 0.010805,Time used 0.002992s\n",
      "batch 1314, train_loss 0.012504,Time used 0.002993s\n",
      "batch 1315, train_loss 0.010634,Time used 0.002990s\n",
      "batch 1316, train_loss 0.012016,Time used 0.001994s\n",
      "batch 1317, train_loss 0.009839,Time used 0.001994s\n",
      "batch 1318, train_loss 0.009413,Time used 0.002995s\n",
      "batch 1319, train_loss 0.007730,Time used 0.002989s\n",
      "batch 1320, train_loss 0.009234,Time used 0.001994s\n",
      "batch 1321, train_loss 0.010322,Time used 0.001994s\n",
      "batch 1322, train_loss 0.010455,Time used 0.002992s\n",
      "batch 1323, train_loss 0.013562,Time used 0.001994s\n",
      "batch 1324, train_loss 0.011347,Time used 0.001995s\n",
      "batch 1325, train_loss 0.007765,Time used 0.002992s\n",
      "batch 1326, train_loss 0.012292,Time used 0.001993s\n",
      "batch 1327, train_loss 0.011670,Time used 0.002993s\n",
      "batch 1328, train_loss 0.010684,Time used 0.002992s\n",
      "batch 1329, train_loss 0.008779,Time used 0.001994s\n",
      "batch 1330, train_loss 0.010270,Time used 0.002992s\n",
      "batch 1331, train_loss 0.010542,Time used 0.001994s\n",
      "batch 1332, train_loss 0.011798,Time used 0.002992s\n",
      "batch 1333, train_loss 0.010474,Time used 0.001994s\n",
      "batch 1334, train_loss 0.008714,Time used 0.002993s\n",
      "batch 1335, train_loss 0.009715,Time used 0.001993s\n",
      "batch 1336, train_loss 0.011182,Time used 0.001994s\n",
      "batch 1337, train_loss 0.010362,Time used 0.002992s\n",
      "batch 1338, train_loss 0.010088,Time used 0.001994s\n",
      "batch 1339, train_loss 0.009875,Time used 0.002993s\n",
      "batch 1340, train_loss 0.010591,Time used 0.001994s\n",
      "batch 1341, train_loss 0.014560,Time used 0.001994s\n",
      "batch 1342, train_loss 0.009577,Time used 0.002992s\n",
      "batch 1343, train_loss 0.011032,Time used 0.001995s\n",
      "batch 1344, train_loss 0.009544,Time used 0.002992s\n",
      "batch 1345, train_loss 0.009390,Time used 0.001994s\n",
      "batch 1346, train_loss 0.009000,Time used 0.001995s\n",
      "batch 1347, train_loss 0.009949,Time used 0.002992s\n",
      "batch 1348, train_loss 0.012215,Time used 0.002992s\n",
      "batch 1349, train_loss 0.008562,Time used 0.001996s\n",
      "batch 1350, train_loss 0.011506,Time used 0.001995s\n",
      "batch 1351, train_loss 0.012371,Time used 0.001994s\n",
      "batch 1352, train_loss 0.013654,Time used 0.002993s\n",
      "batch 1353, train_loss 0.009347,Time used 0.002992s\n",
      "batch 1354, train_loss 0.012122,Time used 0.001994s\n",
      "batch 1355, train_loss 0.011167,Time used 0.002992s\n",
      "batch 1356, train_loss 0.008883,Time used 0.001996s\n",
      "batch 1357, train_loss 0.010324,Time used 0.001995s\n",
      "batch 1358, train_loss 0.010128,Time used 0.002993s\n",
      "batch 1359, train_loss 0.012492,Time used 0.001992s\n",
      "batch 1360, train_loss 0.007169,Time used 0.001994s\n",
      "batch 1361, train_loss 0.011441,Time used 0.002992s\n",
      "batch 1362, train_loss 0.009527,Time used 0.001993s\n",
      "batch 1363, train_loss 0.007990,Time used 0.001993s\n",
      "batch 1364, train_loss 0.011482,Time used 0.001996s\n",
      "batch 1365, train_loss 0.008811,Time used 0.001994s\n",
      "batch 1366, train_loss 0.010513,Time used 0.001994s\n",
      "batch 1367, train_loss 0.012031,Time used 0.002991s\n",
      "batch 1368, train_loss 0.009616,Time used 0.002994s\n",
      "batch 1369, train_loss 0.011427,Time used 0.002990s\n",
      "batch 1370, train_loss 0.012059,Time used 0.002992s\n",
      "batch 1371, train_loss 0.009967,Time used 0.001994s\n",
      "batch 1372, train_loss 0.010487,Time used 0.001995s\n",
      "batch 1373, train_loss 0.009566,Time used 0.001994s\n",
      "batch 1374, train_loss 0.011304,Time used 0.001995s\n",
      "batch 1375, train_loss 0.009461,Time used 0.001996s\n",
      "batch 1376, train_loss 0.010542,Time used 0.001994s\n",
      "batch 1377, train_loss 0.008105,Time used 0.002992s\n",
      "batch 1378, train_loss 0.009349,Time used 0.001994s\n",
      "batch 1379, train_loss 0.011835,Time used 0.002992s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1380, train_loss 0.010440,Time used 0.002994s\n",
      "batch 1381, train_loss 0.008437,Time used 0.002991s\n",
      "batch 1382, train_loss 0.011491,Time used 0.001994s\n",
      "batch 1383, train_loss 0.010555,Time used 0.002993s\n",
      "batch 1384, train_loss 0.009753,Time used 0.002992s\n",
      "batch 1385, train_loss 0.012046,Time used 0.001994s\n",
      "batch 1386, train_loss 0.010375,Time used 0.003991s\n",
      "batch 1387, train_loss 0.008449,Time used 0.002991s\n",
      "batch 1388, train_loss 0.012060,Time used 0.002991s\n",
      "batch 1389, train_loss 0.011424,Time used 0.002991s\n",
      "batch 1390, train_loss 0.009449,Time used 0.002993s\n",
      "batch 1391, train_loss 0.009204,Time used 0.002993s\n",
      "batch 1392, train_loss 0.012939,Time used 0.002992s\n",
      "batch 1393, train_loss 0.010839,Time used 0.002991s\n",
      "batch 1394, train_loss 0.010428,Time used 0.001994s\n",
      "batch 1395, train_loss 0.008692,Time used 0.002992s\n",
      "batch 1396, train_loss 0.009054,Time used 0.002992s\n",
      "batch 1397, train_loss 0.009450,Time used 0.002992s\n",
      "batch 1398, train_loss 0.008824,Time used 0.002992s\n",
      "batch 1399, train_loss 0.013181,Time used 0.001995s\n",
      "batch 1400, train_loss 0.010189,Time used 0.002992s\n",
      "test_batch 1400, test_rmse_loss 0.104638,test_mae_loss 0.079857,test_mape_loss 44.089270,Time used 0.004986s\n",
      "batch 1401, train_loss 0.009713,Time used 0.002992s\n",
      "batch 1402, train_loss 0.007845,Time used 0.001994s\n",
      "batch 1403, train_loss 0.008406,Time used 0.002992s\n",
      "batch 1404, train_loss 0.008956,Time used 0.001994s\n",
      "batch 1405, train_loss 0.013887,Time used 0.002993s\n",
      "batch 1406, train_loss 0.010945,Time used 0.001994s\n",
      "batch 1407, train_loss 0.009907,Time used 0.002993s\n",
      "batch 1408, train_loss 0.011563,Time used 0.001994s\n",
      "batch 1409, train_loss 0.013361,Time used 0.002993s\n",
      "batch 1410, train_loss 0.010265,Time used 0.001994s\n",
      "batch 1411, train_loss 0.010618,Time used 0.001994s\n",
      "batch 1412, train_loss 0.008728,Time used 0.002992s\n",
      "batch 1413, train_loss 0.009455,Time used 0.001995s\n",
      "batch 1414, train_loss 0.008712,Time used 0.001993s\n",
      "batch 1415, train_loss 0.009921,Time used 0.001993s\n",
      "batch 1416, train_loss 0.011044,Time used 0.002994s\n",
      "batch 1417, train_loss 0.010299,Time used 0.002991s\n",
      "batch 1418, train_loss 0.011827,Time used 0.001993s\n",
      "batch 1419, train_loss 0.008587,Time used 0.001994s\n",
      "batch 1420, train_loss 0.011606,Time used 0.002992s\n",
      "batch 1421, train_loss 0.008801,Time used 0.002328s\n",
      "batch 1422, train_loss 0.011008,Time used 0.001991s\n",
      "batch 1423, train_loss 0.010106,Time used 0.002502s\n",
      "batch 1424, train_loss 0.013479,Time used 0.003281s\n",
      "batch 1425, train_loss 0.010802,Time used 0.001994s\n",
      "batch 1426, train_loss 0.010106,Time used 0.001994s\n",
      "batch 1427, train_loss 0.011404,Time used 0.001995s\n",
      "batch 1428, train_loss 0.009773,Time used 0.001993s\n",
      "batch 1429, train_loss 0.010794,Time used 0.001994s\n",
      "batch 1430, train_loss 0.011966,Time used 0.002992s\n",
      "batch 1431, train_loss 0.009099,Time used 0.001994s\n",
      "batch 1432, train_loss 0.011827,Time used 0.002993s\n",
      "batch 1433, train_loss 0.011837,Time used 0.001993s\n",
      "batch 1434, train_loss 0.008264,Time used 0.001995s\n",
      "batch 1435, train_loss 0.008903,Time used 0.002992s\n",
      "batch 1436, train_loss 0.010790,Time used 0.002992s\n",
      "batch 1437, train_loss 0.011646,Time used 0.001995s\n",
      "batch 1438, train_loss 0.009699,Time used 0.001993s\n",
      "batch 1439, train_loss 0.007547,Time used 0.001995s\n",
      "batch 1440, train_loss 0.010962,Time used 0.001994s\n",
      "batch 1441, train_loss 0.010811,Time used 0.001993s\n",
      "batch 1442, train_loss 0.010056,Time used 0.001995s\n",
      "batch 1443, train_loss 0.009795,Time used 0.002991s\n",
      "batch 1444, train_loss 0.009005,Time used 0.001994s\n",
      "batch 1445, train_loss 0.008961,Time used 0.001992s\n",
      "batch 1446, train_loss 0.009268,Time used 0.002993s\n",
      "batch 1447, train_loss 0.011431,Time used 0.002991s\n",
      "batch 1448, train_loss 0.010508,Time used 0.002993s\n",
      "batch 1449, train_loss 0.009966,Time used 0.001993s\n",
      "batch 1450, train_loss 0.011860,Time used 0.002993s\n",
      "batch 1451, train_loss 0.010964,Time used 0.002992s\n",
      "batch 1452, train_loss 0.009917,Time used 0.002993s\n",
      "batch 1453, train_loss 0.011148,Time used 0.002992s\n",
      "batch 1454, train_loss 0.011362,Time used 0.002990s\n",
      "batch 1455, train_loss 0.009303,Time used 0.001994s\n",
      "batch 1456, train_loss 0.010338,Time used 0.002993s\n",
      "batch 1457, train_loss 0.010490,Time used 0.002991s\n",
      "batch 1458, train_loss 0.010996,Time used 0.001995s\n",
      "batch 1459, train_loss 0.009569,Time used 0.002992s\n",
      "batch 1460, train_loss 0.009181,Time used 0.002007s\n",
      "batch 1461, train_loss 0.011249,Time used 0.001994s\n",
      "batch 1462, train_loss 0.012074,Time used 0.002993s\n",
      "batch 1463, train_loss 0.009474,Time used 0.002991s\n",
      "batch 1464, train_loss 0.009482,Time used 0.001993s\n",
      "batch 1465, train_loss 0.009700,Time used 0.002993s\n",
      "batch 1466, train_loss 0.010342,Time used 0.002992s\n",
      "batch 1467, train_loss 0.011102,Time used 0.001994s\n",
      "batch 1468, train_loss 0.010617,Time used 0.002995s\n",
      "batch 1469, train_loss 0.011069,Time used 0.002990s\n",
      "batch 1470, train_loss 0.008004,Time used 0.002992s\n",
      "batch 1471, train_loss 0.010137,Time used 0.001992s\n",
      "batch 1472, train_loss 0.010965,Time used 0.003020s\n",
      "batch 1473, train_loss 0.009322,Time used 0.002966s\n",
      "batch 1474, train_loss 0.008327,Time used 0.002991s\n",
      "batch 1475, train_loss 0.008902,Time used 0.002993s\n",
      "batch 1476, train_loss 0.011510,Time used 0.001994s\n",
      "batch 1477, train_loss 0.009407,Time used 0.002993s\n",
      "batch 1478, train_loss 0.011099,Time used 0.002992s\n",
      "batch 1479, train_loss 0.009063,Time used 0.001993s\n",
      "batch 1480, train_loss 0.010119,Time used 0.001994s\n",
      "batch 1481, train_loss 0.011169,Time used 0.001994s\n",
      "batch 1482, train_loss 0.010190,Time used 0.002994s\n",
      "batch 1483, train_loss 0.010807,Time used 0.002991s\n",
      "batch 1484, train_loss 0.010692,Time used 0.002992s\n",
      "batch 1485, train_loss 0.011996,Time used 0.001994s\n",
      "batch 1486, train_loss 0.008907,Time used 0.002991s\n",
      "batch 1487, train_loss 0.008411,Time used 0.002993s\n",
      "batch 1488, train_loss 0.010016,Time used 0.002991s\n",
      "batch 1489, train_loss 0.011079,Time used 0.002992s\n",
      "batch 1490, train_loss 0.010472,Time used 0.001994s\n",
      "batch 1491, train_loss 0.011044,Time used 0.002993s\n",
      "batch 1492, train_loss 0.010347,Time used 0.002992s\n",
      "batch 1493, train_loss 0.010164,Time used 0.001995s\n",
      "batch 1494, train_loss 0.009058,Time used 0.001995s\n",
      "batch 1495, train_loss 0.009232,Time used 0.002993s\n",
      "batch 1496, train_loss 0.010033,Time used 0.002992s\n",
      "batch 1497, train_loss 0.011208,Time used 0.002994s\n",
      "batch 1498, train_loss 0.011215,Time used 0.002991s\n",
      "batch 1499, train_loss 0.012152,Time used 0.002991s\n",
      "batch 1500, train_loss 0.009602,Time used 0.001993s\n",
      "test_batch 1500, test_rmse_loss 0.103341,test_mae_loss 0.079043,test_mape_loss 44.227152,Time used 0.004959s\n",
      "The total time is 4.865750s\n"
     ]
    }
   ],
   "source": [
    "train_log = []\n",
    "test_log = []\n",
    "\n",
    "#开始时间\n",
    "timestart = time.time()\n",
    "trained_batches = 0 #记录多少个batch \n",
    "for epoch in range(100):\n",
    "    total_1oss = 0 #记录Loss\n",
    "    for batch in next_batch(shuffle(train_set), batch_size=128):  # 每次取128个短序列\n",
    "        #每一个batch的开始时间\n",
    "        batchstart = time.time()\n",
    "        \n",
    "        batch = torch.from_numpy(batch).float().to(device)  # (batch, seq_len)\n",
    "        # 使用短序列的前6个值作为历史，最后一个值作为预测值。\n",
    "        x, label = batch[:, :6], batch[:, -1]\n",
    "        out, hidden = model(x.unsqueeze(-1))  # out: (batch_size, seq_len, hidden_size)\n",
    "        out = out_linear(out[:, -1, :])\n",
    "        prediction = out.squeeze(-1)  # (batch)\n",
    "        \n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     \n",
    "        #累加loss\n",
    "        total_1oss += loss.item( )\n",
    "        trained_batches += 1\n",
    "    \n",
    "        train_log.append(loss.detach().cpu().numpy().tolist());\n",
    "        train_batch_time = (time.time() - batchstart)\n",
    "        print('batch %d, train_loss %.6f,Time used %.6fs'%(trained_batches, loss,train_batch_time))\n",
    "       \n",
    "        \n",
    "        # 每训练一定数量的batch，就在测试集上测试模型效果。\n",
    "        if trained_batches % 100 == 0:\n",
    "            #每一个batch的开始时间\n",
    "            batch_test_start = time.time()\n",
    "            #在每个epoch上测试\n",
    "            all_prediction = []\n",
    "            for batch in next_batch(test_set, batch_size=128):\n",
    "                batch = torch.from_numpy(batch).float().to(device)  # (batch, seq_len)\n",
    "                x, label = batch[:, :6], batch[:, -1]\n",
    "                out, hidden = model(x.unsqueeze(-1))  # out: (batch_size, seq_len, hidden_size)\n",
    "                out = out_linear(out[:, -1, :])\n",
    "                prediction = out.squeeze(-1)  # (batch)\n",
    "                all_prediction.append(prediction.detach().cpu().numpy())\n",
    "\n",
    "            all_prediction = np.concatenate(all_prediction)\n",
    "            all_label = test_set[:, -1]\n",
    "            # 没有进行反归一化操作。\n",
    "            #all_prediction = denormalize(all_prediction)\n",
    "            #all_label = denormalize(all_label)\n",
    "            # 计算测试指标。\n",
    "            rmse_score = math.sqrt(mse(all_label, all_prediction))\n",
    "            mae_score = mae(all_label, all_prediction)\n",
    "            mape_score = mape(all_label, all_prediction)\n",
    "            test_log.append([rmse_score, mae_score, mape_score])\n",
    "            test_batch_time = (time.time() - batch_test_start)\n",
    "            print('test_batch %d, test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(trained_batches, rmse_score,mae_score,mape_score,test_batch_time))\n",
    "\n",
    "        #每一个epoch的结束时间\n",
    "        #elapsed = (time.time() - epochstart)\n",
    "    #print('epoch %d, train_loss %.6f,test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(epoch+1, train_loss,rmse_score,mae_score,mape_score,elapsed))\n",
    "    #print('epoch %d, train_loss %.6f,test_rmse_loss %.6f,test_mae_loss %.6f,test_mape_loss %.6f,Time used %.6fs'%(epoch+1, train_loss,rmse_score,mae_score,mape_score,elapsed),file=f)\n",
    "    \n",
    "#计算总时间\n",
    "timesum = (time.time() - timestart)\n",
    "print('The total time is %fs'%(timesum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_loss曲线  \n",
    "x = np.linspace(0,len(train_log),len(train_log))  \n",
    "plt.plot(x,train_log,label=\"train_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('train_loss.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test_loss曲线  \n",
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,0],label=\"test_rmse_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_rmse_loss.jpg')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,1],label=\"test_mae_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_mae_loss.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test= np.linspace(0,len(test_log),len(test_log))  \n",
    "test_log = np.array(test_log)  \n",
    "plt.plot(x_test,test_log[:,2],label=\"test_mape_loss\",linewidth=1.5)  \n",
    "plt.xlabel(\"number of batches*100\")  \n",
    "plt.ylabel(\"loss\")  \n",
    "plt.legend()  \n",
    "plt.show()  \n",
    "plt.savefig('test_mape_loss.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
